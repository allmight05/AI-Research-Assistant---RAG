arXiv:1310.4546v1 [cs.CL] 16 Oct 2013 Distributed Representations of Words and Phrases and their Compositionality Tomas Mikolov Google Inc. Mountain View mikolov@google.com Ilya Sutskever Google Inc. Mountain View ilyasu@google.com Kai Chen Google Inc. Mountain View kai@google.com Greg Corrado Google Inc. Mountain View gcorrado@google.com Jeffrey Dean Google Inc. Mountain View jeff@google.com Abstract The recently introduced continuous Skip-gram model is an efÔ¨Åcient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain signiÔ¨Åcant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of ‚ÄúCanada‚Äù and ‚ÄúAir‚Äù cannot be easily combined to obtain ‚ÄúAir Canada‚Äù. Motivated by this example, we present a simple method for Ô¨Ånding phrases in text, and show that learning good vector representations for millions of phrases is possible. 1 Introduction Distributed representations of words in a vector
the meanings of ‚ÄúCanada‚Äù and ‚ÄúAir‚Äù cannot be easily combined to obtain ‚ÄúAir Canada‚Äù. Motivated by this example, we present a simple method for Ô¨Ånding phrases in text, and show that learning good vector representations for millions of phrases is possible. 1 Introduction Distributed representations of words in a vector space help learning algorithms to achieve better performance in natural language processing tasks by grouping similar words. One of the earliest use of word representations dates back to 1986 due to Rumelhart, Hinton, and Williams [13]. This idea has since been applied to statistical language modeling with considerable success [1]. The follow up work includes applications to automatic speech recognition and machine translation [14, 7], and a wide range of NLP tasks [2, 20, 15, 3, 18, 19, 9]. Recently, Mikolov et al. [8] introduced the Skip-gram model, an efÔ¨Åcient method for learning high- quality vector representations of words from large amounts of unstructured text data. Unlike most of the previously used neural network architectures for learning word vectors, training of the Skip- gram model (see Figure 1) does not involve dense matrix multiplications. This makes the training extremely efÔ¨Åcient: an optimized single-machine implementation can train on more than 100
large amounts of unstructured text data. Unlike most of the previously used neural network architectures for learning word vectors, training of the Skip- gram model (see Figure 1) does not involve dense matrix multiplications. This makes the training extremely efÔ¨Åcient: an optimized single-machine implementation can train on more than 100 billion words in one day. The word representations computed using neural networks are very interesting because the learned vectors explicitly encode many linguistic regularities and patterns. Somewhat surprisingly, many of these patterns can be represented as linear translations. For example, the result of a vector calcula- tion vec(‚ÄúMadrid‚Äù) - vec(‚ÄúSpain‚Äù) + vec(‚ÄúFrance‚Äù) is closer to vec(‚ÄúParis‚Äù) than to any other word vector [9, 8]. 1           Figure 1: The Skip-gram model architecture. The training objective is to learn word vector representations that are good at predicting the nearby words. In this paper we present several extensions of the original Skip-gram model. We show that sub- sampling of frequent words during training results in a signiÔ¨Åcant speedup (around 2x - 10x), and improves accuracy of the representations of less frequent words. In addition, we present a simpli- Ô¨Åed variant of Noise Contrastive
paper we present several extensions of the original Skip-gram model. We show that sub- sampling of frequent words during training results in a signiÔ¨Åcant speedup (around 2x - 10x), and improves accuracy of the representations of less frequent words. In addition, we present a simpli- Ô¨Åed variant of Noise Contrastive Estimation (NCE) [4] for training the Skip-gram model that results in faster training and better vector representations for frequent words, compared to more complex hierarchical softmax that was used in the prior work [8]. Word representations are limited by their inability to represent idiomatic phrases that are not com- positions of the individual words. For example, ‚ÄúBoston Globe‚Äù is a newspaper, and so it is not a natural combination of the meanings of ‚ÄúBoston‚Äù and ‚ÄúGlobe‚Äù. Therefore, using vectors to repre- sent the whole phrases makes the Skip-gram model considerably more expressive. Other techniques that aim to represent meaning of sentences by composing the word vectors, such as the recursive autoencoders [15], would also beneÔ¨Åt from using phrase vectors instead of the word vectors. The extension from word based to phrase based models is relatively simple. First we identify a large number of phrases using a data-driven approach, and then
sentences by composing the word vectors, such as the recursive autoencoders [15], would also beneÔ¨Åt from using phrase vectors instead of the word vectors. The extension from word based to phrase based models is relatively simple. First we identify a large number of phrases using a data-driven approach, and then we treat the phrases as individual tokens during the training. To evaluate the quality of the phrase vectors, we developed a test set of analogi- cal reasoning tasks that contains both words and phrases. A typical analogy pair from our test set is ‚ÄúMontreal‚Äù:‚ÄúMontreal Canadiens‚Äù::‚ÄúToronto‚Äù:‚ÄúToronto Maple Leafs‚Äù. It is considered to have been answered correctly if the nearest representation to vec(‚ÄúMontreal Canadiens‚Äù) - vec(‚ÄúMontreal‚Äù) + vec(‚ÄúToronto‚Äù) is vec(‚ÄúToronto Maple Leafs‚Äù). Finally, we describe another interesting property of the Skip-gram model. We found that simple vector addition can often produce meaningful results. For example, vec(‚ÄúRussia‚Äù) + vec(‚Äúriver‚Äù) is close to vec(‚ÄúVolga River‚Äù), and vec(‚ÄúGermany‚Äù) + vec(‚Äúcapital‚Äù) is close to vec(‚ÄúBerlin‚Äù). This compositionality suggests that a non-obvious degree of language understanding can be obtained by using basic mathematical operations on the word vector representations. 2 The Skip-gram Model The training objective of the Skip-gram model is to Ô¨Ånd word representations that are
River‚Äù), and vec(‚ÄúGermany‚Äù) + vec(‚Äúcapital‚Äù) is close to vec(‚ÄúBerlin‚Äù). This compositionality suggests that a non-obvious degree of language understanding can be obtained by using basic mathematical operations on the word vector representations. 2 The Skip-gram Model The training objective of the Skip-gram model is to Ô¨Ånd word representations that are useful for predicting the surrounding words in a sentence or a document. More formally, given a sequence of training words w1, w2, w3, . . . , wT , the objective of the Skip-gram model is to maximize the average log probability 1 T T X t=1 X ‚àíc‚â§j‚â§c,jÃ∏=0 log p(wt+j|wt) (1) where c is the size of the training context (which can be a function of the center word wt). Larger c results in more training examples and thus can lead to a higher accuracy, at the expense of the 2 training time. The basic Skip-gram formulation deÔ¨Ånes p(wt+j|wt) using the softmax function: p(wO|wI) = exp  v‚Ä≤ wO ‚ä§vwI  PW w=1 exp  v‚Ä≤w ‚ä§vwI  (2) where vw and v‚Ä≤ w are the ‚Äúinput‚Äù and ‚Äúoutput‚Äù vector representations of w, and W is the num- ber of words in the vocabulary. This formulation is impractical because
using the softmax function: p(wO|wI) = exp  v‚Ä≤ wO ‚ä§vwI  PW w=1 exp  v‚Ä≤w ‚ä§vwI  (2) where vw and v‚Ä≤ w are the ‚Äúinput‚Äù and ‚Äúoutput‚Äù vector representations of w, and W is the num- ber of words in the vocabulary. This formulation is impractical because the cost of computing ‚àálog p(wO|wI) is proportional to W, which is often large (105‚Äì107 terms). 2.1 Hierarchical Softmax A computationally efÔ¨Åcient approximation of the full softmax is the hierarchical softmax. In the context of neural network language models, it was Ô¨Årst introduced by Morin and Bengio [12]. The main advantage is that instead of evaluating W output nodes in the neural network to obtain the probability distribution, it is needed to evaluate only about log2(W) nodes. The hierarchical softmax uses a binary tree representation of the output layer with the W words as its leaves and, for each node, explicitly represents the relative probabilities of its child nodes. These deÔ¨Åne a random walk that assigns probabilities to words. More precisely, each word w can be reached by an appropriate path from the root of the tree. Let n(w, j) be the j-th node on the path from the root
explicitly represents the relative probabilities of its child nodes. These deÔ¨Åne a random walk that assigns probabilities to words. More precisely, each word w can be reached by an appropriate path from the root of the tree. Let n(w, j) be the j-th node on the path from the root to w, and let L(w) be the length of this path, so n(w, 1) = root and n(w, L(w)) = w. In addition, for any inner node n, let ch(n) be an arbitrary Ô¨Åxed child of n and let [[x]] be 1 if x is true and -1 otherwise. Then the hierarchical softmax deÔ¨Ånes p(wO|wI) as follows: p(w|wI) = L(w)‚àí1 Y j=1 œÉ  [[n(w, j + 1) = ch(n(w, j))]] ¬∑ v‚Ä≤ n(w,j) ‚ä§vwI  (3) where œÉ(x) = 1/(1 + exp(‚àíx)). It can be veriÔ¨Åed that PW w=1 p(w|wI) = 1. This implies that the cost of computing log p(wO|wI) and ‚àálog p(wO|wI) is proportional to L(wO), which on average is no greater than log W. Also, unlike the standard softmax formulation of the Skip-gram which assigns two representations vw and v‚Ä≤ w to each word w, the hierarchical softmax formulation has one representation vw for each word
log p(wO|wI) and ‚àálog p(wO|wI) is proportional to L(wO), which on average is no greater than log W. Also, unlike the standard softmax formulation of the Skip-gram which assigns two representations vw and v‚Ä≤ w to each word w, the hierarchical softmax formulation has one representation vw for each word w and one representation v‚Ä≤ n for every inner node n of the binary tree. The structure of the tree used by the hierarchical softmax has a considerable effect on the perfor- mance. Mnih and Hinton explored a number of methods for constructing the tree structure and the effect on both the training time and the resulting model accuracy [10]. In our work we use a binary Huffman tree, as it assigns short codes to the frequent words which results in fast training. It has been observed before that grouping words together by their frequency works well as a very simple speedup technique for the neural network based language models [5, 8]. 2.2 Negative Sampling An alternative to the hierarchical softmax is Noise Contrastive Estimation (NCE), which was in- troduced by Gutmann and Hyvarinen [4] and applied to language modeling by Mnih and Teh [11]. NCE posits that a good
simple speedup technique for the neural network based language models [5, 8]. 2.2 Negative Sampling An alternative to the hierarchical softmax is Noise Contrastive Estimation (NCE), which was in- troduced by Gutmann and Hyvarinen [4] and applied to language modeling by Mnih and Teh [11]. NCE posits that a good model should be able to differentiate data from noise by means of logistic regression. This is similar to hinge loss used by Collobert and Weston [2] who trained the models by ranking the data above noise. While NCE can be shown to approximately maximize the log probability of the softmax, the Skip- gram model is only concerned with learning high-quality vector representations, so we are free to simplify NCE as long as the vector representations retain their quality. We deÔ¨Åne Negative sampling (NEG) by the objective log œÉ(v‚Ä≤ wO ‚ä§vwI) + k X i=1 Ewi‚àºPn(w) h log œÉ(‚àív‚Ä≤ wi ‚ä§vwI) i (4) 3 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 Country and Capital Vectors Projected by PCA China Japan France Russia Germany Italy Spain Greece Turkey Beijing Paris Tokyo Poland Moscow Portugal Berlin Rome Athens Madrid Ankara Warsaw Lisbon
i (4) 3 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 Country and Capital Vectors Projected by PCA China Japan France Russia Germany Italy Spain Greece Turkey Beijing Paris Tokyo Poland Moscow Portugal Berlin Rome Athens Madrid Ankara Warsaw Lisbon Figure 2: Two-dimensional PCA projection of the 1000-dimensional Skip-gram vectors of countries and their capital cities. The Ô¨Ågure illustrates ability of the model to automatically organize concepts and learn implicitly the relationships between them, as during the training we did not provide any supervised information about what a capital city means. which is used to replace every log P(wO|wI) term in the Skip-gram objective. Thus the task is to distinguish the target word wO from draws from the noise distribution Pn(w) using logistic regres- sion, where there are k negative samples for each data sample. Our experiments indicate that values of k in the range 5‚Äì20 are useful for small training datasets, while for large datasets the k can be as small as 2‚Äì5. The main difference between the Negative sampling and NCE is that NCE needs both samples and the numerical probabilities of the noise distribution, while Negative sampling
of k in the range 5‚Äì20 are useful for small training datasets, while for large datasets the k can be as small as 2‚Äì5. The main difference between the Negative sampling and NCE is that NCE needs both samples and the numerical probabilities of the noise distribution, while Negative sampling uses only samples. And while NCE approximately maximizes the log probability of the softmax, this property is not important for our application. Both NCE and NEG have the noise distribution Pn(w) as a free parameter. We investigated a number of choices for Pn(w) and found that the unigram distribution U(w) raised to the 3/4rd power (i.e., U(w)3/4/Z) outperformed signiÔ¨Åcantly the unigram and the uniform distributions, for both NCE and NEG on every task we tried including language modeling (not reported here). 2.3 Subsampling of Frequent Words In very large corpora, the most frequent words can easily occur hundreds of millions of times (e.g., ‚Äúin‚Äù, ‚Äúthe‚Äù, and ‚Äúa‚Äù). Such words usually provide less information value than the rare words. For example, while the Skip-gram model beneÔ¨Åts from observing the co-occurrences of ‚ÄúFrance‚Äù and ‚ÄúParis‚Äù, it beneÔ¨Åts much less from observing the frequent co-occurrences of ‚ÄúFrance‚Äù and ‚Äúthe‚Äù, as nearly every word
of times (e.g., ‚Äúin‚Äù, ‚Äúthe‚Äù, and ‚Äúa‚Äù). Such words usually provide less information value than the rare words. For example, while the Skip-gram model beneÔ¨Åts from observing the co-occurrences of ‚ÄúFrance‚Äù and ‚ÄúParis‚Äù, it beneÔ¨Åts much less from observing the frequent co-occurrences of ‚ÄúFrance‚Äù and ‚Äúthe‚Äù, as nearly every word co-occurs frequently within a sentence with ‚Äúthe‚Äù. This idea can also be applied in the opposite direction; the vector representations of frequent words do not change signiÔ¨Åcantly after training on several million examples. To counter the imbalance between the rare and frequent words, we used a simple subsampling ap- proach: each word wi in the training set is discarded with probability computed by the formula P(wi) = 1 ‚àí s t f(wi) (5) 4 Method Time [min] Syntactic [%] Semantic [%] Total accuracy [%] NEG-5 38 63 54 59 NEG-15 97 63 58 61 HS-Huffman 41 53 40 47 NCE-5 38 60 45 53 The following results use 10‚àí5 subsampling NEG-5 14 61 58 60 NEG-15 36 61 61 61 HS-Huffman 21 52 59 55 Table 1: Accuracy of various Skip-gram 300-dimensional models on the analogical reasoning task as deÔ¨Åned in [8]. NEG-k stands for Negative Sampling with k negative
38 60 45 53 The following results use 10‚àí5 subsampling NEG-5 14 61 58 60 NEG-15 36 61 61 61 HS-Huffman 21 52 59 55 Table 1: Accuracy of various Skip-gram 300-dimensional models on the analogical reasoning task as deÔ¨Åned in [8]. NEG-k stands for Negative Sampling with k negative samples for each positive sample; NCE stands for Noise Contrastive Estimation and HS-Huffman stands for the Hierarchical Softmax with the frequency-based Huffman codes. where f(wi) is the frequency of word wi and t is a chosen threshold, typically around 10‚àí5. We chose this subsampling formula because it aggressively subsamples words whose frequency is greater than t while preserving the ranking of the frequencies. Although this subsampling for- mula was chosen heuristically, we found it to work well in practice. It accelerates learning and even signiÔ¨Åcantly improves the accuracy of the learned vectors of the rare words, as will be shown in the following sections. 3 Empirical Results In this section we evaluate the Hierarchical Softmax (HS), Noise Contrastive Estimation, Negative Sampling, and subsampling of the training words. We used the analogical reasoning task1 introduced by Mikolov et al. [8]. The task consists of analogies such as ‚ÄúGermany‚Äù : ‚ÄúBerlin‚Äù ::
in the following sections. 3 Empirical Results In this section we evaluate the Hierarchical Softmax (HS), Noise Contrastive Estimation, Negative Sampling, and subsampling of the training words. We used the analogical reasoning task1 introduced by Mikolov et al. [8]. The task consists of analogies such as ‚ÄúGermany‚Äù : ‚ÄúBerlin‚Äù :: ‚ÄúFrance‚Äù : ?, which are solved by Ô¨Ånding a vector x such that vec(x) is closest to vec(‚ÄúBerlin‚Äù) - vec(‚ÄúGermany‚Äù) + vec(‚ÄúFrance‚Äù) according to the cosine distance (we discard the input words from the search). This speciÔ¨Åc example is considered to have been answered correctly if x is ‚ÄúParis‚Äù. The task has two broad categories: the syntactic analogies (such as ‚Äúquick‚Äù : ‚Äúquickly‚Äù :: ‚Äúslow‚Äù : ‚Äúslowly‚Äù) and the semantic analogies, such as the country to capital city relationship. For training the Skip-gram models, we have used a large dataset consisting of various news articles (an internal Google dataset with one billion words). We discarded from the vocabulary all words that occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K. The performance of various Skip-gram models on the word analogy test set is reported in Table 1. The table shows that Negative
one billion words). We discarded from the vocabulary all words that occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K. The performance of various Skip-gram models on the word analogy test set is reported in Table 1. The table shows that Negative Sampling outperforms the Hierarchical Softmax on the analogical reasoning task, and has even slightly better performance than the Noise Contrastive Estimation. The subsampling of the frequent words improves the training speed several times and makes the word representations signiÔ¨Åcantly more accurate. It can be argued that the linearity of the skip-gram model makes its vectors more suitable for such linear analogical reasoning, but the results of Mikolov et al. [8] also show that the vectors learned by the standard sigmoidal recurrent neural networks (which are highly non-linear) improve on this task signiÔ¨Åcantly as the amount of the training data increases, suggesting that non-linear models also have a preference for a linear structure of the word representations. 4 Learning Phrases As discussed earlier, many phrases have a meaning that is not a simple composition of the mean- ings of its individual words. To learn vector representation for phrases, we Ô¨Årst Ô¨Ånd
suggesting that non-linear models also have a preference for a linear structure of the word representations. 4 Learning Phrases As discussed earlier, many phrases have a meaning that is not a simple composition of the mean- ings of its individual words. To learn vector representation for phrases, we Ô¨Årst Ô¨Ånd words that appear frequently together, and infrequently in other contexts. For example, ‚ÄúNew York Times‚Äù and ‚ÄúToronto Maple Leafs‚Äù are replaced by unique tokens in the training data, while a bigram ‚Äúthis is‚Äù will remain unchanged. 1code.google.com/p/word2vec/source/browse/trunk/questions-words.txt 5 Newspapers New York New York Times Baltimore Baltimore Sun San Jose San Jose Mercury News Cincinnati Cincinnati Enquirer NHL Teams Boston Boston Bruins Montreal Montreal Canadiens Phoenix Phoenix Coyotes Nashville Nashville Predators NBA Teams Detroit Detroit Pistons Toronto Toronto Raptors Oakland Golden State Warriors Memphis Memphis Grizzlies Airlines Austria Austrian Airlines Spain Spainair Belgium Brussels Airlines Greece Aegean Airlines Company executives Steve Ballmer Microsoft Larry Page Google Samuel J. Palmisano IBM Werner Vogels Amazon Table 2: Examples of the analogical reasoning task for phrases (the full test set has 3218 examples). The goal is to compute the fourth phrase using the Ô¨Årst three. Our best model achieved an accuracy of 72% on
Ballmer Microsoft Larry Page Google Samuel J. Palmisano IBM Werner Vogels Amazon Table 2: Examples of the analogical reasoning task for phrases (the full test set has 3218 examples). The goal is to compute the fourth phrase using the Ô¨Årst three. Our best model achieved an accuracy of 72% on this dataset. This way, we can form many reasonable phrases without greatly increasing the size of the vocabu- lary; in theory, we can train the Skip-gram model using all n-grams, but that would be too memory intensive. Many techniques have been previously developed to identify phrases in the text; however, it is out of scope of our work to compare them. We decided to use a simple data-driven approach, where phrases are formed based on the unigram and bigram counts, using score(wi, wj) = count(wiwj) ‚àíŒ¥ count(wi) √ó count(wj). (6) The Œ¥ is used as a discounting coefÔ¨Åcient and prevents too many phrases consisting of very infre- quent words to be formed. The bigrams with score above the chosen threshold are then used as phrases. Typically, we run 2-4 passes over the training data with decreasing threshold value, allow- ing longer phrases that consists of several words to be formed.
too many phrases consisting of very infre- quent words to be formed. The bigrams with score above the chosen threshold are then used as phrases. Typically, we run 2-4 passes over the training data with decreasing threshold value, allow- ing longer phrases that consists of several words to be formed. We evaluate the quality of the phrase representations using a new analogical reasoning task that involves phrases. Table 2 shows examples of the Ô¨Åve categories of analogies used in this task. This dataset is publicly available on the web2. 4.1 Phrase Skip-Gram Results Starting with the same news data as in the previous experiments, we Ô¨Årst constructed the phrase based training corpus and then we trained several Skip-gram models using different hyper- parameters. As before, we used vector dimensionality 300 and context size 5. This setting already achieves good performance on the phrase dataset, and allowed us to quickly compare the Negative Sampling and the Hierarchical Softmax, both with and without subsampling of the frequent tokens. The results are summarized in Table 3. The results show that while Negative Sampling achieves a respectable accuracy even with k = 5, using k = 15 achieves considerably better performance. Surprisingly, while we
the Negative Sampling and the Hierarchical Softmax, both with and without subsampling of the frequent tokens. The results are summarized in Table 3. The results show that while Negative Sampling achieves a respectable accuracy even with k = 5, using k = 15 achieves considerably better performance. Surprisingly, while we found the Hierar- chical Softmax to achieve lower performance when trained without subsampling, it became the best performing method when we downsampled the frequent words. This shows that the subsampling can result in faster training and can also improve accuracy, at least in some cases. 2code.google.com/p/word2vec/source/browse/trunk/questions-phrases.txt Method Dimensionality No subsampling [%] 10‚àí5 subsampling [%] NEG-5 300 24 27 NEG-15 300 27 42 HS-Huffman 300 19 47 Table 3: Accuracies of the Skip-gram models on the phrase analogy dataset. The models were trained on approximately one billion words from the news dataset. 6 NEG-15 with 10‚àí5 subsampling HS with 10‚àí5 subsampling Vasco de Gama Lingsugur Italian explorer Lake Baikal Great Rift Valley Aral Sea Alan Bean Rebbeca Naomi moonwalker Ionian Sea Ruegen Ionian Islands chess master chess grandmaster Garry Kasparov Table 4: Examples of the closest entities to the given short phrases, using two different models. Czech + currency Vietnam +
Vasco de Gama Lingsugur Italian explorer Lake Baikal Great Rift Valley Aral Sea Alan Bean Rebbeca Naomi moonwalker Ionian Sea Ruegen Ionian Islands chess master chess grandmaster Garry Kasparov Table 4: Examples of the closest entities to the given short phrases, using two different models. Czech + currency Vietnam + capital German + airlines Russian + river French + actress koruna Hanoi airline Lufthansa Moscow Juliette Binoche Check crown Ho Chi Minh City carrier Lufthansa Volga River Vanessa Paradis Polish zolty Viet Nam Ô¨Çag carrier Lufthansa upriver Charlotte Gainsbourg CTK Vietnamese Lufthansa Russia Cecile De Table 5: Vector compositionality using element-wise addition. Four closest tokens to the sum of two vectors are shown, using the best Skip-gram model. To maximize the accuracy on the phrase analogy task, we increased the amount of the training data by using a dataset with about 33 billion words. We used the hierarchical softmax, dimensionality of 1000, and the entire sentence for the context. This resulted in a model that reached an accuracy of 72%. We achieved lower accuracy 66% when we reduced the size of the training dataset to 6B words, which suggests that the large amount of the training data is crucial. To
of 1000, and the entire sentence for the context. This resulted in a model that reached an accuracy of 72%. We achieved lower accuracy 66% when we reduced the size of the training dataset to 6B words, which suggests that the large amount of the training data is crucial. To gain further insight into how different the representations learned by different models are, we did inspect manually the nearest neighbours of infrequent phrases using various models. In Table 4, we show a sample of such comparison. Consistently with the previous results, it seems that the best representations of phrases are learned by a model with the hierarchical softmax and subsampling. 5 Additive Compositionality We demonstrated that the word and phrase representations learned by the Skip-gram model exhibit a linear structure that makes it possible to perform precise analogical reasoning using simple vector arithmetics. Interestingly, we found that the Skip-gram representations exhibit another kind of linear structure that makes it possible to meaningfully combine words by an element-wise addition of their vector representations. This phenomenon is illustrated in Table 5. The additive property of the vectors can be explained by inspecting the training objective. The word vectors are in a linear
exhibit another kind of linear structure that makes it possible to meaningfully combine words by an element-wise addition of their vector representations. This phenomenon is illustrated in Table 5. The additive property of the vectors can be explained by inspecting the training objective. The word vectors are in a linear relationship with the inputs to the softmax nonlinearity. As the word vectors are trained to predict the surrounding words in the sentence, the vectors can be seen as representing the distribution of the context in which a word appears. These values are related logarithmically to the probabilities computed by the output layer, so the sum of two word vectors is related to the product of the two context distributions. The product works here as the AND function: words that are assigned high probabilities by both word vectors will have high probability, and the other words will have low probability. Thus, if ‚ÄúVolga River‚Äù appears frequently in the same sentence together with the words ‚ÄúRussian‚Äù and ‚Äúriver‚Äù, the sum of these two word vectors will result in such a feature vector that is close to the vector of ‚ÄúVolga River‚Äù. 6 Comparison to Published Word Representations Many authors who previously worked
if ‚ÄúVolga River‚Äù appears frequently in the same sentence together with the words ‚ÄúRussian‚Äù and ‚Äúriver‚Äù, the sum of these two word vectors will result in such a feature vector that is close to the vector of ‚ÄúVolga River‚Äù. 6 Comparison to Published Word Representations Many authors who previously worked on the neural network based representations of words have published their resulting models for further use and comparison: amongst the most well known au- thors are Collobert and Weston [2], Turian et al. [17], and Mnih and Hinton [10]. We downloaded their word vectors from the web3. Mikolov et al. [8] have already evaluated these word representa- tions on the word analogy task, where the Skip-gram models achieved the best performance with a huge margin. 3http://metaoptimize.com/projects/wordreprs/ 7 Model Redmond Havel ninjutsu grafÔ¨Åti capitulate (training time) Collobert (50d) conyers plauen reiki cheesecake abdicate (2 months) lubbock dzerzhinsky kohona gossip accede keene osterreich karate dioramas rearm Turian (200d) McCarthy Jewell - gunÔ¨Åre - (few weeks) Alston Arzu - emotion - Cousins Ovitz - impunity - Mnih (100d) Podhurst Pontiff - anaesthetics Mavericks (7 days) Harlang Pinochet - monkeys planning Agarwal Rodionov - Jews hesitated Skip-Phrase Redmond Wash. Vaclav Havel ninja spray paint
osterreich karate dioramas rearm Turian (200d) McCarthy Jewell - gunÔ¨Åre - (few weeks) Alston Arzu - emotion - Cousins Ovitz - impunity - Mnih (100d) Podhurst Pontiff - anaesthetics Mavericks (7 days) Harlang Pinochet - monkeys planning Agarwal Rodionov - Jews hesitated Skip-Phrase Redmond Wash. Vaclav Havel ninja spray paint capitulation (1000d, 1 day) Redmond Washington president Vaclav Havel martial arts graÔ¨Åtti capitulated Microsoft Velvet Revolution swordsmanship taggers capitulating Table 6: Examples of the closest tokens given various well known models and the Skip-gram model trained on phrases using over 30 billion training words. An empty cell means that the word was not in the vocabulary. To give more insight into the difference of the quality of the learned vectors, we provide empirical comparison by showing the nearest neighbours of infrequent words in Table 6. These examples show that the big Skip-gram model trained on a large corpus visibly outperforms all the other models in the quality of the learned representations. This can be attributed in part to the fact that this model has been trained on about 30 billion words, which is about two to three orders of magnitude more data than the typical size used in the prior
all the other models in the quality of the learned representations. This can be attributed in part to the fact that this model has been trained on about 30 billion words, which is about two to three orders of magnitude more data than the typical size used in the prior work. Interestingly, although the training set is much larger, the training time of the Skip-gram model is just a fraction of the time complexity required by the previous model architectures. 7 Conclusion This work has several key contributions. We show how to train distributed representations of words and phrases with the Skip-gram model and demonstrate that these representations exhibit linear structure that makes precise analogical reasoning possible. The techniques introduced in this paper can be used also for training the continuous bag-of-words model introduced in [8]. We successfully trained models on several orders of magnitude more data than the previously pub- lished models, thanks to the computationally efÔ¨Åcient model architecture. This results in a great improvement in the quality of the learned word and phrase representations, especially for the rare entities. We also found that the subsampling of the frequent words results in both faster training and signiÔ¨Åcantly better representations
pub- lished models, thanks to the computationally efÔ¨Åcient model architecture. This results in a great improvement in the quality of the learned word and phrase representations, especially for the rare entities. We also found that the subsampling of the frequent words results in both faster training and signiÔ¨Åcantly better representations of uncommon words. Another contribution of our paper is the Negative sampling algorithm, which is an extremely simple training method that learns accurate representations especially for frequent words. The choice of the training algorithm and the hyper-parameter selection is a task speciÔ¨Åc decision, as we found that different problems have different optimal hyperparameter conÔ¨Ågurations. In our experiments, the most crucial decisions that affect the performance are the choice of the model architecture, the size of the vectors, the subsampling rate, and the size of the training window. A very interesting result of this work is that the word vectors can be somewhat meaningfully com- bined using just simple vector addition. Another approach for learning representations of phrases presented in this paper is to simply represent the phrases with a single token. Combination of these two approaches gives a powerful yet simple way how to represent longer pieces of text, while
be somewhat meaningfully com- bined using just simple vector addition. Another approach for learning representations of phrases presented in this paper is to simply represent the phrases with a single token. Combination of these two approaches gives a powerful yet simple way how to represent longer pieces of text, while hav- ing minimal computational complexity. Our work can thus be seen as complementary to the existing approach that attempts to represent phrases using recursive matrix-vector operations [16]. We made the code for training the word and phrase vectors based on the techniques described in this paper available as an open-source project4. 4code.google.com/p/word2vec 8 References [1] Yoshua Bengio, R¬¥ejean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic language model. The Journal of Machine Learning Research, 3:1137‚Äì1155, 2003. [2] Ronan Collobert and Jason Weston. A uniÔ¨Åed architecture for natural language processing: deep neu- ral networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160‚Äì167. ACM, 2008. [3] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment classi- Ô¨Åcation: A deep learning approach. In ICML, 513‚Äì520, 2011. [4] Michael U Gutmann and Aapo Hyv¬®arinen. Noise-contrastive estimation of unnormalized statistical mod- els, with applications
the 25th international conference on Machine learning, pages 160‚Äì167. ACM, 2008. [3] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment classi- Ô¨Åcation: A deep learning approach. In ICML, 513‚Äì520, 2011. [4] Michael U Gutmann and Aapo Hyv¬®arinen. Noise-contrastive estimation of unnormalized statistical mod- els, with applications to natural image statistics. The Journal of Machine Learning Research, 13:307‚Äì361, 2012. [5] Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Extensions of recurrent neural network language model. In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, pages 5528‚Äì5531. IEEE, 2011. [6] Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas Burget and Jan Cernocky. Strategies for Training Large Scale Neural Network Language Models. In Proc. Automatic Speech Recognition and Understand- ing, 2011. [7] Tomas Mikolov. Statistical Language Models Based on Neural Networks. PhD thesis, PhD Thesis, Brno University of Technology, 2012. [8] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. EfÔ¨Åcient estimation of word representations in vector space. ICLR Workshop, 2013. [9] Tomas Mikolov, Wen-tau Yih and Geoffrey Zweig. Linguistic Regularities in Continuous Space Word Representations. In Proceedings of NAACL HLT, 2013. [10] Andriy Mnih and Geoffrey E Hinton. A scalable hierarchical distributed language
Chen, Greg Corrado, and Jeffrey Dean. EfÔ¨Åcient estimation of word representations in vector space. ICLR Workshop, 2013. [9] Tomas Mikolov, Wen-tau Yih and Geoffrey Zweig. Linguistic Regularities in Continuous Space Word Representations. In Proceedings of NAACL HLT, 2013. [10] Andriy Mnih and Geoffrey E Hinton. A scalable hierarchical distributed language model. Advances in neural information processing systems, 21:1081‚Äì1088, 2009. [11] Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic language models. arXiv preprint arXiv:1206.6426, 2012. [12] Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model. In Pro- ceedings of the international workshop on artiÔ¨Åcial intelligence and statistics, pages 246‚Äì252, 2005. [13] David E Rumelhart, Geoffrey E Hintont, and Ronald J Williams. Learning representations by back- propagating errors. Nature, 323(6088):533‚Äì536, 1986. [14] Holger Schwenk. Continuous space language models. Computer Speech and Language, vol. 21, 2007. [15] Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 26th International Conference on Machine Learning (ICML), volume 2, 2011. [16] Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. Semantic Compositionality Through Recursive Matrix-Vector Spaces. In Proceedings of
Ng, and Christopher D. Manning. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 26th International Conference on Machine Learning (ICML), volume 2, 2011. [16] Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. Semantic Compositionality Through Recursive Matrix-Vector Spaces. In Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2012. [17] Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computa- tional Linguistics, pages 384‚Äì394. Association for Computational Linguistics, 2010. [18] Peter D. Turney and Patrick Pantel. From frequency to meaning: Vector space models of semantics. In Journal of ArtiÔ¨Åcial Intelligence Research, 37:141-188, 2010. [19] Peter D. Turney. Distributional semantics beyond words: Supervised learning of analogy and paraphrase. In Transactions of the Association for Computational Linguistics (TACL), 353‚Äì366, 2013. [20] Jason Weston, Samy Bengio, and Nicolas Usunier. Wsabie: Scaling up to large vocabulary image annota- tion. In Proceedings of the Twenty-Second international joint conference on ArtiÔ¨Åcial Intelligence-Volume Volume Three, pages 2764‚Äì2770. AAAI Press, 2011. 9
[20] Jason Weston, Samy Bengio, and Nicolas Usunier. Wsabie: Scaling up to large vocabulary image annota- tion. In Proceedings of the Twenty-Second international joint conference on ArtiÔ¨Åcial Intelligence-Volume Volume Three, pages 2764‚Äì2770. AAAI Press, 2011. 9
Deep Residual Learning for Image Recognition Kaiming He Xiangyu Zhang Shaoqing Ren Jian Sun Microsoft Research {kahe, v-xiangz, v-shren, jiansun}@microsoft.com Abstract Deeper neural networks are more difÔ¨Åcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers‚Äî8√ó deeper than VGG nets [41] but still having lower complex- ity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiÔ¨Åcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28% relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions
analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28% relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation. 1. Introduction Deep convolutional neural networks [22, 21] have led to a series of breakthroughs for image classiÔ¨Åcation [21, 50, 40]. Deep networks naturally integrate low/mid/high- level features [50] and classiÔ¨Åers in an end-to-end multi- layer fashion, and the ‚Äúlevels‚Äù of features can be enriched by the number of stacked layers (depth). Recent evidence [41, 44] reveals that network depth is of crucial importance, and the leading results [41, 44, 13, 16] on the challenging ImageNet dataset [36] all exploit ‚Äúvery deep‚Äù [41] models, with a depth of sixteen [41] to thirty [16]. Many other non- trivial visual recognition tasks [8, 12, 7, 32, 27] have also 1http://image-net.org/challenges/LSVRC/2015/ and http://mscoco.org/dataset/#detections-challenge2015. 0 1 2 3 4 5 6 0 10 20 iter. (1e4) training
the challenging ImageNet dataset [36] all exploit ‚Äúvery deep‚Äù [41] models, with a depth of sixteen [41] to thirty [16]. Many other non- trivial visual recognition tasks [8, 12, 7, 32, 27] have also 1http://image-net.org/challenges/LSVRC/2015/ and http://mscoco.org/dataset/#detections-challenge2015. 0 1 2 3 4 5 6 0 10 20 iter. (1e4) training error (%) 0 1 2 3 4 5 6 0 10 20 iter. (1e4) test error (%) 56-layer 20-layer 56-layer 20-layer Figure 1. Training error (left) and test error (right) on CIFAR-10 with 20-layer and 56-layer ‚Äúplain‚Äù networks. The deeper network has higher training error, and thus test error. Similar phenomena on ImageNet is presented in Fig. 4. greatly beneÔ¨Åted from very deep models. Driven by the signiÔ¨Åcance of depth, a question arises: Is learning better networks as easy as stacking more layers? An obstacle to answering this question was the notorious problem of vanishing/exploding gradients [1, 9], which hamper convergence from the beginning. This problem, however, has been largely addressed by normalized initial- ization [23, 9, 37, 13] and intermediate normalization layers [16], which enable networks with tens of layers to start con- verging for stochastic gradient descent (SGD) with back- propagation [22]. When deeper networks are able to
convergence from the beginning. This problem, however, has been largely addressed by normalized initial- ization [23, 9, 37, 13] and intermediate normalization layers [16], which enable networks with tens of layers to start con- verging for stochastic gradient descent (SGD) with back- propagation [22]. When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated (which might be unsurprising) and then degrades rapidly. Unexpectedly, such degradation is not caused by overÔ¨Åtting, and adding more layers to a suitably deep model leads to higher train- ing error, as reported in [11, 42] and thoroughly veriÔ¨Åed by our experiments. Fig. 1 shows a typical example. The degradation (of training accuracy) indicates that not all systems are similarly easy to optimize. Let us consider a shallower architecture and its deeper counterpart that adds more layers onto it. There exists a solution by construction to the deeper model: the added layers are identity mapping, and the other layers are copied from the learned shallower model. The existence of this constructed solution indicates that a deeper model should produce no higher training error than its shallower counterpart. But experiments show that our current solvers
construction to the deeper model: the added layers are identity mapping, and the other layers are copied from the learned shallower model. The existence of this constructed solution indicates that a deeper model should produce no higher training error than its shallower counterpart. But experiments show that our current solvers on hand are unable to Ô¨Ånd solutions that 1 arXiv:1512.03385v1 [cs.CV] 10 Dec 2015 identity weight layer weight layer relu relu F(x)+x x F(x) x Figure 2. Residual learning: a building block. are comparably good or better than the constructed solution (or unable to do so in feasible time). In this paper, we address the degradation problem by introducing a deep residual learning framework. In- stead of hoping each few stacked layers directly Ô¨Åt a desired underlying mapping, we explicitly let these lay- ers Ô¨Åt a residual mapping. Formally, denoting the desired underlying mapping as H(x), we let the stacked nonlinear layers Ô¨Åt another mapping of F(x) := H(x)‚àíx. The orig- inal mapping is recast into F(x)+x. We hypothesize that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push
nonlinear layers Ô¨Åt another mapping of F(x) := H(x)‚àíx. The orig- inal mapping is recast into F(x)+x. We hypothesize that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to Ô¨Åt an identity mapping by a stack of nonlinear layers. The formulation of F(x)+x can be realized by feedfor- ward neural networks with ‚Äúshortcut connections‚Äù (Fig. 2). Shortcut connections [2, 34, 49] are those skipping one or more layers. In our case, the shortcut connections simply perform identity mapping, and their outputs are added to the outputs of the stacked layers (Fig. 2). Identity short- cut connections add neither extra parameter nor computa- tional complexity. The entire network can still be trained end-to-end by SGD with backpropagation, and can be eas- ily implemented using common libraries (e.g., Caffe [19]) without modifying the solvers. We present comprehensive experiments on ImageNet [36] to show the degradation problem and evaluate our method. We show that: 1) Our extremely deep residual nets are easy to optimize, but the counterpart ‚Äúplain‚Äù nets (that simply stack layers) exhibit higher training
common libraries (e.g., Caffe [19]) without modifying the solvers. We present comprehensive experiments on ImageNet [36] to show the degradation problem and evaluate our method. We show that: 1) Our extremely deep residual nets are easy to optimize, but the counterpart ‚Äúplain‚Äù nets (that simply stack layers) exhibit higher training error when the depth increases; 2) Our deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing re- sults substantially better than previous networks. Similar phenomena are also shown on the CIFAR-10 set [20], suggesting that the optimization difÔ¨Åculties and the effects of our method are not just akin to a particular dataset. We present successfully trained models on this dataset with over 100 layers, and explore models with over 1000 layers. On the ImageNet classiÔ¨Åcation dataset [36], we obtain excellent results by extremely deep residual nets. Our 152- layer residual net is the deepest network ever presented on ImageNet, while still having lower complexity than VGG nets [41]. Our ensemble has 3.57% top-5 error on the ImageNet test set, and won the 1st place in the ILSVRC 2015 classiÔ¨Åcation competition. The extremely deep rep- resentations also have excellent generalization performance on other recognition tasks, and lead
presented on ImageNet, while still having lower complexity than VGG nets [41]. Our ensemble has 3.57% top-5 error on the ImageNet test set, and won the 1st place in the ILSVRC 2015 classiÔ¨Åcation competition. The extremely deep rep- resentations also have excellent generalization performance on other recognition tasks, and lead us to further win the 1st places on: ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation in ILSVRC & COCO 2015 competitions. This strong evidence shows that the residual learning principle is generic, and we expect that it is applicable in other vision and non-vision problems. 2. Related Work Residual Representations. In image recognition, VLAD [18] is a representation that encodes by the residual vectors with respect to a dictionary, and Fisher Vector [30] can be formulated as a probabilistic version [18] of VLAD. Both of them are powerful shallow representations for image re- trieval and classiÔ¨Åcation [4, 48]. For vector quantization, encoding residual vectors [17] is shown to be more effec- tive than encoding original vectors. In low-level vision and computer graphics, for solv- ing Partial Differential Equations (PDEs), the widely used Multigrid method [3] reformulates the system as subprob- lems at multiple scales, where each subproblem is
For vector quantization, encoding residual vectors [17] is shown to be more effec- tive than encoding original vectors. In low-level vision and computer graphics, for solv- ing Partial Differential Equations (PDEs), the widely used Multigrid method [3] reformulates the system as subprob- lems at multiple scales, where each subproblem is respon- sible for the residual solution between a coarser and a Ô¨Åner scale. An alternative to Multigrid is hierarchical basis pre- conditioning [45, 46], which relies on variables that repre- sent residual vectors between two scales. It has been shown [3, 45, 46] that these solvers converge much faster than stan- dard solvers that are unaware of the residual nature of the solutions. These methods suggest that a good reformulation or preconditioning can simplify the optimization. Shortcut Connections. Practices and theories that lead to shortcut connections [2, 34, 49] have been studied for a long time. An early practice of training multi-layer perceptrons (MLPs) is to add a linear layer connected from the network input to the output [34, 49]. In [44, 24], a few interme- diate layers are directly connected to auxiliary classiÔ¨Åers for addressing vanishing/exploding gradients. The papers of [39, 38, 31, 47] propose methods for centering layer
training multi-layer perceptrons (MLPs) is to add a linear layer connected from the network input to the output [34, 49]. In [44, 24], a few interme- diate layers are directly connected to auxiliary classiÔ¨Åers for addressing vanishing/exploding gradients. The papers of [39, 38, 31, 47] propose methods for centering layer re- sponses, gradients, and propagated errors, implemented by shortcut connections. In [44], an ‚Äúinception‚Äù layer is com- posed of a shortcut branch and a few deeper branches. Concurrent with our work, ‚Äúhighway networks‚Äù [42, 43] present shortcut connections with gating functions [15]. These gates are data-dependent and have parameters, in contrast to our identity shortcuts that are parameter-free. When a gated shortcut is ‚Äúclosed‚Äù (approaching zero), the layers in highway networks represent non-residual func- tions. On the contrary, our formulation always learns residual functions; our identity shortcuts are never closed, and all information is always passed through, with addi- tional residual functions to be learned. In addition, high- 2 way networks have not demonstrated accuracy gains with extremely increased depth (e.g., over 100 layers). 3. Deep Residual Learning 3.1. Residual Learning Let us consider H(x) as an underlying mapping to be Ô¨Åt by a few stacked layers (not necessarily the
residual functions to be learned. In addition, high- 2 way networks have not demonstrated accuracy gains with extremely increased depth (e.g., over 100 layers). 3. Deep Residual Learning 3.1. Residual Learning Let us consider H(x) as an underlying mapping to be Ô¨Åt by a few stacked layers (not necessarily the entire net), with x denoting the inputs to the Ô¨Årst of these layers. If one hypothesizes that multiple nonlinear layers can asymptoti- cally approximate complicated functions2, then it is equiv- alent to hypothesize that they can asymptotically approxi- mate the residual functions, i.e., H(x) ‚àíx (assuming that the input and output are of the same dimensions). So rather than expect stacked layers to approximate H(x), we explicitly let these layers approximate a residual function F(x) := H(x) ‚àíx. The original function thus becomes F(x)+x. Although both forms should be able to asymptot- ically approximate the desired functions (as hypothesized), the ease of learning might be different. This reformulation is motivated by the counterintuitive phenomena about the degradation problem (Fig. 1, left). As we discussed in the introduction, if the added layers can be constructed as identity mappings, a deeper model should have training error no greater than its shallower counter-
ease of learning might be different. This reformulation is motivated by the counterintuitive phenomena about the degradation problem (Fig. 1, left). As we discussed in the introduction, if the added layers can be constructed as identity mappings, a deeper model should have training error no greater than its shallower counter- part. The degradation problem suggests that the solvers might have difÔ¨Åculties in approximating identity mappings by multiple nonlinear layers. With the residual learning re- formulation, if identity mappings are optimal, the solvers may simply drive the weights of the multiple nonlinear lay- ers toward zero to approach identity mappings. In real cases, it is unlikely that identity mappings are op- timal, but our reformulation may help to precondition the problem. If the optimal function is closer to an identity mapping than to a zero mapping, it should be easier for the solver to Ô¨Ånd the perturbations with reference to an identity mapping, than to learn the function as a new one. We show by experiments (Fig. 7) that the learned residual functions in general have small responses, suggesting that identity map- pings provide reasonable preconditioning. 3.2. Identity Mapping by Shortcuts We adopt residual learning to every few stacked layers. A
identity mapping, than to learn the function as a new one. We show by experiments (Fig. 7) that the learned residual functions in general have small responses, suggesting that identity map- pings provide reasonable preconditioning. 3.2. Identity Mapping by Shortcuts We adopt residual learning to every few stacked layers. A building block is shown in Fig. 2. Formally, in this paper we consider a building block deÔ¨Åned as: y = F(x, {Wi}) + x. (1) Here x and y are the input and output vectors of the lay- ers considered. The function F(x, {Wi}) represents the residual mapping to be learned. For the example in Fig. 2 that has two layers, F = W2œÉ(W1x) in which œÉ denotes 2This hypothesis, however, is still an open question. See [28]. ReLU [29] and the biases are omitted for simplifying no- tations. The operation F + x is performed by a shortcut connection and element-wise addition. We adopt the sec- ond nonlinearity after the addition (i.e., œÉ(y), see Fig. 2). The shortcut connections in Eqn.(1) introduce neither ex- tra parameter nor computation complexity. This is not only attractive in practice but also important in our comparisons between plain and residual networks. We can
and element-wise addition. We adopt the sec- ond nonlinearity after the addition (i.e., œÉ(y), see Fig. 2). The shortcut connections in Eqn.(1) introduce neither ex- tra parameter nor computation complexity. This is not only attractive in practice but also important in our comparisons between plain and residual networks. We can fairly com- pare plain/residual networks that simultaneously have the same number of parameters, depth, width, and computa- tional cost (except for the negligible element-wise addition). The dimensions of x and F must be equal in Eqn.(1). If this is not the case (e.g., when changing the input/output channels), we can perform a linear projection Ws by the shortcut connections to match the dimensions: y = F(x, {Wi}) + Wsx. (2) We can also use a square matrix Ws in Eqn.(1). But we will show by experiments that the identity mapping is sufÔ¨Åcient for addressing the degradation problem and is economical, and thus Ws is only used when matching dimensions. The form of the residual function F is Ô¨Çexible. Exper- iments in this paper involve a function F that has two or three layers (Fig. 5), while more layers are possible. But if F has only a single layer, Eqn.(1) is
and thus Ws is only used when matching dimensions. The form of the residual function F is Ô¨Çexible. Exper- iments in this paper involve a function F that has two or three layers (Fig. 5), while more layers are possible. But if F has only a single layer, Eqn.(1) is similar to a linear layer: y = W1x + x, for which we have not observed advantages. We also note that although the above notations are about fully-connected layers for simplicity, they are applicable to convolutional layers. The function F(x, {Wi}) can repre- sent multiple convolutional layers. The element-wise addi- tion is performed on two feature maps, channel by channel. 3.3. Network Architectures We have tested various plain/residual nets, and have ob- served consistent phenomena. To provide instances for dis- cussion, we describe two models for ImageNet as follows. Plain Network. Our plain baselines (Fig. 3, middle) are mainly inspired by the philosophy of VGG nets [41] (Fig. 3, left). The convolutional layers mostly have 3√ó3 Ô¨Ålters and follow two simple design rules: (i) for the same output feature map size, the layers have the same number of Ô¨Ål- ters; and (ii) if the feature map size is halved, the
by the philosophy of VGG nets [41] (Fig. 3, left). The convolutional layers mostly have 3√ó3 Ô¨Ålters and follow two simple design rules: (i) for the same output feature map size, the layers have the same number of Ô¨Ål- ters; and (ii) if the feature map size is halved, the num- ber of Ô¨Ålters is doubled so as to preserve the time com- plexity per layer. We perform downsampling directly by convolutional layers that have a stride of 2. The network ends with a global average pooling layer and a 1000-way fully-connected layer with softmax. The total number of weighted layers is 34 in Fig. 3 (middle). It is worth noticing that our model has fewer Ô¨Ålters and lower complexity than VGG nets [41] (Fig. 3, left). Our 34- layer baseline has 3.6 billion FLOPs (multiply-adds), which is only 18% of VGG-19 (19.6 billion FLOPs). 3 7x7 conv, 64, /2 pool, /2 3x3 conv, 64 3x3 conv, 64 3x3 conv, 64 3x3 conv, 64 3x3 conv, 64 3x3 conv, 64 3x3 conv, 128, /2 3x3 conv, 128 3x3 conv, 128 3x3 conv, 128 3x3 conv, 128 3x3 conv, 128 3x3 conv, 128 3x3 conv, 128 3x3 conv, 256, /2 3x3
pool, /2 3x3 conv, 64 3x3 conv, 64 3x3 conv, 64 3x3 conv, 64 3x3 conv, 64 3x3 conv, 64 3x3 conv, 128, /2 3x3 conv, 128 3x3 conv, 128 3x3 conv, 128 3x3 conv, 128 3x3 conv, 128 3x3 conv, 128 3x3 conv, 128 3x3 conv, 256, /2 3x3 conv, 256 3x3 conv, 256 3x3 conv, 256 3x3 conv, 256 3x3 conv, 256 3x3 conv, 256 3x3 conv, 256 3x3 conv, 256 3x3 conv, 256 3x3 conv, 256 3x3 conv, 256 3x3 conv, 512, /2 3x3 conv, 512 3x3 conv, 512 3x3 conv, 512 3x3 conv, 512 3x3 conv, 512 avg pool fc 1000 image 3x3 conv, 512 3x3 conv, 64 3x3 conv, 64 pool, /2 3x3 conv, 128 3x3 conv, 128 pool, /2 3x3 conv, 256 3x3 conv, 256 3x3 conv, 256 3x3 conv, 256 pool, /2 3x3 conv, 512 3x3 conv, 512 3x3 conv, 512 pool, /2 3x3 conv, 512 3x3 conv, 512 3x3 conv, 512 3x3 conv, 512 pool, /2 fc 4096 fc 4096 fc 1000 image output size: 112 output size: 224 output size: 56 output size: 28 output size: 14 output size: 7 output size: 1 VGG-19 34-layer plain 7x7 conv, 64, /2 pool,
3x3 conv, 512 3x3 conv, 512 3x3 conv, 512 3x3 conv, 512 pool, /2 fc 4096 fc 4096 fc 1000 image output size: 112 output size: 224 output size: 56 output size: 28 output size: 14 output size: 7 output size: 1 VGG-19 34-layer plain 7x7 conv, 64, /2 pool, /2 3x3 conv, 64 3x3 conv, 64 3x3 conv, 64 3x3 conv, 64 3x3 conv, 64 3x3 conv, 64 3x3 conv, 128, /2 3x3 conv, 128 3x3 conv, 128 3x3 conv, 128 3x3 conv, 128 3x3 conv, 128 3x3 conv, 128 3x3 conv, 128 3x3 conv, 256, /2 3x3 conv, 256 3x3 conv, 256 3x3 conv, 256 3x3 conv, 256 3x3 conv, 256 3x3 conv, 256 3x3 conv, 256 3x3 conv, 256 3x3 conv, 256 3x3 conv, 256 3x3 conv, 256 3x3 conv, 512, /2 3x3 conv, 512 3x3 conv, 512 3x3 conv, 512 3x3 conv, 512 3x3 conv, 512 avg pool fc 1000 image 34-layer residual Figure 3. Example network architectures for ImageNet. Left: the VGG-19 model [41] (19.6 billion FLOPs) as a reference. Mid- dle: a plain network with 34 parameter layers (3.6 billion FLOPs). Right: a residual network with 34 parameter layers (3.6 billion FLOPs). The dotted
avg pool fc 1000 image 34-layer residual Figure 3. Example network architectures for ImageNet. Left: the VGG-19 model [41] (19.6 billion FLOPs) as a reference. Mid- dle: a plain network with 34 parameter layers (3.6 billion FLOPs). Right: a residual network with 34 parameter layers (3.6 billion FLOPs). The dotted shortcuts increase dimensions. Table 1 shows more details and other variants. Residual Network. Based on the above plain network, we insert shortcut connections (Fig. 3, right) which turn the network into its counterpart residual version. The identity shortcuts (Eqn.(1)) can be directly used when the input and output are of the same dimensions (solid line shortcuts in Fig. 3). When the dimensions increase (dotted line shortcuts in Fig. 3), we consider two options: (A) The shortcut still performs identity mapping, with extra zero entries padded for increasing dimensions. This option introduces no extra parameter; (B) The projection shortcut in Eqn.(2) is used to match dimensions (done by 1√ó1 convolutions). For both options, when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2. 3.4. Implementation Our implementation for ImageNet follows the practice in [21, 41]. The image is resized with its shorter side
is used to match dimensions (done by 1√ó1 convolutions). For both options, when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2. 3.4. Implementation Our implementation for ImageNet follows the practice in [21, 41]. The image is resized with its shorter side ran- domly sampled in [256, 480] for scale augmentation [41]. A 224√ó224 crop is randomly sampled from an image or its horizontal Ô¨Çip, with the per-pixel mean subtracted [21]. The standard color augmentation in [21] is used. We adopt batch normalization (BN) [16] right after each convolution and before activation, following [16]. We initialize the weights as in [13] and train all plain/residual nets from scratch. We use SGD with a mini-batch size of 256. The learning rate starts from 0.1 and is divided by 10 when the error plateaus, and the models are trained for up to 60 √ó 104 iterations. We use a weight decay of 0.0001 and a momentum of 0.9. We do not use dropout [14], following the practice in [16]. In testing, for comparison studies we adopt the standard 10-crop testing [21]. For best results, we adopt the fully- convolutional form as in [41, 13],
104 iterations. We use a weight decay of 0.0001 and a momentum of 0.9. We do not use dropout [14], following the practice in [16]. In testing, for comparison studies we adopt the standard 10-crop testing [21]. For best results, we adopt the fully- convolutional form as in [41, 13], and average the scores at multiple scales (images are resized such that the shorter side is in {224, 256, 384, 480, 640}). 4. Experiments 4.1. ImageNet ClassiÔ¨Åcation We evaluate our method on the ImageNet 2012 classiÔ¨Å- cation dataset [36] that consists of 1000 classes. The models are trained on the 1.28 million training images, and evalu- ated on the 50k validation images. We also obtain a Ô¨Ånal result on the 100k test images, reported by the test server. We evaluate both top-1 and top-5 error rates. Plain Networks. We Ô¨Årst evaluate 18-layer and 34-layer plain nets. The 34-layer plain net is in Fig. 3 (middle). The 18-layer plain net is of a similar form. See Table 1 for de- tailed architectures. The results in Table 2 show that the deeper 34-layer plain net has higher validation error than the shallower 18-layer plain net. To reveal the reasons, in Fig. 4
is in Fig. 3 (middle). The 18-layer plain net is of a similar form. See Table 1 for de- tailed architectures. The results in Table 2 show that the deeper 34-layer plain net has higher validation error than the shallower 18-layer plain net. To reveal the reasons, in Fig. 4 (left) we com- pare their training/validation errors during the training pro- cedure. We have observed the degradation problem - the 4 layer name output size 18-layer 34-layer 50-layer 101-layer 152-layer conv1 112√ó112 7√ó7, 64, stride 2 conv2 x 56√ó56 3√ó3 max pool, stride 2  3√ó3, 64 3√ó3, 64  √ó2  3√ó3, 64 3√ó3, 64  √ó3 Ô£Æ Ô£∞ 1√ó1, 64 3√ó3, 64 1√ó1, 256 Ô£π Ô£ª√ó3 Ô£Æ Ô£∞ 1√ó1, 64 3√ó3, 64 1√ó1, 256 Ô£π Ô£ª√ó3 Ô£Æ Ô£∞ 1√ó1, 64 3√ó3, 64 1√ó1, 256 Ô£π Ô£ª√ó3 conv3 x 28√ó28  3√ó3, 128 3√ó3, 128  √ó2  3√ó3, 128 3√ó3, 128  √ó4 Ô£Æ Ô£∞ 1√ó1, 128 3√ó3, 128 1√ó1, 512 Ô£π Ô£ª√ó4 Ô£Æ Ô£∞ 1√ó1, 128 3√ó3, 128 1√ó1, 512 Ô£π Ô£ª√ó4 Ô£Æ Ô£∞ 1√ó1, 128 3√ó3, 128 1√ó1, 512 Ô£π Ô£ª√ó8 conv4 x 14√ó14  3√ó3, 256 3√ó3, 256  √ó2  3√ó3, 256 3√ó3, 256
128 3√ó3, 128  √ó4 Ô£Æ Ô£∞ 1√ó1, 128 3√ó3, 128 1√ó1, 512 Ô£π Ô£ª√ó4 Ô£Æ Ô£∞ 1√ó1, 128 3√ó3, 128 1√ó1, 512 Ô£π Ô£ª√ó4 Ô£Æ Ô£∞ 1√ó1, 128 3√ó3, 128 1√ó1, 512 Ô£π Ô£ª√ó8 conv4 x 14√ó14  3√ó3, 256 3√ó3, 256  √ó2  3√ó3, 256 3√ó3, 256  √ó6 Ô£Æ Ô£∞ 1√ó1, 256 3√ó3, 256 1√ó1, 1024 Ô£π Ô£ª√ó6 Ô£Æ Ô£∞ 1√ó1, 256 3√ó3, 256 1√ó1, 1024 Ô£π Ô£ª√ó23 Ô£Æ Ô£∞ 1√ó1, 256 3√ó3, 256 1√ó1, 1024 Ô£π Ô£ª√ó36 conv5 x 7√ó7  3√ó3, 512 3√ó3, 512  √ó2  3√ó3, 512 3√ó3, 512  √ó3 Ô£Æ Ô£∞ 1√ó1, 512 3√ó3, 512 1√ó1, 2048 Ô£π Ô£ª√ó3 Ô£Æ Ô£∞ 1√ó1, 512 3√ó3, 512 1√ó1, 2048 Ô£π Ô£ª√ó3 Ô£Æ Ô£∞ 1√ó1, 512 3√ó3, 512 1√ó1, 2048 Ô£π Ô£ª√ó3 1√ó1 average pool, 1000-d fc, softmax FLOPs 1.8√ó109 3.6√ó109 3.8√ó109 7.6√ó109 11.3√ó109 Table 1. Architectures for ImageNet. Building blocks are shown in brackets (see also Fig. 5), with the numbers of blocks stacked. Down- sampling is performed by conv3 1, conv4 1, and conv5 1 with a stride of 2. 0 10 20 30 40 50 20 30 40 50 60 iter. (1e4) error (%) plain-18 plain-34 0 10 20 30
in brackets (see also Fig. 5), with the numbers of blocks stacked. Down- sampling is performed by conv3 1, conv4 1, and conv5 1 with a stride of 2. 0 10 20 30 40 50 20 30 40 50 60 iter. (1e4) error (%) plain-18 plain-34 0 10 20 30 40 50 20 30 40 50 60 iter. (1e4) error (%) ResNet-18 ResNet-34 18-layer 34-layer 18-layer 34-layer Figure 4. Training on ImageNet. Thin curves denote training error, and bold curves denote validation error of the center crops. Left: plain networks of 18 and 34 layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to their plain counterparts. plain ResNet 18 layers 27.94 27.88 34 layers 28.54 25.03 Table 2. Top-1 error (%, 10-crop testing) on ImageNet validation. Here the ResNets have no extra parameter compared to their plain counterparts. Fig. 4 shows the training procedures. 34-layer plain net has higher training error throughout the whole training procedure, even though the solution space of the 18-layer plain network is a subspace of that of the 34-layer one. We argue that this optimization difÔ¨Åculty is unlikely to be caused by vanishing gradients.
4 shows the training procedures. 34-layer plain net has higher training error throughout the whole training procedure, even though the solution space of the 18-layer plain network is a subspace of that of the 34-layer one. We argue that this optimization difÔ¨Åculty is unlikely to be caused by vanishing gradients. These plain networks are trained with BN [16], which ensures forward propagated signals to have non-zero variances. We also verify that the backward propagated gradients exhibit healthy norms with BN. So neither forward nor backward signals vanish. In fact, the 34-layer plain net is still able to achieve compet- itive accuracy (Table 3), suggesting that the solver works to some extent. We conjecture that the deep plain nets may have exponentially low convergence rates, which impact the reducing of the training error3. The reason for such opti- mization difÔ¨Åculties will be studied in the future. Residual Networks. Next we evaluate 18-layer and 34- layer residual nets (ResNets). The baseline architectures are the same as the above plain nets, expect that a shortcut connection is added to each pair of 3√ó3 Ô¨Ålters as in Fig. 3 (right). In the Ô¨Årst comparison (Table 2 and Fig. 4 right), we use identity mapping
18-layer and 34- layer residual nets (ResNets). The baseline architectures are the same as the above plain nets, expect that a shortcut connection is added to each pair of 3√ó3 Ô¨Ålters as in Fig. 3 (right). In the Ô¨Årst comparison (Table 2 and Fig. 4 right), we use identity mapping for all shortcuts and zero-padding for increasing dimensions (option A). So they have no extra parameter compared to the plain counterparts. We have three major observations from Table 2 and Fig. 4. First, the situation is reversed with residual learn- ing ‚Äì the 34-layer ResNet is better than the 18-layer ResNet (by 2.8%). More importantly, the 34-layer ResNet exhibits considerably lower training error and is generalizable to the validation data. This indicates that the degradation problem is well addressed in this setting and we manage to obtain accuracy gains from increased depth. Second, compared to its plain counterpart, the 34-layer 3We have experimented with more training iterations (3√ó) and still ob- served the degradation problem, suggesting that this problem cannot be feasibly addressed by simply using more iterations. 5 model top-1 err. top-5 err. VGG-16 [41] 28.07 9.33 GoogLeNet [44] - 9.15 PReLU-net [13] 24.27 7.38 plain-34 28.54 10.02 ResNet-34
3We have experimented with more training iterations (3√ó) and still ob- served the degradation problem, suggesting that this problem cannot be feasibly addressed by simply using more iterations. 5 model top-1 err. top-5 err. VGG-16 [41] 28.07 9.33 GoogLeNet [44] - 9.15 PReLU-net [13] 24.27 7.38 plain-34 28.54 10.02 ResNet-34 A 25.03 7.76 ResNet-34 B 24.52 7.46 ResNet-34 C 24.19 7.40 ResNet-50 22.85 6.71 ResNet-101 21.75 6.05 ResNet-152 21.43 5.71 Table 3. Error rates (%, 10-crop testing) on ImageNet validation. VGG-16 is based on our test. ResNet-50/101/152 are of option B that only uses projections for increasing dimensions. method top-1 err. top-5 err. VGG [41] (ILSVRC‚Äô14) - 8.43‚Ä† GoogLeNet [44] (ILSVRC‚Äô14) - 7.89 VGG [41] (v5) 24.4 7.1 PReLU-net [13] 21.59 5.71 BN-inception [16] 21.99 5.81 ResNet-34 B 21.84 5.71 ResNet-34 C 21.53 5.60 ResNet-50 20.74 5.25 ResNet-101 19.87 4.60 ResNet-152 19.38 4.49 Table 4. Error rates (%) of single-model results on the ImageNet validation set (except ‚Ä† reported on the test set). method top-5 err. (test) VGG [41] (ILSVRC‚Äô14) 7.32 GoogLeNet [44] (ILSVRC‚Äô14) 6.66 VGG [41] (v5) 6.8 PReLU-net [13] 4.94 BN-inception [16] 4.82 ResNet (ILSVRC‚Äô15) 3.57 Table 5. Error rates (%) of ensembles. The top-5 error is on
results on the ImageNet validation set (except ‚Ä† reported on the test set). method top-5 err. (test) VGG [41] (ILSVRC‚Äô14) 7.32 GoogLeNet [44] (ILSVRC‚Äô14) 6.66 VGG [41] (v5) 6.8 PReLU-net [13] 4.94 BN-inception [16] 4.82 ResNet (ILSVRC‚Äô15) 3.57 Table 5. Error rates (%) of ensembles. The top-5 error is on the test set of ImageNet and reported by the test server. ResNet reduces the top-1 error by 3.5% (Table 2), resulting from the successfully reduced training error (Fig. 4 right vs. left). This comparison veriÔ¨Åes the effectiveness of residual learning on extremely deep systems. Last, we also note that the 18-layer plain/residual nets are comparably accurate (Table 2), but the 18-layer ResNet converges faster (Fig. 4 right vs. left). When the net is ‚Äúnot overly deep‚Äù (18 layers here), the current SGD solver is still able to Ô¨Ånd good solutions to the plain net. In this case, the ResNet eases the optimization by providing faster conver- gence at the early stage. Identity vs. Projection Shortcuts. We have shown that 3x3, 64 1x1, 64 relu 1x1, 256 relu relu 3x3, 64 3x3, 64 relu relu 64-d 256-d Figure 5. A deeper residual function F for ImageNet. Left: a building block (on
the optimization by providing faster conver- gence at the early stage. Identity vs. Projection Shortcuts. We have shown that 3x3, 64 1x1, 64 relu 1x1, 256 relu relu 3x3, 64 3x3, 64 relu relu 64-d 256-d Figure 5. A deeper residual function F for ImageNet. Left: a building block (on 56√ó56 feature maps) as in Fig. 3 for ResNet- 34. Right: a ‚Äúbottleneck‚Äù building block for ResNet-50/101/152. parameter-free, identity shortcuts help with training. Next we investigate projection shortcuts (Eqn.(2)). In Table 3 we compare three options: (A) zero-padding shortcuts are used for increasing dimensions, and all shortcuts are parameter- free (the same as Table 2 and Fig. 4 right); (B) projec- tion shortcuts are used for increasing dimensions, and other shortcuts are identity; and (C) all shortcuts are projections. Table 3 shows that all three options are considerably bet- ter than the plain counterpart. B is slightly better than A. We argue that this is because the zero-padded dimensions in A indeed have no residual learning. C is marginally better than B, and we attribute this to the extra parameters introduced by many (thirteen) projection shortcuts. But the small dif- ferences among A/B/C indicate that projection shortcuts are not essential
We argue that this is because the zero-padded dimensions in A indeed have no residual learning. C is marginally better than B, and we attribute this to the extra parameters introduced by many (thirteen) projection shortcuts. But the small dif- ferences among A/B/C indicate that projection shortcuts are not essential for addressing the degradation problem. So we do not use option C in the rest of this paper, to reduce mem- ory/time complexity and model sizes. Identity shortcuts are particularly important for not increasing the complexity of the bottleneck architectures that are introduced below. Deeper Bottleneck Architectures. Next we describe our deeper nets for ImageNet. Because of concerns on the train- ing time that we can afford, we modify the building block as a bottleneck design4. For each residual function F, we use a stack of 3 layers instead of 2 (Fig. 5). The three layers are 1√ó1, 3√ó3, and 1√ó1 convolutions, where the 1√ó1 layers are responsible for reducing and then increasing (restoring) dimensions, leaving the 3√ó3 layer a bottleneck with smaller input/output dimensions. Fig. 5 shows an example, where both designs have similar time complexity. The parameter-free identity shortcuts are particularly im- portant for the bottleneck architectures. If
1√ó1 convolutions, where the 1√ó1 layers are responsible for reducing and then increasing (restoring) dimensions, leaving the 3√ó3 layer a bottleneck with smaller input/output dimensions. Fig. 5 shows an example, where both designs have similar time complexity. The parameter-free identity shortcuts are particularly im- portant for the bottleneck architectures. If the identity short- cut in Fig. 5 (right) is replaced with projection, one can show that the time complexity and model size are doubled, as the shortcut is connected to the two high-dimensional ends. So identity shortcuts lead to more efÔ¨Åcient models for the bottleneck designs. 50-layer ResNet: We replace each 2-layer block in the 4Deeper non-bottleneck ResNets (e.g., Fig. 5 left) also gain accuracy from increased depth (as shown on CIFAR-10), but are not as economical as the bottleneck ResNets. So the usage of bottleneck designs is mainly due to practical considerations. We further note that the degradation problem of plain nets is also witnessed for the bottleneck designs. 6 34-layer net with this 3-layer bottleneck block, resulting in a 50-layer ResNet (Table 1). We use option B for increasing dimensions. This model has 3.8 billion FLOPs. 101-layer and 152-layer ResNets: We construct 101- layer and 152-layer ResNets by
of plain nets is also witnessed for the bottleneck designs. 6 34-layer net with this 3-layer bottleneck block, resulting in a 50-layer ResNet (Table 1). We use option B for increasing dimensions. This model has 3.8 billion FLOPs. 101-layer and 152-layer ResNets: We construct 101- layer and 152-layer ResNets by using more 3-layer blocks (Table 1). Remarkably, although the depth is signiÔ¨Åcantly increased, the 152-layer ResNet (11.3 billion FLOPs) still has lower complexity than VGG-16/19 nets (15.3/19.6 bil- lion FLOPs). The 50/101/152-layer ResNets are more accurate than the 34-layer ones by considerable margins (Table 3 and 4). We do not observe the degradation problem and thus en- joy signiÔ¨Åcant accuracy gains from considerably increased depth. The beneÔ¨Åts of depth are witnessed for all evaluation metrics (Table 3 and 4). Comparisons with State-of-the-art Methods. In Table 4 we compare with the previous best single-model results. Our baseline 34-layer ResNets have achieved very compet- itive accuracy. Our 152-layer ResNet has a single-model top-5 validation error of 4.49%. This single-model result outperforms all previous ensemble results (Table 5). We combine six models of different depth to form an ensemble (only with two 152-layer ones at the time of submitting). This leads to 3.57%
very compet- itive accuracy. Our 152-layer ResNet has a single-model top-5 validation error of 4.49%. This single-model result outperforms all previous ensemble results (Table 5). We combine six models of different depth to form an ensemble (only with two 152-layer ones at the time of submitting). This leads to 3.57% top-5 error on the test set (Table 5). This entry won the 1st place in ILSVRC 2015. 4.2. CIFAR-10 and Analysis We conducted more studies on the CIFAR-10 dataset [20], which consists of 50k training images and 10k test- ing images in 10 classes. We present experiments trained on the training set and evaluated on the test set. Our focus is on the behaviors of extremely deep networks, but not on pushing the state-of-the-art results, so we intentionally use simple architectures as follows. The plain/residual architectures follow the form in Fig. 3 (middle/right). The network inputs are 32√ó32 images, with the per-pixel mean subtracted. The Ô¨Årst layer is 3√ó3 convo- lutions. Then we use a stack of 6n layers with 3√ó3 convo- lutions on the feature maps of sizes {32, 16, 8} respectively, with 2n layers for each feature map size. The numbers of Ô¨Ålters are {16, 32, 64} respectively.
the per-pixel mean subtracted. The Ô¨Årst layer is 3√ó3 convo- lutions. Then we use a stack of 6n layers with 3√ó3 convo- lutions on the feature maps of sizes {32, 16, 8} respectively, with 2n layers for each feature map size. The numbers of Ô¨Ålters are {16, 32, 64} respectively. The subsampling is per- formed by convolutions with a stride of 2. The network ends with a global average pooling, a 10-way fully-connected layer, and softmax. There are totally 6n+2 stacked weighted layers. The following table summarizes the architecture: output map size 32√ó32 16√ó16 8√ó8 # layers 1+2n 2n 2n # Ô¨Ålters 16 32 64 When shortcut connections are used, they are connected to the pairs of 3√ó3 layers (totally 3n shortcuts). On this dataset we use identity shortcuts in all cases (i.e., option A), method error (%) Maxout [10] 9.38 NIN [25] 8.81 DSN [24] 8.22 # layers # params FitNet [35] 19 2.5M 8.39 Highway [42, 43] 19 2.3M 7.54 (7.72¬±0.16) Highway [42, 43] 32 1.25M 8.80 ResNet 20 0.27M 8.75 ResNet 32 0.46M 7.51 ResNet 44 0.66M 7.17 ResNet 56 0.85M 6.97 ResNet 110 1.7M 6.43 (6.61¬±0.16) ResNet 1202 19.4M 7.93 Table 6. ClassiÔ¨Åcation error on the
params FitNet [35] 19 2.5M 8.39 Highway [42, 43] 19 2.3M 7.54 (7.72¬±0.16) Highway [42, 43] 32 1.25M 8.80 ResNet 20 0.27M 8.75 ResNet 32 0.46M 7.51 ResNet 44 0.66M 7.17 ResNet 56 0.85M 6.97 ResNet 110 1.7M 6.43 (6.61¬±0.16) ResNet 1202 19.4M 7.93 Table 6. ClassiÔ¨Åcation error on the CIFAR-10 test set. All meth- ods are with data augmentation. For ResNet-110, we run it 5 times and show ‚Äúbest (mean¬±std)‚Äù as in [43]. so our residual models have exactly the same depth, width, and number of parameters as the plain counterparts. We use a weight decay of 0.0001 and momentum of 0.9, and adopt the weight initialization in [13] and BN [16] but with no dropout. These models are trained with a mini- batch size of 128 on two GPUs. We start with a learning rate of 0.1, divide it by 10 at 32k and 48k iterations, and terminate training at 64k iterations, which is determined on a 45k/5k train/val split. We follow the simple data augmen- tation in [24] for training: 4 pixels are padded on each side, and a 32√ó32 crop is randomly sampled from the padded image or its horizontal Ô¨Çip. For testing, we only evaluate
training at 64k iterations, which is determined on a 45k/5k train/val split. We follow the simple data augmen- tation in [24] for training: 4 pixels are padded on each side, and a 32√ó32 crop is randomly sampled from the padded image or its horizontal Ô¨Çip. For testing, we only evaluate the single view of the original 32√ó32 image. We compare n = {3, 5, 7, 9}, leading to 20, 32, 44, and 56-layer networks. Fig. 6 (left) shows the behaviors of the plain nets. The deep plain nets suffer from increased depth, and exhibit higher training error when going deeper. This phenomenon is similar to that on ImageNet (Fig. 4, left) and on MNIST (see [42]), suggesting that such an optimization difÔ¨Åculty is a fundamental problem. Fig. 6 (middle) shows the behaviors of ResNets. Also similar to the ImageNet cases (Fig. 4, right), our ResNets manage to overcome the optimization difÔ¨Åculty and demon- strate accuracy gains when the depth increases. We further explore n = 18 that leads to a 110-layer ResNet. In this case, we Ô¨Ånd that the initial learning rate of 0.1 is slightly too large to start converging5. So we use 0.01 to warm up the training
difÔ¨Åculty and demon- strate accuracy gains when the depth increases. We further explore n = 18 that leads to a 110-layer ResNet. In this case, we Ô¨Ånd that the initial learning rate of 0.1 is slightly too large to start converging5. So we use 0.01 to warm up the training until the training error is below 80% (about 400 iterations), and then go back to 0.1 and con- tinue training. The rest of the learning schedule is as done previously. This 110-layer network converges well (Fig. 6, middle). It has fewer parameters than other deep and thin 5With an initial learning rate of 0.1, it starts converging (<90% error) after several epochs, but still reaches similar accuracy. 7 0 1 2 3 4 5 6 0 5 10 20 iter. (1e4) error (%) plain-20 plain-32 plain-44 plain-56 0 1 2 3 4 5 6 0 5 10 20 iter. (1e4) error (%) ResNet-20 ResNet-32 ResNet-44 ResNet-56 ResNet-110 56-layer 20-layer 110-layer 20-layer 4 5 6 0 1 5 10 20 iter. (1e4) error (%) residual-110 residual-1202 Figure 6. Training on CIFAR-10. Dashed lines denote training error, and bold lines denote testing error. Left: plain networks. The error of plain-110 is higher
error (%) ResNet-20 ResNet-32 ResNet-44 ResNet-56 ResNet-110 56-layer 20-layer 110-layer 20-layer 4 5 6 0 1 5 10 20 iter. (1e4) error (%) residual-110 residual-1202 Figure 6. Training on CIFAR-10. Dashed lines denote training error, and bold lines denote testing error. Left: plain networks. The error of plain-110 is higher than 60% and not displayed. Middle: ResNets. Right: ResNets with 110 and 1202 layers. 0 20 40 60 80 100 1 2 3 layer index (sorted by magnitude) std plain-20 plain-56 ResNet-20 ResNet-56 ResNet-110 0 20 40 60 80 100 1 2 3 layer index (original) std plain-20 plain-56 ResNet-20 ResNet-56 ResNet-110 Figure 7. Standard deviations (std) of layer responses on CIFAR- 10. The responses are the outputs of each 3√ó3 layer, after BN and before nonlinearity. Top: the layers are shown in their original order. Bottom: the responses are ranked in descending order. networks such as FitNet [35] and Highway [42] (Table 6), yet is among the state-of-the-art results (6.43%, Table 6). Analysis of Layer Responses. Fig. 7 shows the standard deviations (std) of the layer responses. The responses are the outputs of each 3√ó3 layer, after BN and before other nonlinearity (ReLU/addition). For ResNets, this analy- sis reveals
Highway [42] (Table 6), yet is among the state-of-the-art results (6.43%, Table 6). Analysis of Layer Responses. Fig. 7 shows the standard deviations (std) of the layer responses. The responses are the outputs of each 3√ó3 layer, after BN and before other nonlinearity (ReLU/addition). For ResNets, this analy- sis reveals the response strength of the residual functions. Fig. 7 shows that ResNets have generally smaller responses than their plain counterparts. These results support our ba- sic motivation (Sec.3.1) that the residual functions might be generally closer to zero than the non-residual functions. We also notice that the deeper ResNet has smaller magni- tudes of responses, as evidenced by the comparisons among ResNet-20, 56, and 110 in Fig. 7. When there are more layers, an individual layer of ResNets tends to modify the signal less. Exploring Over 1000 layers. We explore an aggressively deep model of over 1000 layers. We set n = 200 that leads to a 1202-layer network, which is trained as described above. Our method shows no optimization difÔ¨Åculty, and this 103-layer network is able to achieve training error <0.1% (Fig. 6, right). Its test error is still fairly good (7.93%, Table 6). But there are still open
n = 200 that leads to a 1202-layer network, which is trained as described above. Our method shows no optimization difÔ¨Åculty, and this 103-layer network is able to achieve training error <0.1% (Fig. 6, right). Its test error is still fairly good (7.93%, Table 6). But there are still open problems on such aggressively deep models. The testing result of this 1202-layer network is worse than that of our 110-layer network, although both training data 07+12 07++12 test data VOC 07 test VOC 12 test VGG-16 73.2 70.4 ResNet-101 76.4 73.8 Table 7. Object detection mAP (%) on the PASCAL VOC 2007/2012 test sets using baseline Faster R-CNN. See also Ta- ble 10 and 11 for better results. metric mAP@.5 mAP@[.5, .95] VGG-16 41.5 21.2 ResNet-101 48.4 27.2 Table 8. Object detection mAP (%) on the COCO validation set using baseline Faster R-CNN. See also Table 9 for better results. have similar training error. We argue that this is because of overÔ¨Åtting. The 1202-layer network may be unnecessarily large (19.4M) for this small dataset. Strong regularization such as maxout [10] or dropout [14] is applied to obtain the best results ([10, 25, 24, 35]) on this dataset. In this paper,
have similar training error. We argue that this is because of overÔ¨Åtting. The 1202-layer network may be unnecessarily large (19.4M) for this small dataset. Strong regularization such as maxout [10] or dropout [14] is applied to obtain the best results ([10, 25, 24, 35]) on this dataset. In this paper, we use no maxout/dropout and just simply impose regular- ization via deep and thin architectures by design, without distracting from the focus on the difÔ¨Åculties of optimiza- tion. But combining with stronger regularization may im- prove results, which we will study in the future. 4.3. Object Detection on PASCAL and MS COCO Our method has good generalization performance on other recognition tasks. Table 7 and 8 show the object de- tection baseline results on PASCAL VOC 2007 and 2012 [5] and COCO [26]. We adopt Faster R-CNN [32] as the de- tection method. Here we are interested in the improvements of replacing VGG-16 [41] with ResNet-101. The detection implementation (see appendix) of using both models is the same, so the gains can only be attributed to better networks. Most remarkably, on the challenging COCO dataset we ob- tain a 6.0% increase in COCO‚Äôs standard metric (mAP@[.5, .95]), which is a
of replacing VGG-16 [41] with ResNet-101. The detection implementation (see appendix) of using both models is the same, so the gains can only be attributed to better networks. Most remarkably, on the challenging COCO dataset we ob- tain a 6.0% increase in COCO‚Äôs standard metric (mAP@[.5, .95]), which is a 28% relative improvement. This gain is solely due to the learned representations. Based on deep residual nets, we won the 1st places in several tracks in ILSVRC & COCO 2015 competitions: Im- ageNet detection, ImageNet localization, COCO detection, and COCO segmentation. The details are in the appendix. 8 References [1] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependen- cies with gradient descent is difÔ¨Åcult. IEEE Transactions on Neural Networks, 5(2):157‚Äì166, 1994. [2] C. M. Bishop. Neural networks for pattern recognition. Oxford university press, 1995. [3] W. L. Briggs, S. F. McCormick, et al. A Multigrid Tutorial. Siam, 2000. [4] K. ChatÔ¨Åeld, V. Lempitsky, A. Vedaldi, and A. Zisserman. The devil is in the details: an evaluation of recent feature encoding methods. In BMVC, 2011. [5] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis- serman. The Pascal Visual Object Classes (VOC) Challenge. IJCV, pages
[4] K. ChatÔ¨Åeld, V. Lempitsky, A. Vedaldi, and A. Zisserman. The devil is in the details: an evaluation of recent feature encoding methods. In BMVC, 2011. [5] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis- serman. The Pascal Visual Object Classes (VOC) Challenge. IJCV, pages 303‚Äì338, 2010. [6] S. Gidaris and N. Komodakis. Object detection via a multi-region & semantic segmentation-aware cnn model. In ICCV, 2015. [7] R. Girshick. Fast R-CNN. In ICCV, 2015. [8] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hier- archies for accurate object detection and semantic segmentation. In CVPR, 2014. [9] X. Glorot and Y. Bengio. Understanding the difÔ¨Åculty of training deep feedforward neural networks. In AISTATS, 2010. [10] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio. Maxout networks. arXiv:1302.4389, 2013. [11] K. He and J. Sun. Convolutional neural networks at constrained time cost. In CVPR, 2015. [12] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV, 2014. [13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectiÔ¨Åers: Surpassing human-level performance on imagenet classiÔ¨Åcation. In ICCV, 2015.
cost. In CVPR, 2015. [12] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV, 2014. [13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectiÔ¨Åers: Surpassing human-level performance on imagenet classiÔ¨Åcation. In ICCV, 2015. [14] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving neural networks by preventing co- adaptation of feature detectors. arXiv:1207.0580, 2012. [15] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735‚Äì1780, 1997. [16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. [17] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. TPAMI, 33, 2011. [18] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and C. Schmid. Aggregating local image descriptors into compact codes. TPAMI, 2012. [19] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv:1408.5093, 2014. [20] A. Krizhevsky. Learning multiple layers of features from tiny im- ages. Tech Report, 2009. [21] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet
Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv:1408.5093, 2014. [20] A. Krizhevsky. Learning multiple layers of features from tiny im- ages. Tech Report, 2009. [21] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiÔ¨Åcation with deep convolutional neural networks. In NIPS, 2012. [22] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to hand- written zip code recognition. Neural computation, 1989. [23] Y. LeCun, L. Bottou, G. B. Orr, and K.-R. M¬®uller. EfÔ¨Åcient backprop. In Neural Networks: Tricks of the Trade, pages 9‚Äì50. Springer, 1998. [24] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply- supervised nets. arXiv:1409.5185, 2014. [25] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv:1312.4400, 2013. [26] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll¬¥ar, and C. L. Zitnick. Microsoft COCO: Common objects in context. In ECCV. 2014. [27] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015. [28] G. Mont¬¥ufar, R. Pascanu, K. Cho, and Y. Bengio. On the
J. Hays, P. Perona, D. Ramanan, P. Doll¬¥ar, and C. L. Zitnick. Microsoft COCO: Common objects in context. In ECCV. 2014. [27] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015. [28] G. Mont¬¥ufar, R. Pascanu, K. Cho, and Y. Bengio. On the number of linear regions of deep neural networks. In NIPS, 2014. [29] V. Nair and G. E. Hinton. RectiÔ¨Åed linear units improve restricted boltzmann machines. In ICML, 2010. [30] F. Perronnin and C. Dance. Fisher kernels on visual vocabularies for image categorization. In CVPR, 2007. [31] T. Raiko, H. Valpola, and Y. LeCun. Deep learning made easier by linear transformations in perceptrons. In AISTATS, 2012. [32] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In NIPS, 2015. [33] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun. Object detection networks on convolutional feature maps. arXiv:1504.06066, 2015. [34] B. D. Ripley. Pattern recognition and neural networks. Cambridge university press, 1996. [35] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio. Fitnets: Hints for thin deep nets. In ICLR, 2015. [36] O. Russakovsky, J.
Object detection networks on convolutional feature maps. arXiv:1504.06066, 2015. [34] B. D. Ripley. Pattern recognition and neural networks. Cambridge university press, 1996. [35] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio. Fitnets: Hints for thin deep nets. In ICLR, 2015. [36] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. arXiv:1409.0575, 2014. [37] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv:1312.6120, 2013. [38] N. N. Schraudolph. Accelerated gradient descent by factor-centering decomposition. Technical report, 1998. [39] N. N. Schraudolph. Centering neural network gradient factors. In Neural Networks: Tricks of the Trade, pages 207‚Äì226. Springer, 1998. [40] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. Le- Cun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014. [41] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. [42] R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway networks. arXiv:1505.00387, 2015. [43] R. K. Srivastava, K. Greff, and J.
Overfeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014. [41] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. [42] R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway networks. arXiv:1505.00387, 2015. [43] R. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep networks. 1507.06228, 2015. [44] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Er- han, V. Vanhoucke, and A. Rabinovich. Going deeper with convolu- tions. In CVPR, 2015. [45] R. Szeliski. Fast surface interpolation using hierarchical basis func- tions. TPAMI, 1990. [46] R. Szeliski. Locally adapted hierarchical basis preconditioning. In SIGGRAPH, 2006. [47] T. Vatanen, T. Raiko, H. Valpola, and Y. LeCun. Pushing stochas- tic gradient towards second-order methods‚Äìbackpropagation learn- ing with transformations in nonlinearities. In Neural Information Processing, 2013. [48] A. Vedaldi and B. Fulkerson. VLFeat: An open and portable library of computer vision algorithms, 2008. [49] W. Venables and B. Ripley. Modern applied statistics with s-plus. 1999. [50] M. D. Zeiler and R. Fergus. Visualizing and understanding convolu- tional neural networks. In ECCV, 2014. 9 A. Object Detection Baselines In this section we introduce our detection method based on the
of computer vision algorithms, 2008. [49] W. Venables and B. Ripley. Modern applied statistics with s-plus. 1999. [50] M. D. Zeiler and R. Fergus. Visualizing and understanding convolu- tional neural networks. In ECCV, 2014. 9 A. Object Detection Baselines In this section we introduce our detection method based on the baseline Faster R-CNN [32] system. The models are initialized by the ImageNet classiÔ¨Åcation models, and then Ô¨Åne-tuned on the object detection data. We have experi- mented with ResNet-50/101 at the time of the ILSVRC & COCO 2015 detection competitions. Unlike VGG-16 used in [32], our ResNet has no hidden fc layers. We adopt the idea of ‚ÄúNetworks on Conv fea- ture maps‚Äù (NoC) [33] to address this issue. We compute the full-image shared conv feature maps using those lay- ers whose strides on the image are no greater than 16 pixels (i.e., conv1, conv2 x, conv3 x, and conv4 x, totally 91 conv layers in ResNet-101; Table 1). We consider these layers as analogous to the 13 conv layers in VGG-16, and by doing so, both ResNet and VGG-16 have conv feature maps of the same total stride (16 pixels). These layers are shared by a region proposal network (RPN,
totally 91 conv layers in ResNet-101; Table 1). We consider these layers as analogous to the 13 conv layers in VGG-16, and by doing so, both ResNet and VGG-16 have conv feature maps of the same total stride (16 pixels). These layers are shared by a region proposal network (RPN, generating 300 proposals) [32] and a Fast R-CNN detection network [7]. RoI pool- ing [7] is performed before conv5 1. On this RoI-pooled feature, all layers of conv5 x and up are adopted for each region, playing the roles of VGG-16‚Äôs fc layers. The Ô¨Ånal classiÔ¨Åcation layer is replaced by two sibling layers (classi- Ô¨Åcation and box regression [7]). For the usage of BN layers, after pre-training, we com- pute the BN statistics (means and variances) for each layer on the ImageNet training set. Then the BN layers are Ô¨Åxed during Ô¨Åne-tuning for object detection. As such, the BN layers become linear activations with constant offsets and scales, and BN statistics are not updated by Ô¨Åne-tuning. We Ô¨Åx the BN layers mainly for reducing memory consumption in Faster R-CNN training. PASCAL VOC Following [7, 32], for the PASCAL VOC 2007 test set, we use the 5k trainval images in VOC
become linear activations with constant offsets and scales, and BN statistics are not updated by Ô¨Åne-tuning. We Ô¨Åx the BN layers mainly for reducing memory consumption in Faster R-CNN training. PASCAL VOC Following [7, 32], for the PASCAL VOC 2007 test set, we use the 5k trainval images in VOC 2007 and 16k train- val images in VOC 2012 for training (‚Äú07+12‚Äù). For the PASCAL VOC 2012 test set, we use the 10k trainval+test images in VOC 2007 and 16k trainval images in VOC 2012 for training (‚Äú07++12‚Äù). The hyper-parameters for train- ing Faster R-CNN are the same as in [32]. Table 7 shows the results. ResNet-101 improves the mAP by >3% over VGG-16. This gain is solely because of the improved fea- tures learned by ResNet. MS COCO The MS COCO dataset [26] involves 80 object cate- gories. We evaluate the PASCAL VOC metric (mAP @ IoU = 0.5) and the standard COCO metric (mAP @ IoU = .5:.05:.95). We use the 80k images on the train set for train- ing and the 40k images on the val set for evaluation. Our detection system for COCO is similar to that for PASCAL VOC. We train the COCO models with
and the standard COCO metric (mAP @ IoU = .5:.05:.95). We use the 80k images on the train set for train- ing and the 40k images on the val set for evaluation. Our detection system for COCO is similar to that for PASCAL VOC. We train the COCO models with an 8-GPU imple- mentation, and thus the RPN step has a mini-batch size of 8 images (i.e., 1 per GPU) and the Fast R-CNN step has a mini-batch size of 16 images. The RPN step and Fast R- CNN step are both trained for 240k iterations with a learn- ing rate of 0.001 and then for 80k iterations with 0.0001. Table 8 shows the results on the MS COCO validation set. ResNet-101 has a 6% increase of mAP@[.5, .95] over VGG-16, which is a 28% relative improvement, solely con- tributed by the features learned by the better network. Re- markably, the mAP@[.5, .95]‚Äôs absolute increase (6.0%) is nearly as big as mAP@.5‚Äôs (6.9%). This suggests that a deeper network can improve both recognition and localiza- tion. B. Object Detection Improvements For completeness, we report the improvements made for the competitions. These improvements are based on deep features and thus should
the mAP@[.5, .95]‚Äôs absolute increase (6.0%) is nearly as big as mAP@.5‚Äôs (6.9%). This suggests that a deeper network can improve both recognition and localiza- tion. B. Object Detection Improvements For completeness, we report the improvements made for the competitions. These improvements are based on deep features and thus should beneÔ¨Åt from residual learning. MS COCO Box reÔ¨Ånement. Our box reÔ¨Ånement partially follows the it- erative localization in [6]. In Faster R-CNN, the Ô¨Ånal output is a regressed box that is different from its proposal box. So for inference, we pool a new feature from the regressed box and obtain a new classiÔ¨Åcation score and a new regressed box. We combine these 300 new predictions with the orig- inal 300 predictions. Non-maximum suppression (NMS) is applied on the union set of predicted boxes using an IoU threshold of 0.3 [8], followed by box voting [6]. Box re- Ô¨Ånement improves mAP by about 2 points (Table 9). Global context. We combine global context in the Fast R-CNN step. Given the full-image conv feature map, we pool a feature by global Spatial Pyramid Pooling [12] (with a ‚Äúsingle-level‚Äù pyramid) which can be implemented as ‚ÄúRoI‚Äù pooling using the entire image‚Äôs bounding box
by about 2 points (Table 9). Global context. We combine global context in the Fast R-CNN step. Given the full-image conv feature map, we pool a feature by global Spatial Pyramid Pooling [12] (with a ‚Äúsingle-level‚Äù pyramid) which can be implemented as ‚ÄúRoI‚Äù pooling using the entire image‚Äôs bounding box as the RoI. This pooled feature is fed into the post-RoI layers to obtain a global context feature. This global feature is con- catenated with the original per-region feature, followed by the sibling classiÔ¨Åcation and box regression layers. This new structure is trained end-to-end. Global context im- proves mAP@.5 by about 1 point (Table 9). Multi-scale testing. In the above, all results are obtained by single-scale training/testing as in [32], where the image‚Äôs shorter side is s = 600 pixels. Multi-scale training/testing has been developed in [12, 7] by selecting a scale from a feature pyramid, and in [33] by using maxout layers. In our current implementation, we have performed multi-scale testing following [33]; we have not performed multi-scale training because of limited time. In addition, we have per- formed multi-scale testing only for the Fast R-CNN step (but not yet for the RPN step). With a trained model, we
using maxout layers. In our current implementation, we have performed multi-scale testing following [33]; we have not performed multi-scale training because of limited time. In addition, we have per- formed multi-scale testing only for the Fast R-CNN step (but not yet for the RPN step). With a trained model, we compute conv feature maps on an image pyramid, where the image‚Äôs shorter sides are s ‚àà{200, 400, 600, 800, 1000}. 10 training data COCO train COCO trainval test data COCO val COCO test-dev mAP @.5 @[.5, .95] @.5 @[.5, .95] baseline Faster R-CNN (VGG-16) 41.5 21.2 baseline Faster R-CNN (ResNet-101) 48.4 27.2 +box reÔ¨Ånement 49.9 29.9 +context 51.1 30.0 53.3 32.2 +multi-scale testing 53.8 32.5 55.7 34.9 ensemble 59.0 37.4 Table 9. Object detection improvements on MS COCO using Faster R-CNN and ResNet-101. system net data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv baseline VGG-16 07+12 73.2 76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6 baseline ResNet-101 07+12 76.4 79.8 80.7 76.2 68.3 55.9 85.1 85.3 89.8 56.7 87.8 69.4 88.3 88.9 80.9 78.4
mbike person plant sheep sofa train tv baseline VGG-16 07+12 73.2 76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6 baseline ResNet-101 07+12 76.4 79.8 80.7 76.2 68.3 55.9 85.1 85.3 89.8 56.7 87.8 69.4 88.3 88.9 80.9 78.4 41.7 78.6 79.8 85.3 72.0 baseline+++ ResNet-101 COCO+07+12 85.6 90.0 89.6 87.8 80.8 76.1 89.9 89.9 89.6 75.5 90.0 80.7 89.6 90.3 89.1 88.7 65.4 88.1 85.6 89.0 86.8 Table 10. Detection results on the PASCAL VOC 2007 test set. The baseline is the Faster R-CNN system. The system ‚Äúbaseline+++‚Äù include box reÔ¨Ånement, context, and multi-scale testing in Table 9. system net data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv baseline VGG-16 07++12 70.4 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5 baseline ResNet-101 07++12 73.8 86.5 81.6 77.2 58.0 51.0 78.6 76.6 93.2 48.6 80.4 59.0 92.1 85.3 84.8 80.7 48.1 77.3 66.5 84.7 65.6 baseline+++ ResNet-101 COCO+07++12 83.8 92.1 88.4 84.8 75.9 71.4 86.3 87.8 94.2 66.8 89.4 69.2 93.9 91.9 90.9
81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5 baseline ResNet-101 07++12 73.8 86.5 81.6 77.2 58.0 51.0 78.6 76.6 93.2 48.6 80.4 59.0 92.1 85.3 84.8 80.7 48.1 77.3 66.5 84.7 65.6 baseline+++ ResNet-101 COCO+07++12 83.8 92.1 88.4 84.8 75.9 71.4 86.3 87.8 94.2 66.8 89.4 69.2 93.9 91.9 90.9 89.6 67.9 88.2 76.8 90.3 80.0 Table 11. Detection results on the PASCAL VOC 2012 test set (http://host.robots.ox.ac.uk:8080/leaderboard/ displaylb.php?challengeid=11&compid=4). The baseline is the Faster R-CNN system. The system ‚Äúbaseline+++‚Äù include box reÔ¨Ånement, context, and multi-scale testing in Table 9. We select two adjacent scales from the pyramid following [33]. RoI pooling and subsequent layers are performed on the feature maps of these two scales [33], which are merged by maxout as in [33]. Multi-scale testing improves the mAP by over 2 points (Table 9). Using validation data. Next we use the 80k+40k trainval set for training and the 20k test-dev set for evaluation. The test- dev set has no publicly available ground truth and the result is reported by the evaluation server. Under this setting, the results are an mAP@.5 of 55.7% and an mAP@[.5, .95] of 34.9% (Table 9). This is our single-model result. Ensemble. In Faster R-CNN, the
set for evaluation. The test- dev set has no publicly available ground truth and the result is reported by the evaluation server. Under this setting, the results are an mAP@.5 of 55.7% and an mAP@[.5, .95] of 34.9% (Table 9). This is our single-model result. Ensemble. In Faster R-CNN, the system is designed to learn region proposals and also object classiÔ¨Åers, so an ensemble can be used to boost both tasks. We use an ensemble for proposing regions, and the union set of proposals are pro- cessed by an ensemble of per-region classiÔ¨Åers. Table 9 shows our result based on an ensemble of 3 networks. The mAP is 59.0% and 37.4% on the test-dev set. This result won the 1st place in the detection task in COCO 2015. PASCAL VOC We revisit the PASCAL VOC dataset based on the above model. With the single model on the COCO dataset (55.7% mAP@.5 in Table 9), we Ô¨Åne-tune this model on the PAS- CAL VOC sets. The improvements of box reÔ¨Ånement, con- text, and multi-scale testing are also adopted. By doing so val2 test GoogLeNet [44] (ILSVRC‚Äô14) - 43.9 our single model (ILSVRC‚Äô15) 60.5 58.8 our ensemble (ILSVRC‚Äô15) 63.6 62.1 Table 12.
mAP@.5 in Table 9), we Ô¨Åne-tune this model on the PAS- CAL VOC sets. The improvements of box reÔ¨Ånement, con- text, and multi-scale testing are also adopted. By doing so val2 test GoogLeNet [44] (ILSVRC‚Äô14) - 43.9 our single model (ILSVRC‚Äô15) 60.5 58.8 our ensemble (ILSVRC‚Äô15) 63.6 62.1 Table 12. Our results (mAP, %) on the ImageNet detection dataset. Our detection system is Faster R-CNN [32] with the improvements in Table 9, using ResNet-101. we achieve 85.6% mAP on PASCAL VOC 2007 (Table 10) and 83.8% on PASCAL VOC 2012 (Table 11)6. The result on PASCAL VOC 2012 is 10 points higher than the previ- ous state-of-the-art result [6]. ImageNet Detection The ImageNet Detection (DET) task involves 200 object categories. The accuracy is evaluated by mAP@.5. Our object detection algorithm for ImageNet DET is the same as that for MS COCO in Table 9. The networks are pre- trained on the 1000-class ImageNet classiÔ¨Åcation set, and are Ô¨Åne-tuned on the DET data. We split the validation set into two parts (val1/val2) following [8]. We Ô¨Åne-tune the detection models using the DET training set and the val1 set. The val2 set is used for validation. We do not use other ILSVRC
the 1000-class ImageNet classiÔ¨Åcation set, and are Ô¨Åne-tuned on the DET data. We split the validation set into two parts (val1/val2) following [8]. We Ô¨Åne-tune the detection models using the DET training set and the val1 set. The val2 set is used for validation. We do not use other ILSVRC 2015 data. Our single model with ResNet-101 has 6http://host.robots.ox.ac.uk:8080/anonymous/3OJ4OJ.html, submitted on 2015-11-26. 11 LOC method LOC network testing LOC error on GT CLS classiÔ¨Åcation network top-5 LOC error on predicted CLS VGG‚Äôs [41] VGG-16 1-crop 33.1 [41] RPN ResNet-101 1-crop 13.3 RPN ResNet-101 dense 11.7 RPN ResNet-101 dense ResNet-101 14.4 RPN+RCNN ResNet-101 dense ResNet-101 10.6 RPN+RCNN ensemble dense ensemble 8.9 Table 13. Localization error (%) on the ImageNet validation. In the column of ‚ÄúLOC error on GT class‚Äù ([41]), the ground truth class is used. In the ‚Äútesting‚Äù column, ‚Äú1-crop‚Äù denotes testing on a center crop of 224√ó224 pixels, ‚Äúdense‚Äù denotes dense (fully convolutional) and multi-scale testing. 58.8% mAP and our ensemble of 3 models has 62.1% mAP on the DET test set (Table 12). This result won the 1st place in the ImageNet detection task in ILSVRC 2015, surpassing the second place by 8.5 points (absolute). C. ImageNet Localization
denotes dense (fully convolutional) and multi-scale testing. 58.8% mAP and our ensemble of 3 models has 62.1% mAP on the DET test set (Table 12). This result won the 1st place in the ImageNet detection task in ILSVRC 2015, surpassing the second place by 8.5 points (absolute). C. ImageNet Localization The ImageNet Localization (LOC) task [36] requires to classify and localize the objects. Following [40, 41], we assume that the image-level classiÔ¨Åers are Ô¨Årst adopted for predicting the class labels of an image, and the localiza- tion algorithm only accounts for predicting bounding boxes based on the predicted classes. We adopt the ‚Äúper-class re- gression‚Äù (PCR) strategy [40, 41], learning a bounding box regressor for each class. We pre-train the networks for Im- ageNet classiÔ¨Åcation and then Ô¨Åne-tune them for localiza- tion. We train networks on the provided 1000-class Ima- geNet training set. Our localization algorithm is based on the RPN frame- work of [32] with a few modiÔ¨Åcations. Unlike the way in [32] that is category-agnostic, our RPN for localization is designed in a per-class form. This RPN ends with two sib- ling 1√ó1 convolutional layers for binary classiÔ¨Åcation (cls) and box regression (reg), as in [32]. The cls
RPN frame- work of [32] with a few modiÔ¨Åcations. Unlike the way in [32] that is category-agnostic, our RPN for localization is designed in a per-class form. This RPN ends with two sib- ling 1√ó1 convolutional layers for binary classiÔ¨Åcation (cls) and box regression (reg), as in [32]. The cls and reg layers are both in a per-class from, in contrast to [32]. SpeciÔ¨Å- cally, the cls layer has a 1000-d output, and each dimension is binary logistic regression for predicting being or not be- ing an object class; the reg layer has a 1000√ó4-d output consisting of box regressors for 1000 classes. As in [32], our bounding box regression is with reference to multiple translation-invariant ‚Äúanchor‚Äù boxes at each position. As in our ImageNet classiÔ¨Åcation training (Sec. 3.4), we randomly sample 224√ó224 crops for data augmentation. We use a mini-batch size of 256 images for Ô¨Åne-tuning. To avoid negative samples being dominate, 8 anchors are ran- domly sampled for each image, where the sampled positive and negative anchors have a ratio of 1:1 [32]. For testing, the network is applied on the image fully-convolutionally. Table 13 compares the localization results. Following [41], we Ô¨Årst perform ‚Äúoracle‚Äù testing using the
samples being dominate, 8 anchors are ran- domly sampled for each image, where the sampled positive and negative anchors have a ratio of 1:1 [32]. For testing, the network is applied on the image fully-convolutionally. Table 13 compares the localization results. Following [41], we Ô¨Årst perform ‚Äúoracle‚Äù testing using the ground truth class as the classiÔ¨Åcation prediction. VGG‚Äôs paper [41] re- method top-5 localization err val test OverFeat [40] (ILSVRC‚Äô13) 30.0 29.9 GoogLeNet [44] (ILSVRC‚Äô14) - 26.7 VGG [41] (ILSVRC‚Äô14) 26.9 25.3 ours (ILSVRC‚Äô15) 8.9 9.0 Table 14. Comparisons of localization error (%) on the ImageNet dataset with state-of-the-art methods. ports a center-crop error of 33.1% (Table 13) using ground truth classes. Under the same setting, our RPN method us- ing ResNet-101 net signiÔ¨Åcantly reduces the center-crop er- ror to 13.3%. This comparison demonstrates the excellent performance of our framework. With dense (fully convolu- tional) and multi-scale testing, our ResNet-101 has an error of 11.7% using ground truth classes. Using ResNet-101 for predicting classes (4.6% top-5 classiÔ¨Åcation error, Table 4), the top-5 localization error is 14.4%. The above results are only based on the proposal network (RPN) in Faster R-CNN [32]. One may use the detection network (Fast R-CNN [7])
has an error of 11.7% using ground truth classes. Using ResNet-101 for predicting classes (4.6% top-5 classiÔ¨Åcation error, Table 4), the top-5 localization error is 14.4%. The above results are only based on the proposal network (RPN) in Faster R-CNN [32]. One may use the detection network (Fast R-CNN [7]) in Faster R-CNN to improve the results. But we notice that on this dataset, one image usually contains a single dominate object, and the proposal regions highly overlap with each other and thus have very similar RoI-pooled features. As a result, the image-centric training of Fast R-CNN [7] generates samples of small variations, which may not be desired for stochastic training. Motivated by this, in our current experiment we use the original R- CNN [8] that is RoI-centric, in place of Fast R-CNN. Our R-CNN implementation is as follows. We apply the per-class RPN trained as above on the training images to predict bounding boxes for the ground truth class. These predicted boxes play a role of class-dependent proposals. For each training image, the highest scored 200 proposals are extracted as training samples to train an R-CNN classi- Ô¨Åer. The image region is cropped from a proposal, warped to 224√ó224
images to predict bounding boxes for the ground truth class. These predicted boxes play a role of class-dependent proposals. For each training image, the highest scored 200 proposals are extracted as training samples to train an R-CNN classi- Ô¨Åer. The image region is cropped from a proposal, warped to 224√ó224 pixels, and fed into the classiÔ¨Åcation network as in R-CNN [8]. The outputs of this network consist of two sibling fc layers for cls and reg, also in a per-class form. This R-CNN network is Ô¨Åne-tuned on the training set us- ing a mini-batch size of 256 in the RoI-centric fashion. For testing, the RPN generates the highest scored 200 proposals for each predicted class, and the R-CNN network is used to update these proposals‚Äô scores and box positions. This method reduces the top-5 localization error to 10.6% (Table 13). This is our single-model result on the validation set. Using an ensemble of networks for both clas- siÔ¨Åcation and localization, we achieve a top-5 localization error of 9.0% on the test set. This number signiÔ¨Åcantly out- performs the ILSVRC 14 results (Table 14), showing a 64% relative reduction of error. This result won the 1st place in the ImageNet localization
an ensemble of networks for both clas- siÔ¨Åcation and localization, we achieve a top-5 localization error of 9.0% on the test set. This number signiÔ¨Åcantly out- performs the ILSVRC 14 results (Table 14), showing a 64% relative reduction of error. This result won the 1st place in the ImageNet localization task in ILSVRC 2015. 12
Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani‚àó Google Brain avaswani@google.com Noam Shazeer‚àó Google Brain noam@google.com Niki Parmar‚àó Google Research nikip@google.com Jakob Uszkoreit‚àó Google Research usz@google.com Llion Jones‚àó Google Research llion@google.com Aidan N. Gomez‚àó‚Ä† University of Toronto aidan@cs.toronto.edu ≈Åukasz Kaiser‚àó Google Brain lukaszkaiser@google.com Illia Polosukhin‚àó‚Ä° illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days
to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. ‚àóEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and
involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. ‚Ä†Work performed while at Google Brain. ‚Ä°Work performed while at Google Research. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. arXiv:1706.03762v7 [cs.CL] 2 Aug 2023 1 Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]. Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht‚àí1 and the input for
and encoder-decoder architectures [38, 24, 15]. Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht‚àí1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and
cases [27], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention,
This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [5,
representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next. 2 Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a
of Figure 1, respectively. 3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512. Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures
Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3 Scaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. 3.2.1 Scaled Dot-Product Attention We call our particular attention "Scaled Dot-Product Attention" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by ‚àödk, and apply a
the corresponding key. 3.2.1 Scaled Dot-Product Attention We call our particular attention "Scaled Dot-Product Attention" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by ‚àödk, and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as: Attention(Q, K, V ) = softmax(QKT ‚àödk )V (1) The two most commonly used attention functions are additive attention [2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1 ‚àödk . Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product
with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4. To counteract this effect, we scale the dot products by 1 ‚àödk . 3.2.2 Multi-Head Attention Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional 4To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q ¬∑ k = Pdk i=1 qiki,
keys and values we then perform the attention function in parallel, yielding dv-dimensional 4To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q ¬∑ k = Pdk i=1 qiki, has mean 0 and variance dk. 4 output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O where headi = Attention(QW Q i , KW K i , V W V i ) Where the projections are parameter matrices W Q i ‚ààRdmodel√ódk, W K i ‚ààRdmodel√ódk, W V i ‚ààRdmodel√ódv and W O ‚ààRhdv√ódmodel. In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. 3.2.3 Applications of Attention
work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. 3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: ‚Ä¢ In "encoder-decoder attention" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. ‚Ä¢ The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. ‚Ä¢ Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to
in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. ‚Ä¢ Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to ‚àí‚àû) all values in the input of the softmax which correspond to illegal connections. See Figure 2. 3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0, xW1 + b1)W2 + b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff = 2048. 3.4 Embeddings and Softmax Similarly
are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff = 2048. 3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by ‚àödmodel. 5 Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention. Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2 ¬∑ d) O(1) O(1) Recurrent O(n ¬∑ d2) O(n) O(n) Convolutional O(k ¬∑ n ¬∑ d2) O(1) O(logk(n))
the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention. Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2 ¬∑ d) O(1) O(1) Recurrent O(n ¬∑ d2) O(n) O(n) Convolutional O(k ¬∑ n ¬∑ d2) O(1) O(logk(n)) Self-Attention (restricted) O(r ¬∑ n ¬∑ d) O(1) O(n/r) 3.5 Positional Encoding Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9]. In this work, we use sine and cosine functions of different frequencies: PE(pos,2i) = sin(pos/100002i/dmodel) PE(pos,2i+1) = cos(pos/100002i/dmodel) where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2œÄ to
[9]. In this work, we use sine and cosine functions of different frequencies: PE(pos,2i) = sin(pos/100002i/dmodel) PE(pos,2i+1) = cos(pos/100002i/dmodel) where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2œÄ to 10000 ¬∑ 2œÄ. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos. We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. 4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ‚ààRd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of
to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ‚ààRd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas
it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence 6 length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work. A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions
investigate this approach further in future work. A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k ¬∑ n ¬∑ d + n ¬∑ d2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 5 Training This section describes the training regime for our models. 5.1 Training Data and Batching We trained on
examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 5 Training This section describes the training regime for our models. 5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. 5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were
base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). 5.3 Optimizer We used the Adam optimizer [20] with Œ≤1 = 0.9, Œ≤2 = 0.98 and œµ = 10‚àí9. We varied the learning rate over the course of training, according to the formula: lrate = d‚àí0.5 model ¬∑ min(step_num‚àí0.5, step_num ¬∑ warmup_steps‚àí1.5) (3) This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000. 5.4 Regularization We employ three types of regularization during training: 7 Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost. Model BLEU Training Cost (FLOPs) EN-DE EN-FR EN-DE EN-FR ByteNet [18] 23.75 Deep-Att + PosUnk [39] 39.2 1.0 ¬∑ 1020 GNMT + RL [38] 24.6 39.92 2.3 ¬∑ 1019 1.4 ¬∑
scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost. Model BLEU Training Cost (FLOPs) EN-DE EN-FR EN-DE EN-FR ByteNet [18] 23.75 Deep-Att + PosUnk [39] 39.2 1.0 ¬∑ 1020 GNMT + RL [38] 24.6 39.92 2.3 ¬∑ 1019 1.4 ¬∑ 1020 ConvS2S [9] 25.16 40.46 9.6 ¬∑ 1018 1.5 ¬∑ 1020 MoE [32] 26.03 40.56 2.0 ¬∑ 1019 1.2 ¬∑ 1020 Deep-Att + PosUnk Ensemble [39] 40.4 8.0 ¬∑ 1020 GNMT + RL Ensemble [38] 26.30 41.16 1.8 ¬∑ 1020 1.1 ¬∑ 1021 ConvS2S Ensemble [9] 26.36 41.29 7.7 ¬∑ 1019 1.2 ¬∑ 1021 Transformer (base model) 27.3 38.1 3.3 ¬∑ 1018 Transformer (big) 28.4 41.8 2.3 ¬∑ 1019 Residual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1. Label Smoothing During training, we employed label smoothing of value œµls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure,
the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1. Label Smoothing During training, we employed label smoothing of value œµls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3. For the base models,
task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3. For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty Œ± = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5. 6.2 Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different
point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5. 6.2 Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the 5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively. 8 Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities. N dmodel dff h dk dv Pdrop œµls train PPL BLEU params steps (dev) (dev) √ó106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 (A) 1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 (B) 16 5.16 25.1 58 32 5.01 25.4 60 (C) 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5
25.8 65 (A) 1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 (B) 16 5.16 25.1 58 32 5.01 25.4 60 (C) 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 (D) 0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility
While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model. 6.3 English Constituency Parsing To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora
been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we 9 Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) Parser Training WSJ 23 F1 Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3 Petrov et al. (2006) [29] WSJ only, discriminative 90.4 Zhu et al. (2013) [40] WSJ only, discriminative 90.4 Dyer et al. (2016) [8] WSJ only, discriminative 91.7 Transformer (4 layers) WSJ only, discriminative 91.3 Zhu et al. (2013) [40] semi-supervised 91.3 Huang & Harper (2009)
[37] WSJ only, discriminative 88.3 Petrov et al. (2006) [29] WSJ only, discriminative 90.4 Zhu et al. (2013) [40] WSJ only, discriminative 90.4 Dyer et al. (2016) [8] WSJ only, discriminative 91.7 Transformer (4 layers) WSJ only, discriminative 91.3 Zhu et al. (2013) [40] semi-supervised 91.3 Huang & Harper (2009) [14] semi-supervised 91.3 McClosky et al. (2006) [26] semi-supervised 92.1 Vinyals & Kaiser el al. (2014) [37] semi-supervised 92.1 Transformer (4 layers) semi-supervised 92.7 Luong et al. (2015) [23] multi-task 93.0 Dyer et al. (2016) [8] generative 93.3 increased the maximum output length to input length + 300. We used a beam size of 21 and Œ± = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur- prisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]. In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley- Parser [29] even when training only on the WSJ training set of 40K sentences. 7 Conclusion In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the
Neural Network Grammar [8]. In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley- Parser [29] even when training only on the WSJ training set of 40K sentences. 7 Conclusion In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours. The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor. Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for
mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours. The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor. Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration. References [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014. [3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017. [4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016. 10 [5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014. [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016. [7] Junyoung Chung, √áaglar G√ºl√ßehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.
Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014. [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016. [7] Junyoung Chung, √áaglar G√ºl√ßehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014. [8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL, 2016. [9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu- tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017. [10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013. [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770‚Äì778, 2016. [12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J√ºrgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001. [13] Sepp Hochreiter and J√ºrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735‚Äì1780, 1997. [14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,
in recurrent nets: the difficulty of learning long-term dependencies, 2001. [13] Sepp Hochreiter and J√ºrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735‚Äì1780, 1997. [14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832‚Äì841. ACL, August 2009. [15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016. [16] ≈Åukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016. [17] ≈Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016. [18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017. [19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017. [20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. [21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017. [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira
Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017. [20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. [21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017. [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017. [23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015. [24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention- based neural machine translation. arXiv preprint arXiv:1508.04025, 2015. 11 [25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313‚Äì330, 1993. [26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152‚Äì159. ACL, June 2006. [27] Ankur Parikh, Oscar T√§ckstr√∂m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016. [28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep
In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152‚Äì159. ACL, June 2006. [27] Ankur Parikh, Oscar T√§ckstr√∂m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016. [28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017. [29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433‚Äì440. ACL, July 2006. [30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016. [31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015. [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. [33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929‚Äì1958, 2014. [34] Sainbayar Sukhbaatar, Arthur
and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. [33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929‚Äì1958, 2014. [34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440‚Äì2448. Curran Associates, Inc., 2015. [35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104‚Äì3112, 2014. [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015. [37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems, 2015. [38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google‚Äôs neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016. [39] Jie
Advances in Neural Information Processing Systems, 2015. [38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google‚Äôs neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016. [39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016. [40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages 434‚Äì443. ACL, August 2013. 12 Attention Visualizations It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many
spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‚Äòmaking‚Äô, completing the phrase ‚Äòmaking...more difficult‚Äô. Attentions here shown only for the word ‚Äòmaking‚Äô. Different colors represent different heads. Best viewed in color. 13 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> Figure 4: Two attention heads, also
should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‚Äòits‚Äô for attention heads 5 and 6. Note that the attentions are very sharp for this word. 14 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> Figure
perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks. 15
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout}@google.com Abstract We introduce a new language representa- tion model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language repre- sentation models (Peters et al., 2018a; Rad- ford et al., 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be Ô¨Åne- tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task- speciÔ¨Åc architecture modiÔ¨Åcations. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art re- sults on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answer- ing Test F1 to 93.2 (1.5 point absolute im- provement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement). 1 Introduction Language model pre-training has been shown to be effective for improving many natural language processing tasks
MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answer- ing Test F1 to 93.2 (1.5 point absolute im- provement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement). 1 Introduction Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the re- lationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce Ô¨Åne-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016). There are two existing strategies for apply- ing pre-trained language representations to down- stream tasks: feature-based and Ô¨Åne-tuning. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-speciÔ¨Åc architectures that include the pre-trained representations as addi- tional features. The Ô¨Åne-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-speciÔ¨Åc parameters, and is trained
down- stream tasks: feature-based and Ô¨Åne-tuning. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-speciÔ¨Åc architectures that include the pre-trained representations as addi- tional features. The Ô¨Åne-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-speciÔ¨Åc parameters, and is trained on the downstream tasks by simply Ô¨Åne-tuning all pre- trained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, espe- cially for the Ô¨Åne-tuning approaches. The ma- jor limitation is that standard language models are unidirectional, and this limits the choice of archi- tectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-to- right architecture, where every token can only at- tend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017). Such re- strictions are sub-optimal for sentence-level tasks, and could be very harmful when applying Ô¨Åne- tuning based approaches to token-level tasks such as question answering, where it is crucial to incor- porate context from both directions. In this paper, we improve
layers of the Transformer (Vaswani et al., 2017). Such re- strictions are sub-optimal for sentence-level tasks, and could be very harmful when applying Ô¨Åne- tuning based approaches to token-level tasks such as question answering, where it is crucial to incor- porate context from both directions. In this paper, we improve the Ô¨Åne-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidi- rectionality constraint by using a ‚Äúmasked lan- guage model‚Äù (MLM) pre-training objective, in- spired by the Cloze task (Taylor, 1953). The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked arXiv:1810.04805v2 [cs.CL] 24 May 2019 word based only on its context. Unlike left-to- right language model pre-training, the MLM ob- jective enables the representation to fuse the left and the right context, which allows us to pre- train a deep bidirectional Transformer. In addi- tion to the masked language model, we also use a ‚Äúnext sentence prediction‚Äù task that jointly pre- trains text-pair representations. The contributions of our paper are as follows: ‚Ä¢ We demonstrate the importance of bidirectional pre-training for language representations. Un- like Radford
pre- train a deep bidirectional Transformer. In addi- tion to the masked language model, we also use a ‚Äúnext sentence prediction‚Äù task that jointly pre- trains text-pair representations. The contributions of our paper are as follows: ‚Ä¢ We demonstrate the importance of bidirectional pre-training for language representations. Un- like Radford et al. (2018), which uses unidirec- tional language models for pre-training, BERT uses masked language models to enable pre- trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. ‚Ä¢ We show that pre-trained representations reduce the need for many heavily-engineered task- speciÔ¨Åc architectures. BERT is the Ô¨Årst Ô¨Åne- tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outper- forming many task-speciÔ¨Åc architectures. ‚Ä¢ BERT advances the state of the art for eleven NLP tasks. The code and pre-trained mod- els are available at https://github.com/ google-research/bert. 2 Related Work There is a long history of pre-training general lan- guage representations, and we brieÔ¨Çy review the most widely-used approaches in this section. 2.1 Unsupervised Feature-based Approaches Learning widely applicable representations of words has been an active area
code and pre-trained mod- els are available at https://github.com/ google-research/bert. 2 Related Work There is a long history of pre-training general lan- guage representations, and we brieÔ¨Çy review the most widely-used approaches in this section. 2.1 Unsupervised Feature-based Approaches Learning widely applicable representations of words has been an active area of research for decades, including non-neural (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and neural (Mikolov et al., 2013; Pennington et al., 2014) methods. Pre-trained word embeddings are an integral part of modern NLP systems, of- fering signiÔ¨Åcant improvements over embeddings learned from scratch (Turian et al., 2010). To pre- train word embedding vectors, left-to-right lan- guage modeling objectives have been used (Mnih and Hinton, 2009), as well as objectives to dis- criminate correct from incorrect words in left and right context (Mikolov et al., 2013). These approaches have been generalized to coarser granularities, such as sentence embed- dings (Kiros et al., 2015; Logeswaran and Lee, 2018) or paragraph embeddings (Le and Mikolov, 2014). To train sentence representations, prior work has used objectives to rank candidate next sentences (Jernite et al., 2017; Logeswaran and Lee, 2018), left-to-right generation of next sen- tence words given a
as sentence embed- dings (Kiros et al., 2015; Logeswaran and Lee, 2018) or paragraph embeddings (Le and Mikolov, 2014). To train sentence representations, prior work has used objectives to rank candidate next sentences (Jernite et al., 2017; Logeswaran and Lee, 2018), left-to-right generation of next sen- tence words given a representation of the previous sentence (Kiros et al., 2015), or denoising auto- encoder derived objectives (Hill et al., 2016). ELMo and its predecessor (Peters et al., 2017, 2018a) generalize traditional word embedding re- search along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual rep- resentation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-speciÔ¨Åc architectures, ELMo advances the state of the art for several major NLP benchmarks (Peters et al., 2018a) including ques- tion answering (Rajpurkar et al., 2016), sentiment analysis (Socher et al., 2013), and named entity recognition (Tjong Kim Sang and De Meulder, 2003). Melamud et al. (2016) proposed learning contextual representations through a task to pre- dict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not
analysis (Socher et al., 2013), and named entity recognition (Tjong Kim Sang and De Meulder, 2003). Melamud et al. (2016) proposed learning contextual representations through a task to pre- dict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional. Fedus et al. (2018) shows that the cloze task can be used to improve the robustness of text generation mod- els. 2.2 Unsupervised Fine-tuning Approaches As with the feature-based approaches, the Ô¨Årst works in this direction only pre-trained word em- bedding parameters from unlabeled text (Col- lobert and Weston, 2008). More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and Ô¨Åne-tuned for a supervised downstream task (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018). The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT (Radford et al., 2018) achieved pre- viously state-of-the-art results on many sentence- level tasks from the GLUE benchmark (Wang et al., 2018a). Left-to-right language model- BERT BERT E[CLS] E1 E[SEP] ... EN E1‚Äô ... EM‚Äô C T1 T[SEP]
learned from scratch. At least partly due to this advantage, OpenAI GPT (Radford et al., 2018) achieved pre- viously state-of-the-art results on many sentence- level tasks from the GLUE benchmark (Wang et al., 2018a). Left-to-right language model- BERT BERT E[CLS] E1 E[SEP] ... EN E1‚Äô ... EM‚Äô C T1 T[SEP] ... TN T1‚Äô ... TM‚Äô [CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM Question Paragraph Start/End Span BERT E[CLS] E1 E[SEP] ... EN E1‚Äô ... EM‚Äô C T1 T[SEP] ... TN T1‚Äô ... TM‚Äô [CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM Masked Sentence A Masked Sentence B Pre-training Fine-Tuning NSP Mask LM Mask LM Unlabeled Sentence A and B Pair SQuAD Question Answer Pair NER MNLI Figure 1: Overall pre-training and Ô¨Åne-tuning procedures for BERT. Apart from output layers, the same architec- tures are used in both pre-training and Ô¨Åne-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During Ô¨Åne-tuning, all parameters are Ô¨Åne-tuned. [CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques- tions/answers). ing and auto-encoder objectives have been used for pre-training such
model parameters are used to initialize models for different down-stream tasks. During Ô¨Åne-tuning, all parameters are Ô¨Åne-tuned. [CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques- tions/answers). ing and auto-encoder objectives have been used for pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015). 2.3 Transfer Learning from Supervised Data There has also been work showing effective trans- fer from supervised tasks with large datasets, such as natural language inference (Conneau et al., 2017) and machine translation (McCann et al., 2017). Computer vision research has also demon- strated the importance of transfer learning from large pre-trained models, where an effective recipe is to Ô¨Åne-tune models pre-trained with Ima- geNet (Deng et al., 2009; Yosinski et al., 2014). 3 BERT We introduce BERT and its detailed implementa- tion in this section. There are two steps in our framework: pre-training and Ô¨Åne-tuning. Dur- ing pre-training, the model is trained on unlabeled data over different pre-training tasks. For Ô¨Åne- tuning, the BERT model is Ô¨Årst initialized with the pre-trained parameters, and all of the param- eters are Ô¨Åne-tuned using labeled data from the downstream
are two steps in our framework: pre-training and Ô¨Åne-tuning. Dur- ing pre-training, the model is trained on unlabeled data over different pre-training tasks. For Ô¨Åne- tuning, the BERT model is Ô¨Årst initialized with the pre-trained parameters, and all of the param- eters are Ô¨Åne-tuned using labeled data from the downstream tasks. Each downstream task has sep- arate Ô¨Åne-tuned models, even though they are ini- tialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section. A distinctive feature of BERT is its uniÔ¨Åed ar- chitecture across different tasks. There is mini- mal difference between the pre-trained architec- ture and the Ô¨Ånal downstream architecture. Model Architecture BERT‚Äôs model architec- ture is a multi-layer bidirectional Transformer en- coder based on the original implementation de- scribed in Vaswani et al. (2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our im- plementation is almost identical to the original, we will omit an exhaustive background descrip- tion of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as ‚ÄúThe Annotated Transformer.‚Äù2 In this work, we denote the number of
become common and our im- plementation is almost identical to the original, we will omit an exhaustive background descrip- tion of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as ‚ÄúThe Annotated Transformer.‚Äù2 In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Param- eters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M). BERTBASE was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Trans- former uses constrained self-attention where every token can only attend to context to its left.4 1https://github.com/tensorÔ¨Çow/tensor2tensor 2http://nlp.seas.harvard.edu/2018/04/03/attention.html 3In all cases we set the feed-forward/Ô¨Ålter size to be 4H, i.e., 3072 for the H = 768 and 4096 for the H = 1024. 4We note that in the literature the bidirectional Trans- Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., ‚ü®Question, Answer
H = 768 and 4096 for the H = 1024. 4We note that in the literature the bidirectional Trans- Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., ‚ü®Question, Answer ‚ü©) in one token sequence. Throughout this work, a ‚Äúsentence‚Äù can be an arbi- trary span of contiguous text, rather than an actual linguistic sentence. A ‚Äúsequence‚Äù refers to the in- put token sequence to BERT, which may be a sin- gle sentence or two sentences packed together. We use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The Ô¨Årst token of every sequence is always a special clas- siÔ¨Åcation token ([CLS]). The Ô¨Ånal hidden state corresponding to this token is used as the ag- gregate sequence representation for classiÔ¨Åcation tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embed- ding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E,
a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embed- ding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the Ô¨Ånal hidden vector of the special [CLS] token as C ‚ààRH, and the Ô¨Ånal hidden vector for the ith input token as Ti ‚ààRH. For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualiza- tion of this construction can be seen in Figure 2. 3.1 Pre-training BERT Unlike Peters et al. (2018a) and Radford et al. (2018), we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsuper- vised tasks, described in this section. This step is presented in the left part of Figure 1. Task #1: Masked LM Intuitively, it is reason- able to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to- right and a right-to-left model. Unfortunately, standard conditional language models can only be
left part of Figure 1. Task #1: Masked LM Intuitively, it is reason- able to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to- right and a right-to-left model. Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirec- tional conditioning would allow each word to in- directly ‚Äúsee itself‚Äù, and the model could trivially predict the target word in a multi-layered context. former is often referred to as a ‚ÄúTransformer encoder‚Äù while the left-context-only version is referred to as a ‚ÄúTransformer decoder‚Äù since it can be used for text generation. In order to train a deep bidirectional representa- tion, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a ‚Äúmasked LM‚Äù (MLM), although it is often referred to as a Cloze task in the literature (Taylor, 1953). In this case, the Ô¨Ånal hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece to- kens in each
as a Cloze task in the literature (Taylor, 1953). In this case, the Ô¨Ånal hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece to- kens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than recon- structing the entire input. Although this allows us to obtain a bidirec- tional pre-trained model, a downside is that we are creating a mismatch between pre-training and Ô¨Åne-tuning, since the [MASK] token does not ap- pear during Ô¨Åne-tuning. To mitigate this, we do not always replace ‚Äúmasked‚Äù words with the ac- tual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. Then, Ti will be used to predict the original token with cross entropy loss. We compare variations of this procedure in Appendix C.2. Task
with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. Then, Ti will be used to predict the original token with cross entropy loss. We compare variations of this procedure in Appendix C.2. Task #2: Next Sentence Prediction (NSP) Many important downstream tasks such as Ques- tion Answering (QA) and Natural Language Infer- ence (NLI) are based on understanding the rela- tionship between two sentences, which is not di- rectly captured by language modeling. In order to train a model that understands sentence rela- tionships, we pre-train for a binarized next sen- tence prediction task that can be trivially gener- ated from any monolingual corpus. SpeciÔ¨Åcally, when choosing the sentences A and B for each pre- training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure 1, C is used for next sentence predic- tion (NSP).5 Despite its simplicity, we demon- strate in Section 5.1 that pre-training towards this task is very beneÔ¨Åcial to both
and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure 1, C is used for next sentence predic- tion (NSP).5 Despite its simplicity, we demon- strate in Section 5.1 that pre-training towards this task is very beneÔ¨Åcial to both QA and NLI. 6 5The Ô¨Ånal model achieves 97%-98% accuracy on NSP. 6The vector C is not a meaningful sentence representation without Ô¨Åne-tuning, since it was trained with NSP. [CLS] he likes play ##ing [SEP] my dog is cute [SEP] Input E[CLS] Ehe Elikes Eplay E##ing E[SEP] Emy Edog Eis Ecute E[SEP] Token Embeddings EA EB EB EB EB EB EA EA EA EA EA Segment Embeddings E0 E6 E7 E8 E9 E10 E1 E2 E3 E4 E5 Position Embeddings Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta- tion embeddings and the position embeddings. The NSP task is closely related to representation- learning objectives used in Jernite et al. (2017) and Logeswaran and Lee (2018). However, in prior work, only sentence embeddings are transferred to down-stream tasks, where BERT transfers all pa- rameters to initialize end-task model parameters. Pre-training data The
the position embeddings. The NSP task is closely related to representation- learning objectives used in Jernite et al. (2017) and Logeswaran and Lee (2018). However, in prior work, only sentence embeddings are transferred to down-stream tasks, where BERT transfers all pa- rameters to initialize end-task model parameters. Pre-training data The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is criti- cal to use a document-level corpus rather than a shufÔ¨Çed sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences. 3.2 Fine-tuning BERT Fine-tuning is straightforward since the self- attention mechanism in the Transformer al- lows BERT to model many downstream tasks‚Äî whether they involve single text or text pairs‚Äîby swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs be- fore applying bidirectional cross attention, such as Parikh et al. (2016); Seo et al. (2017). BERT instead uses the self-attention mechanism to
involve single text or text pairs‚Äîby swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs be- fore applying bidirectional cross attention, such as Parikh et al. (2016); Seo et al. (2017). BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidi- rectional cross attention between two sentences. For each task, we simply plug in the task- speciÔ¨Åc inputs and outputs into BERT and Ô¨Åne- tune all the parameters end-to-end. At the in- put, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphras- ing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and (4) a degenerate text-‚àÖpair in text classiÔ¨Åcation or sequence tagging. At the output, the token rep- resentations are fed into an output layer for token- level tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classiÔ¨Åcation, such as en- tailment or sentiment analysis. Compared to pre-training, Ô¨Åne-tuning is rela- tively inexpensive. All of the results in the pa- per can be replicated in at most
level tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classiÔ¨Åcation, such as en- tailment or sentiment analysis. Compared to pre-training, Ô¨Åne-tuning is rela- tively inexpensive. All of the results in the pa- per can be replicated in at most 1 hour on a sin- gle Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model.7 We de- scribe the task-speciÔ¨Åc details in the correspond- ing subsections of Section 4. More details can be found in Appendix A.5. 4 Experiments In this section, we present BERT Ô¨Åne-tuning re- sults on 11 NLP tasks. 4.1 GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018a) is a col- lection of diverse natural language understanding tasks. Detailed descriptions of GLUE datasets are included in Appendix B.1. To Ô¨Åne-tune on GLUE, we represent the input sequence (for single sentence or sentence pairs) as described in Section 3, and use the Ô¨Ånal hid- den vector C ‚ààRH corresponding to the Ô¨Årst input token ([CLS]) as the aggregate representa- tion. The only new parameters introduced during Ô¨Åne-tuning are classiÔ¨Åcation layer weights W ‚àà RK√óH, where K is
sequence (for single sentence or sentence pairs) as described in Section 3, and use the Ô¨Ånal hid- den vector C ‚ààRH corresponding to the Ô¨Årst input token ([CLS]) as the aggregate representa- tion. The only new parameters introduced during Ô¨Åne-tuning are classiÔ¨Åcation layer weights W ‚àà RK√óH, where K is the number of labels. We com- pute a standard classiÔ¨Åcation loss with C and W, i.e., log(softmax(CW T )). 7For example, the BERT SQuAD model can be trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of 91.0%. 8See (10) in https://gluebenchmark.com/faq. System MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE Average 392k 363k 108k 67k 8.5k 5.7k 3.5k 2.5k - Pre-OpenAI SOTA 80.6/80.1 66.1 82.3 93.2 35.0 81.0 86.0 61.7 74.0 BiLSTM+ELMo+Attn 76.4/76.1 64.8 79.8 90.4 36.0 73.3 84.9 56.8 71.0 OpenAI GPT 82.1/81.4 70.3 87.4 91.3 45.4 80.0 82.3 56.0 75.1 BERTBASE 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6 BERTLARGE 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1 Table 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard). The number below each task denotes the number of training examples. The ‚ÄúAverage‚Äù column is slightly different than the
BERTBASE 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6 BERTLARGE 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1 Table 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard). The number below each task denotes the number of training examples. The ‚ÄúAverage‚Äù column is slightly different than the ofÔ¨Åcial GLUE score, since we exclude the problematic WNLI set.8 BERT and OpenAI GPT are single- model, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components. We use a batch size of 32 and Ô¨Åne-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best Ô¨Åne-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. Additionally, for BERTLARGE we found that Ô¨Åne- tuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set. With random restarts, we use the same pre-trained checkpoint but per- form different Ô¨Åne-tuning data shufÔ¨Çing and clas- siÔ¨Åer layer initialization.9 Results are presented in Table 1. Both BERTBASE
was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set. With random restarts, we use the same pre-trained checkpoint but per- form different Ô¨Åne-tuning data shufÔ¨Çing and clas- siÔ¨Åer layer initialization.9 Results are presented in Table 1. Both BERTBASE and BERTLARGE outperform all sys- tems on all tasks by a substantial margin, obtaining 4.5% and 7.0% respective average accuracy im- provement over the prior state of the art. Note that BERTBASE and OpenAI GPT are nearly identical in terms of model architecture apart from the at- tention masking. For the largest and most widely reported GLUE task, MNLI, BERT obtains a 4.6% absolute accuracy improvement. On the ofÔ¨Åcial GLUE leaderboard10, BERTLARGE obtains a score of 80.5, compared to OpenAI GPT, which obtains 72.8 as of the date of writing. We Ô¨Ånd that BERTLARGE signiÔ¨Åcantly outper- forms BERTBASE across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section 5.2. 4.2 SQuAD v1.1 The Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowd- sourced question/answer pairs (Rajpurkar et al., 2016). Given a question and a passage from
tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section 5.2. 4.2 SQuAD v1.1 The Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowd- sourced question/answer pairs (Rajpurkar et al., 2016). Given a question and a passage from 9The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERTBASE and BERTLARGE. 10https://gluebenchmark.com/leaderboard Wikipedia containing the answer, the task is to predict the answer text span in the passage. As shown in Figure 1, in the question answer- ing task, we represent the input question and pas- sage as a single packed sequence, with the ques- tion using the A embedding and the passage using the B embedding. We only introduce a start vec- tor S ‚ààRH and an end vector E ‚ààRH during Ô¨Åne-tuning. The probability of word i being the start of the answer span is computed as a dot prod- uct between Ti and S followed by a softmax over all of the words in the paragraph: Pi = eS¬∑Ti P j eS¬∑Tj . The analogous formula is used for the end
The probability of word i being the start of the answer span is computed as a dot prod- uct between Ti and S followed by a softmax over all of the words in the paragraph: Pi = eS¬∑Ti P j eS¬∑Tj . The analogous formula is used for the end of the answer span. The score of a candidate span from position i to position j is deÔ¨Åned as S¬∑Ti + E¬∑Tj, and the maximum scoring span where j ‚â•i is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We Ô¨Åne-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32. Table 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available,11 and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by Ô¨Årst Ô¨Åne-tuning on TriviaQA (Joshi et al., 2017) befor Ô¨Åne-tuning on SQuAD. Our best performing
results from the SQuAD leaderboard do not have up-to-date public system descriptions available,11 and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by Ô¨Årst Ô¨Åne-tuning on TriviaQA (Joshi et al., 2017) befor Ô¨Åne-tuning on SQuAD. Our best performing system outperforms the top leaderboard system by +1.5 F1 in ensembling and +1.3 F1 as a single system. In fact, our single BERT model outperforms the top ensemble sys- tem in terms of F1 score. Without TriviaQA Ô¨Åne- 11QANet is described in Yu et al. (2018), but the system has improved substantially after publication. System Dev Test EM F1 EM F1 Top Leaderboard Systems (Dec 10th, 2018) Human - - 82.3 91.2 #1 Ensemble - nlnet - - 86.0 91.7 #2 Ensemble - QANet - - 84.5 90.5 Published BiDAF+ELMo (Single) - 85.6 - 85.8 R.M. Reader (Ensemble) 81.2 87.9 82.3 88.5 Ours BERTBASE (Single) 80.8 88.5 - - BERTLARGE (Single) 84.1 90.9 - - BERTLARGE (Ensemble) 85.8 91.8 - - BERTLARGE (Sgl.+TriviaQA) 84.2 91.1 85.1 91.8 BERTLARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2 Table 2: SQuAD 1.1 results. The BERT ensemble is 7x systems which use different pre-training check- points
82.3 88.5 Ours BERTBASE (Single) 80.8 88.5 - - BERTLARGE (Single) 84.1 90.9 - - BERTLARGE (Ensemble) 85.8 91.8 - - BERTLARGE (Sgl.+TriviaQA) 84.2 91.1 85.1 91.8 BERTLARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2 Table 2: SQuAD 1.1 results. The BERT ensemble is 7x systems which use different pre-training check- points and Ô¨Åne-tuning seeds. System Dev Test EM F1 EM F1 Top Leaderboard Systems (Dec 10th, 2018) Human 86.3 89.0 86.9 89.5 #1 Single - MIR-MRC (F-Net) - - 74.8 78.0 #2 Single - nlnet - - 74.2 77.1 Published unet (Ensemble) - - 71.4 74.9 SLQA+ (Single) - 71.4 74.4 Ours BERTLARGE (Single) 78.7 81.9 80.0 83.1 Table 3: SQuAD 2.0 results. We exclude entries that use BERT as one of their components. tuning data, we only lose 0.1-0.4 F1, still outper- forming all existing systems by a wide margin.12 4.3 SQuAD v2.0 The SQuAD 2.0 task extends the SQuAD 1.1 problem deÔ¨Ånition by allowing for the possibility that no short answer exists in the provided para- graph, making the problem more realistic. We use a simple approach to extend the SQuAD v1.1 BERT model for this task. We treat ques- tions that do not have an answer as having
1.1 problem deÔ¨Ånition by allowing for the possibility that no short answer exists in the provided para- graph, making the problem more realistic. We use a simple approach to extend the SQuAD v1.1 BERT model for this task. We treat ques- tions that do not have an answer as having an an- swer span with start and end at the [CLS] to- ken. The probability space for the start and end answer span positions is extended to include the position of the [CLS] token. For prediction, we compare the score of the no-answer span: snull = S¬∑C + E¬∑C to the score of the best non-null span 12The TriviaQA data we used consists of paragraphs from TriviaQA-Wiki formed of the Ô¨Årst 400 tokens in documents, that contain at least one of the provided possible answers. System Dev Test ESIM+GloVe 51.9 52.7 ESIM+ELMo 59.1 59.2 OpenAI GPT - 78.0 BERTBASE 81.6 - BERTLARGE 86.6 86.3 Human (expert)‚Ä† - 85.0 Human (5 annotations)‚Ä† - 88.0 Table 4: SWAG Dev and Test accuracies. ‚Ä†Human per- formance is measured with 100 samples, as reported in the SWAG paper. ÀÜ si,j = maxj‚â•iS¬∑Ti + E¬∑Tj. We predict a non-null answer when ÀÜ si,j > snull
- BERTLARGE 86.6 86.3 Human (expert)‚Ä† - 85.0 Human (5 annotations)‚Ä† - 88.0 Table 4: SWAG Dev and Test accuracies. ‚Ä†Human per- formance is measured with 100 samples, as reported in the SWAG paper. ÀÜ si,j = maxj‚â•iS¬∑Ti + E¬∑Tj. We predict a non-null answer when ÀÜ si,j > snull + œÑ, where the thresh- old œÑ is selected on the dev set to maximize F1. We did not use TriviaQA data for this model. We Ô¨Åne-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48. The results compared to prior leaderboard en- tries and top published work (Sun et al., 2018; Wang et al., 2018b) are shown in Table 3, exclud- ing systems that use BERT as one of their com- ponents. We observe a +5.1 F1 improvement over the previous best system. 4.4 SWAG The Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair com- pletion examples that evaluate grounded common- sense inference (Zellers et al., 2018). Given a sen- tence, the task is to choose the most plausible con- tinuation among four choices. When Ô¨Åne-tuning on the SWAG dataset, we construct four input sequences, each containing the concatenation of the given
113k sentence-pair com- pletion examples that evaluate grounded common- sense inference (Zellers et al., 2018). Given a sen- tence, the task is to choose the most plausible con- tinuation among four choices. When Ô¨Åne-tuning on the SWAG dataset, we construct four input sequences, each containing the concatenation of the given sentence (sentence A) and a possible continuation (sentence B). The only task-speciÔ¨Åc parameters introduced is a vec- tor whose dot product with the [CLS] token rep- resentation C denotes a score for each choice which is normalized with a softmax layer. We Ô¨Åne-tune the model for 3 epochs with a learning rate of 2e-5 and a batch size of 16. Re- sults are presented in Table 4. BERTLARGE out- performs the authors‚Äô baseline ESIM+ELMo sys- tem by +27.1% and OpenAI GPT by 8.3%. 5 Ablation Studies In this section, we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance. Additional Dev Set Tasks MNLI-m QNLI MRPC SST-2 SQuAD (Acc) (Acc) (Acc) (Acc) (F1) BERTBASE 84.4 88.4 86.7 92.7 88.5 No NSP 83.9 84.9 86.5 92.6 87.9 LTR & No NSP 82.1 84.3 77.5 92.1 77.8 + BiLSTM 82.1 84.1 75.7 91.6
in order to better understand their relative importance. Additional Dev Set Tasks MNLI-m QNLI MRPC SST-2 SQuAD (Acc) (Acc) (Acc) (Acc) (F1) BERTBASE 84.4 88.4 86.7 92.7 88.5 No NSP 83.9 84.9 86.5 92.6 87.9 LTR & No NSP 82.1 84.3 77.5 92.1 77.8 + BiLSTM 82.1 84.1 75.7 91.6 84.9 Table 5: Ablation over the pre-training tasks using the BERTBASE architecture. ‚ÄúNo NSP‚Äù is trained without the next sentence prediction task. ‚ÄúLTR & No NSP‚Äù is trained as a left-to-right LM without the next sentence prediction, like OpenAI GPT. ‚Äú+ BiLSTM‚Äù adds a ran- domly initialized BiLSTM on top of the ‚ÄúLTR + No NSP‚Äù model during Ô¨Åne-tuning. ablation studies can be found in Appendix C. 5.1 Effect of Pre-training Tasks We demonstrate the importance of the deep bidi- rectionality of BERT by evaluating two pre- training objectives using exactly the same pre- training data, Ô¨Åne-tuning scheme, and hyperpa- rameters as BERTBASE: No NSP: A bidirectional model which is trained using the ‚Äúmasked LM‚Äù (MLM) but without the ‚Äúnext sentence prediction‚Äù (NSP) task. LTR & No NSP: A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. The left-only constraint was also applied
rameters as BERTBASE: No NSP: A bidirectional model which is trained using the ‚Äúmasked LM‚Äù (MLM) but without the ‚Äúnext sentence prediction‚Äù (NSP) task. LTR & No NSP: A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. The left-only constraint was also applied at Ô¨Åne-tuning, because removing it introduced a pre-train/Ô¨Åne-tune mismatch that degraded downstream performance. Additionally, this model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but using our larger training dataset, our input repre- sentation, and our Ô¨Åne-tuning scheme. We Ô¨Årst examine the impact brought by the NSP task. In Table 5, we show that removing NSP hurts performance signiÔ¨Åcantly on QNLI, MNLI, and SQuAD 1.1. Next, we evaluate the impact of training bidirectional representations by com- paring ‚ÄúNo NSP‚Äù to ‚ÄúLTR & No NSP‚Äù. The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD. For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions, since the token-level hidden states have no right- side context. In order to make a good faith at- tempt at strengthening the LTR system, we added
tasks, with large drops on MRPC and SQuAD. For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions, since the token-level hidden states have no right- side context. In order to make a good faith at- tempt at strengthening the LTR system, we added a randomly initialized BiLSTM on top. This does signiÔ¨Åcantly improve results on SQuAD, but the results are still far worse than those of the pre- trained bidirectional models. The BiLSTM hurts performance on the GLUE tasks. We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two mod- els, as ELMo does. However: (a) this is twice as expensive as a single bidirectional model; (b) this is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question; (c) this it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer. 5.2 Effect of Model Size In this section, we explore the effect of model size on Ô¨Åne-tuning task accuracy. We trained a number of BERT
question; (c) this it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer. 5.2 Effect of Model Size In this section, we explore the effect of model size on Ô¨Åne-tuning task accuracy. We trained a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training pro- cedure as described previously. Results on selected GLUE tasks are shown in Table 6. In this table, we report the average Dev Set accuracy from 5 random restarts of Ô¨Åne-tuning. We can see that larger models lead to a strict ac- curacy improvement across all four datasets, even for MRPC which only has 3,600 labeled train- ing examples, and is substantially different from the pre-training tasks. It is also perhaps surpris- ing that we are able to achieve such signiÔ¨Åcant improvements on top of models which are al- ready quite large relative to the existing literature. For example, the largest Transformer explored in Vaswani et al. (2017) is (L=6, H=1024, A=16) with 100M parameters for the encoder, and the largest Transformer we have found in the literature is (L=64,
signiÔ¨Åcant improvements on top of models which are al- ready quite large relative to the existing literature. For example, the largest Transformer explored in Vaswani et al. (2017) is (L=6, H=1024, A=16) with 100M parameters for the encoder, and the largest Transformer we have found in the literature is (L=64, H=512, A=2) with 235M parameters (Al-Rfou et al., 2018). By contrast, BERTBASE contains 110M parameters and BERTLARGE con- tains 340M parameters. It has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table 6. However, we believe that this is the Ô¨Årst work to demonstrate convinc- ingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufÔ¨Å- ciently pre-trained. Peters et al. (2018b) presented mixed results on the downstream task impact of increasing the pre-trained bi-LM size from two to four layers and Melamud et al. (2016) men- tioned in passing that increasing hidden dimen- sion size from 200 to 600 helped, but increasing further to 1,000 did not bring
et al. (2018b) presented mixed results on the downstream task impact of increasing the pre-trained bi-LM size from two to four layers and Melamud et al. (2016) men- tioned in passing that increasing hidden dimen- sion size from 200 to 600 helped, but increasing further to 1,000 did not bring further improve- ments. Both of these prior works used a feature- based approach ‚Äî we hypothesize that when the model is Ô¨Åne-tuned directly on the downstream tasks and uses only a very small number of ran- domly initialized additional parameters, the task- speciÔ¨Åc models can beneÔ¨Åt from the larger, more expressive pre-trained representations even when downstream task data is very small. 5.3 Feature-based Approach with BERT All of the BERT results presented so far have used the Ô¨Åne-tuning approach, where a simple classiÔ¨Å- cation layer is added to the pre-trained model, and all parameters are jointly Ô¨Åne-tuned on a down- stream task. However, the feature-based approach, where Ô¨Åxed features are extracted from the pre- trained model, has certain advantages. First, not all tasks can be easily represented by a Trans- former encoder architecture, and therefore require a task-speciÔ¨Åc model architecture to be added. Second, there are major computational beneÔ¨Åts to
stream task. However, the feature-based approach, where Ô¨Åxed features are extracted from the pre- trained model, has certain advantages. First, not all tasks can be easily represented by a Trans- former encoder architecture, and therefore require a task-speciÔ¨Åc model architecture to be added. Second, there are major computational beneÔ¨Åts to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation. In this section, we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task (Tjong Kim Sang and De Meulder, 2003). In the input to BERT, we use a case-preserving WordPiece model, and we include the maximal document context provided by the data. Following standard practice, we for- mulate this as a tagging task but do not use a CRF Hyperparams Dev Set Accuracy #L #H #A LM (ppl) MNLI-m MRPC SST-2 3 768 12 5.84 77.9 79.8 88.4 6 768 3 5.24 80.6 82.2 90.7 6 768 12 4.68 81.9 84.8 91.3 12 768 12 3.99 84.4 86.7 92.9 12 1024 16 3.54 85.7 86.9 93.3 24 1024 16 3.23 86.6 87.8 93.7 Table 6: Ablation over BERT model size. #L =
768 12 5.84 77.9 79.8 88.4 6 768 3 5.24 80.6 82.2 90.7 6 768 12 4.68 81.9 84.8 91.3 12 768 12 3.99 84.4 86.7 92.9 12 1024 16 3.54 85.7 86.9 93.3 24 1024 16 3.23 86.6 87.8 93.7 Table 6: Ablation over BERT model size. #L = the number of layers; #H = hidden size; #A = number of at- tention heads. ‚ÄúLM (ppl)‚Äù is the masked LM perplexity of held-out training data. System Dev F1 Test F1 ELMo (Peters et al., 2018a) 95.7 92.2 CVT (Clark et al., 2018) - 92.6 CSE (Akbik et al., 2018) - 93.1 Fine-tuning approach BERTLARGE 96.6 92.8 BERTBASE 96.4 92.4 Feature-based approach (BERTBASE) Embeddings 91.0 - Second-to-Last Hidden 95.6 - Last Hidden 94.9 - Weighted Sum Last Four Hidden 95.9 - Concat Last Four Hidden 96.1 - Weighted Sum All 12 Layers 95.5 - Table 7: CoNLL-2003 Named Entity Recognition re- sults. Hyperparameters were selected using the Dev set. The reported Dev and Test scores are averaged over 5 random restarts using those hyperparameters. layer in the output. We use the representation of the Ô¨Årst sub-token as the input to the token-level classiÔ¨Åer over the NER label set. To ablate
re- sults. Hyperparameters were selected using the Dev set. The reported Dev and Test scores are averaged over 5 random restarts using those hyperparameters. layer in the output. We use the representation of the Ô¨Årst sub-token as the input to the token-level classiÔ¨Åer over the NER label set. To ablate the Ô¨Åne-tuning approach, we apply the feature-based approach by extracting the activa- tions from one or more layers without Ô¨Åne-tuning any parameters of BERT. These contextual em- beddings are used as input to a randomly initial- ized two-layer 768-dimensional BiLSTM before the classiÔ¨Åcation layer. Results are presented in Table 7. BERTLARGE performs competitively with state-of-the-art meth- ods. The best performing method concatenates the token representations from the top four hidden lay- ers of the pre-trained Transformer, which is only 0.3 F1 behind Ô¨Åne-tuning the entire model. This demonstrates that BERT is effective for both Ô¨Åne- tuning and feature-based approaches. 6 Conclusion Recent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to beneÔ¨Åt from deep unidirectional architec- tures. Our major contribution is further general- izing these Ô¨Åndings
Conclusion Recent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to beneÔ¨Åt from deep unidirectional architec- tures. Our major contribution is further general- izing these Ô¨Åndings to deep bidirectional architec- tures, allowing the same pre-trained model to suc- cessfully tackle a broad set of NLP tasks. References Alan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual string embeddings for sequence labeling. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1638‚Äì1649. Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2018. Character-level lan- guage modeling with deeper self-attention. arXiv preprint arXiv:1808.04444. Rie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6(Nov):1817‚Äì1853. Luisa Bentivogli, Bernardo Magnini, Ido Dagan, Hoa Trang Dang, and Danilo Giampiccolo. 2009. The Ô¨Åfth PASCAL recognizing textual entailment challenge. In TAC. NIST. John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspon- dence learning. In Proceedings of the 2006 confer- ence on empirical methods in natural language pro- cessing, pages 120‚Äì128. Association
Hoa Trang Dang, and Danilo Giampiccolo. 2009. The Ô¨Åfth PASCAL recognizing textual entailment challenge. In TAC. NIST. John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspon- dence learning. In Proceedings of the 2006 confer- ence on empirical methods in natural language pro- cessing, pages 120‚Äì128. Association for Computa- tional Linguistics. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large anno- tated corpus for learning natural language inference. In EMNLP. Association for Computational Linguis- tics. Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4):467‚Äì479. Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez- Gazpio, and Lucia Specia. 2017. Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1‚Äì14, Vancou- ver, Canada. Association for Computational Lin- guistics. Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robin- son. 2013. One billion word benchmark for measur- ing progress in statistical language modeling. arXiv preprint arXiv:1312.3005. Z. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018. Quora question pairs.
Association for Computational Lin- guistics. Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robin- son. 2013. One billion word benchmark for measur- ing progress in statistical language modeling. arXiv preprint arXiv:1312.3005. Z. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018. Quora question pairs. Christopher Clark and Matt Gardner. 2018. Simple and effective multi-paragraph reading comprehen- sion. In ACL. Kevin Clark, Minh-Thang Luong, Christopher D Man- ning, and Quoc Le. 2018. Semi-supervised se- quence modeling with cross-view training. In Pro- ceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing, pages 1914‚Äì 1925. Ronan Collobert and Jason Weston. 2008. A uniÔ¨Åed architecture for natural language processing: Deep neural networks with multitask learning. In Pro- ceedings of the 25th international conference on Machine learning, pages 160‚Äì167. ACM. Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¬®ƒ±c Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Nat- ural Language Processing, pages 670‚Äì680, Copen- hagen, Denmark. Association for Computational Linguistics. Andrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. In Advances in neural informa- tion processing systems,
universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Nat- ural Language Processing, pages 670‚Äì680, Copen- hagen, Denmark. Association for Computational Linguistics. Andrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. In Advances in neural informa- tion processing systems, pages 3079‚Äì3087. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei- Fei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09. William B Dolan and Chris Brockett. 2005. Automati- cally constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005). William Fedus, Ian Goodfellow, and Andrew M Dai. 2018. Maskgan: Better text generation via Ô¨Ålling in the . arXiv preprint arXiv:1801.07736. Dan Hendrycks and Kevin Gimpel. 2016. Bridging nonlinearities and stochastic regularizers with gaus- sian error linear units. CoRR, abs/1606.08415. Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sentences from unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computa- tional Linguistics. Jeremy Howard and Sebastian Ruder. 2018. Universal language model Ô¨Åne-tuning for text classiÔ¨Åcation. In ACL. Association for Computational Linguistics.
representations of sentences from unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computa- tional Linguistics. Jeremy Howard and Sebastian Ruder. 2018. Universal language model Ô¨Åne-tuning for text classiÔ¨Åcation. In ACL. Association for Computational Linguistics. Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, and Ming Zhou. 2018. Reinforced mnemonic reader for machine reading comprehen- sion. In IJCAI. Yacine Jernite, Samuel R. Bowman, and David Son- tag. 2017. Discourse-based objectives for fast un- supervised sentence representation learning. CoRR, abs/1705.00557. Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehen- sion. In ACL. Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In Advances in neural information processing systems, pages 3294‚Äì3302. Quoc Le and Tomas Mikolov. 2014. Distributed rep- resentations of sentences and documents. In Inter- national Conference on Machine Learning, pages 1188‚Äì1196. Hector J Levesque, Ernest Davis, and Leora Morgen- stern. 2011. The winograd schema challenge. In Aaai spring symposium: Logical formalizations of commonsense reasoning, volume 46, page 47. Lajanugen Logeswaran and
and Tomas Mikolov. 2014. Distributed rep- resentations of sentences and documents. In Inter- national Conference on Machine Learning, pages 1188‚Äì1196. Hector J Levesque, Ernest Davis, and Leora Morgen- stern. 2011. The winograd schema challenge. In Aaai spring symposium: Logical formalizations of commonsense reasoning, volume 46, page 47. Lajanugen Logeswaran and Honglak Lee. 2018. An efÔ¨Åcient framework for learning sentence represen- tations. In International Conference on Learning Representations. Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Con- textualized word vectors. In NIPS. Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning generic context em- bedding with bidirectional LSTM. In CoNLL. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositional- ity. In Advances in Neural Information Processing Systems 26, pages 3111‚Äì3119. Curran Associates, Inc. Andriy Mnih and Geoffrey E Hinton. 2009. A scal- able hierarchical distributed language model. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bot- tou, editors, Advances in Neural Information Pro- cessing Systems 21, pages 1081‚Äì1088. Curran As- sociates, Inc. Ankur P Parikh, Oscar T¬®ackstr¬®om, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural
A scal- able hierarchical distributed language model. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bot- tou, editors, Advances in Neural Information Pro- cessing Systems 21, pages 1081‚Äì1088. Curran As- sociates, Inc. Ankur P Parikh, Oscar T¬®ackstr¬®om, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In EMNLP. Jeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Nat- ural Language Processing (EMNLP), pages 1532‚Äì 1543. Matthew Peters, Waleed Ammar, Chandra Bhagavat- ula, and Russell Power. 2017. Semi-supervised se- quence tagging with bidirectional language models. In ACL. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word rep- resentations. In NAACL. Matthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. 2018b. Dissecting contextual word embeddings: Architecture and representation. In Proceedings of the 2018 Conference on Empiri- cal Methods in Natural Language Processing, pages 1499‚Äì1509. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing with unsupervised learning. Technical re- port, OpenAI. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings
Empiri- cal Methods in Natural Language Processing, pages 1499‚Äì1509. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing with unsupervised learning. Technical re- port, OpenAI. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Nat- ural Language Processing, pages 2383‚Äì2392. Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention Ô¨Çow for machine comprehension. In ICLR. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment tree- bank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631‚Äì1642. Fu Sun, Linyang Li, Xipeng Qiu, and Yang Liu. 2018. U-net: Machine reading comprehension with unanswerable questions. arXiv preprint arXiv:1810.06638. Wilson L Taylor. 1953. Cloze procedure: A new tool for measuring readability. Journalism Bulletin, 30(4):415‚Äì433. Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In CoNLL. Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning.
procedure: A new tool for measuring readability. Journalism Bulletin, 30(4):415‚Äì433. Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In CoNLL. Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Compu- tational Linguistics, ACL ‚Äô10, pages 384‚Äì394. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, pages 6000‚Äì6010. Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoen- coders. In Proceedings of the 25th international conference on Machine learning, pages 1096‚Äì1103. ACM. Alex Wang, Amanpreet Singh, Julian Michael, Fe- lix Hill, Omer Levy, and Samuel Bowman. 2018a. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: An- alyzing and Interpreting Neural Networks for NLP, pages 353‚Äì355. Wei Wang, Ming Yan, and Chen Wu. 2018b. Multi- granularity hierarchical attention fusion networks for reading comprehension and question answering. In Proceedings
A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: An- alyzing and Interpreting Neural Networks for NLP, pages 353‚Äì355. Wei Wang, Ming Yan, and Chen Wu. 2018b. Multi- granularity hierarchical attention fusion networks for reading comprehension and question answering. In Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers). Association for Computational Lin- guistics. Alex Warstadt, Amanpreet Singh, and Samuel R Bow- man. 2018. Neural network acceptability judg- ments. arXiv preprint arXiv:1805.12471. Adina Williams, Nikita Nangia, and Samuel R Bow- man. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google‚Äôs neural ma- chine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144. Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are features in deep neural networks? In Advances in neural information processing systems, pages 3320‚Äì3328. Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. 2018. QANet: Combining local convolution
arXiv:1609.08144. Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are features in deep neural networks? In Advances in neural information processing systems, pages 3320‚Äì3328. Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. 2018. QANet: Combining local convolution with global self-attention for reading comprehen- sion. In ICLR. Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A large-scale adversarial dataset for grounded commonsense inference. In Proceed- ings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP). Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut- dinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19‚Äì27. Appendix for ‚ÄúBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding‚Äù We organize the appendix into three sections: ‚Ä¢ Additional implementation details for BERT are presented in Appendix A; ‚Ä¢ Additional details for our experiments are presented in Appendix B; and ‚Ä¢ Additional ablation studies are presented in Appendix C. We present additional ablation studies for BERT including: ‚Äì Effect of Number of Training
appendix into three sections: ‚Ä¢ Additional implementation details for BERT are presented in Appendix A; ‚Ä¢ Additional details for our experiments are presented in Appendix B; and ‚Ä¢ Additional ablation studies are presented in Appendix C. We present additional ablation studies for BERT including: ‚Äì Effect of Number of Training Steps; and ‚Äì Ablation for Different Masking Proce- dures. A Additional Details for BERT A.1 Illustration of the Pre-training Tasks We provide examples of the pre-training tasks in the following. Masked LM and the Masking Procedure As- suming the unlabeled sentence is my dog is hairy, and during the random masking procedure we chose the 4-th token (which corresponding to hairy), our masking procedure can be further il- lustrated by ‚Ä¢ 80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy ‚Üí my dog is [MASK] ‚Ä¢ 10% of the time: Replace the word with a random word, e.g., my dog is hairy ‚Üímy dog is apple ‚Ä¢ 10% of the time: Keep the word un- changed, e.g., my dog is hairy ‚Üímy dog is hairy. The purpose of this is to bias the representation towards the actual observed word. The advantage of this
with a random word, e.g., my dog is hairy ‚Üímy dog is apple ‚Ä¢ 10% of the time: Keep the word un- changed, e.g., my dog is hairy ‚Üímy dog is hairy. The purpose of this is to bias the representation towards the actual observed word. The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been re- placed by random words, so it is forced to keep a distributional contextual representation of ev- ery input token. Additionally, because random replacement only occurs for 1.5% of all tokens (i.e., 10% of 15%), this does not seem to harm the model‚Äôs language understanding capability. In Section C.2, we evaluate the impact this proce- dure. Compared to standard langauge model training, the masked LM only make predictions on 15% of tokens in each batch, which suggests that more pre-training steps may be required for the model BERT (Ours) Trm Trm Trm Trm Trm Trm ... ... Trm Trm Trm Trm Trm Trm ... ... OpenAI GPT Lstm ELMo Lstm Lstm Lstm Lstm Lstm Lstm Lstm Lstm Lstm Lstm Lstm T1 T2 TN ... ... ... ... ... E1
pre-training steps may be required for the model BERT (Ours) Trm Trm Trm Trm Trm Trm ... ... Trm Trm Trm Trm Trm Trm ... ... OpenAI GPT Lstm ELMo Lstm Lstm Lstm Lstm Lstm Lstm Lstm Lstm Lstm Lstm Lstm T1 T2 TN ... ... ... ... ... E1 E2 EN ... T1 T2 TN ... E1 E2 EN ... T1 T2 TN ... E1 E2 EN ... Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT uses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to- left LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly conditioned on both left and right context in all layers. In addition to the architecture differences, BERT and OpenAI GPT are Ô¨Åne-tuning approaches, while ELMo is a feature-based approach. to converge. In Section C.1 we demonstrate that MLM does converge marginally slower than a left- to-right model (which predicts every token), but the empirical improvements of the MLM model far outweigh the increased training cost. Next Sentence Prediction The next sentence prediction task can be illustrated in the following examples. Input = [CLS] the man went to
MLM does converge marginally slower than a left- to-right model (which predicts every token), but the empirical improvements of the MLM model far outweigh the increased training cost. Next Sentence Prediction The next sentence prediction task can be illustrated in the following examples. Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP] Label = IsNext Input = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP] Label = NotNext A.2 Pre-training Procedure To generate each training input sequence, we sam- ple two spans of text from the corpus, which we refer to as ‚Äúsentences‚Äù even though they are typ- ically much longer than single sentences (but can be shorter also). The Ô¨Årst sentence receives the A embedding and the second receives the B embed- ding. 50% of the time B is the actual next sentence that follows A and 50% of the time it is a random sentence, which is done for the ‚Äúnext sentence pre- diction‚Äù task. They are sampled such that the com- bined length is ‚â§512 tokens. The LM masking is applied after WordPiece tokenization with a uni- form masking rate of 15%,
follows A and 50% of the time it is a random sentence, which is done for the ‚Äúnext sentence pre- diction‚Äù task. They are sampled such that the com- bined length is ‚â§512 tokens. The LM masking is applied after WordPiece tokenization with a uni- form masking rate of 15%, and no special consid- eration given to partial word pieces. We train with batch size of 256 sequences (256 sequences * 512 tokens = 128,000 tokens/batch) for 1,000,000 steps, which is approximately 40 epochs over the 3.3 billion word corpus. We use Adam with learning rate of 1e-4, Œ≤1 = 0.9, Œ≤2 = 0.999, L2 weight decay of 0.01, learning rate warmup over the Ô¨Årst 10,000 steps, and linear decay of the learning rate. We use a dropout prob- ability of 0.1 on all layers. We use a gelu acti- vation (Hendrycks and Gimpel, 2016) rather than the standard relu, following OpenAI GPT. The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood. Training of BERTBASE was performed on 4 Cloud TPUs in Pod conÔ¨Åguration (16 TPU chips total).13 Training of BERTLARGE was performed on 16 Cloud TPUs (64 TPU chips
following OpenAI GPT. The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood. Training of BERTBASE was performed on 4 Cloud TPUs in Pod conÔ¨Åguration (16 TPU chips total).13 Training of BERTLARGE was performed on 16 Cloud TPUs (64 TPU chips total). Each pre- training took 4 days to complete. Longer sequences are disproportionately expen- sive because attention is quadratic to the sequence length. To speed up pretraing in our experiments, we pre-train the model with sequence length of 128 for 90% of the steps. Then, we train the rest 10% of the steps of sequence of 512 to learn the positional embeddings. A.3 Fine-tuning Procedure For Ô¨Åne-tuning, most model hyperparameters are the same as in pre-training, with the exception of the batch size, learning rate, and number of train- ing epochs. The dropout probability was always kept at 0.1. The optimal hyperparameter values are task-speciÔ¨Åc, but we found the following range of possible values to work well across all tasks: ‚Ä¢ Batch size: 16, 32 13https://cloudplatform.googleblog.com/2018/06/Cloud- TPU-now-offers-preemptible-pricing-and-global- availability.html ‚Ä¢ Learning rate (Adam): 5e-5, 3e-5, 2e-5 ‚Ä¢ Number of epochs: 2, 3, 4 We also observed that large data sets (e.g.,
optimal hyperparameter values are task-speciÔ¨Åc, but we found the following range of possible values to work well across all tasks: ‚Ä¢ Batch size: 16, 32 13https://cloudplatform.googleblog.com/2018/06/Cloud- TPU-now-offers-preemptible-pricing-and-global- availability.html ‚Ä¢ Learning rate (Adam): 5e-5, 3e-5, 2e-5 ‚Ä¢ Number of epochs: 2, 3, 4 We also observed that large data sets (e.g., 100k+ labeled training examples) were far less sensitive to hyperparameter choice than small data sets. Fine-tuning is typically very fast, so it is rea- sonable to simply run an exhaustive search over the above parameters and choose the model that performs best on the development set. A.4 Comparison of BERT, ELMo ,and OpenAI GPT Here we studies the differences in recent popular representation learning models including ELMo, OpenAI GPT and BERT. The comparisons be- tween the model architectures are shown visually in Figure 3. Note that in addition to the architec- ture differences, BERT and OpenAI GPT are Ô¨Åne- tuning approaches, while ELMo is a feature-based approach. The most comparable existing pre-training method to BERT is OpenAI GPT, which trains a left-to-right Transformer LM on a large text cor- pus. In fact, many of the design decisions in BERT were intentionally made to make it as close to GPT as
approaches, while ELMo is a feature-based approach. The most comparable existing pre-training method to BERT is OpenAI GPT, which trains a left-to-right Transformer LM on a large text cor- pus. In fact, many of the design decisions in BERT were intentionally made to make it as close to GPT as possible so that the two methods could be minimally compared. The core argument of this work is that the bi-directionality and the two pre- training tasks presented in Section 3.1 account for the majority of the empirical improvements, but we do note that there are several other differences between how BERT and GPT were trained: ‚Ä¢ GPT is trained on the BooksCorpus (800M words); BERT is trained on the BooksCor- pus (800M words) and Wikipedia (2,500M words). ‚Ä¢ GPT uses a sentence separator ([SEP]) and classiÔ¨Åer token ([CLS]) which are only in- troduced at Ô¨Åne-tuning time; BERT learns [SEP], [CLS] and sentence A/B embed- dings during pre-training. ‚Ä¢ GPT was trained for 1M steps with a batch size of 32,000 words; BERT was trained for 1M steps with a batch size of 128,000 words. ‚Ä¢ GPT used the same learning rate of 5e-5 for all Ô¨Åne-tuning experiments; BERT chooses a
and sentence A/B embed- dings during pre-training. ‚Ä¢ GPT was trained for 1M steps with a batch size of 32,000 words; BERT was trained for 1M steps with a batch size of 128,000 words. ‚Ä¢ GPT used the same learning rate of 5e-5 for all Ô¨Åne-tuning experiments; BERT chooses a task-speciÔ¨Åc Ô¨Åne-tuning learning rate which performs the best on the development set. To isolate the effect of these differences, we per- form ablation experiments in Section 5.1 which demonstrate that the majority of the improvements are in fact coming from the two pre-training tasks and the bidirectionality they enable. A.5 Illustrations of Fine-tuning on Different Tasks The illustration of Ô¨Åne-tuning BERT on different tasks can be seen in Figure 4. Our task-speciÔ¨Åc models are formed by incorporating BERT with one additional output layer, so a minimal num- ber of parameters need to be learned from scratch. Among the tasks, (a) and (b) are sequence-level tasks while (c) and (d) are token-level tasks. In the Ô¨Ågure, E represents the input embedding, Ti represents the contextual representation of token i, [CLS] is the special symbol for classiÔ¨Åcation out- put, and [SEP] is the special symbol to separate non-consecutive token sequences. B Detailed
and (b) are sequence-level tasks while (c) and (d) are token-level tasks. In the Ô¨Ågure, E represents the input embedding, Ti represents the contextual representation of token i, [CLS] is the special symbol for classiÔ¨Åcation out- put, and [SEP] is the special symbol to separate non-consecutive token sequences. B Detailed Experimental Setup B.1 Detailed Descriptions for the GLUE Benchmark Experiments. Our GLUE results in Table1 are obtained from https://gluebenchmark.com/ leaderboard and https://blog. openai.com/language-unsupervised. The GLUE benchmark includes the following datasets, the descriptions of which were originally summarized in Wang et al. (2018a): MNLI Multi-Genre Natural Language Inference is a large-scale, crowdsourced entailment classiÔ¨Å- cation task (Williams et al., 2018). Given a pair of sentences, the goal is to predict whether the sec- ond sentence is an entailment, contradiction, or neutral with respect to the Ô¨Årst one. QQP Quora Question Pairs is a binary classiÔ¨Å- cation task where the goal is to determine if two questions asked on Quora are semantically equiv- alent (Chen et al., 2018). QNLI Question Natural Language Inference is a version of the Stanford Question Answering Dataset (Rajpurkar et al., 2016) which has been converted to a binary classiÔ¨Åcation task (Wang et al., 2018a). The positive examples
to determine if two questions asked on Quora are semantically equiv- alent (Chen et al., 2018). QNLI Question Natural Language Inference is a version of the Stanford Question Answering Dataset (Rajpurkar et al., 2016) which has been converted to a binary classiÔ¨Åcation task (Wang et al., 2018a). The positive examples are (ques- tion, sentence) pairs which do contain the correct answer, and the negative examples are (question, sentence) from the same paragraph which do not contain the answer. BERT E[CLS] E1 E[SEP] ... EN E1‚Äô ... EM‚Äô C T1 T[SEP] ... TN T1‚Äô ... TM‚Äô [CLS] Tok 1 [SEP] ... Tok N Tok 1 ... Tok M Question Paragraph BERT E[CLS] E1 E2 EN C T1 T2 TN Single Sentence ... ... BERT Tok 1 Tok 2 Tok N ... [CLS] E[CLS] E1 E2 EN C T1 T2 TN Single Sentence B-PER O O ... ... E[CLS] E1 E[SEP] Class Label ... EN E1‚Äô ... EM‚Äô C T1 T[SEP] ... TN T1‚Äô ... TM‚Äô Start/End Span Class Label BERT Tok 1 Tok 2 Tok N ... [CLS] Tok 1 [CLS] [CLS] Tok 1 [SEP] ... Tok N Tok 1 ... Tok M Sentence 1 ... Sentence 2 Figure 4: Illustrations
Label ... EN E1‚Äô ... EM‚Äô C T1 T[SEP] ... TN T1‚Äô ... TM‚Äô Start/End Span Class Label BERT Tok 1 Tok 2 Tok N ... [CLS] Tok 1 [CLS] [CLS] Tok 1 [SEP] ... Tok N Tok 1 ... Tok M Sentence 1 ... Sentence 2 Figure 4: Illustrations of Fine-tuning BERT on Different Tasks. SST-2 The Stanford Sentiment Treebank is a binary single-sentence classiÔ¨Åcation task consist- ing of sentences extracted from movie reviews with human annotations of their sentiment (Socher et al., 2013). CoLA The Corpus of Linguistic Acceptability is a binary single-sentence classiÔ¨Åcation task, where the goal is to predict whether an English sentence is linguistically ‚Äúacceptable‚Äù or not (Warstadt et al., 2018). STS-B The Semantic Textual Similarity Bench- mark is a collection of sentence pairs drawn from news headlines and other sources (Cer et al., 2017). They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning. MRPC Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semanti- cally equivalent (Dolan and Brockett, 2005). RTE Recognizing Textual Entailment is
denoting how similar the two sentences are in terms of semantic meaning. MRPC Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semanti- cally equivalent (Dolan and Brockett, 2005). RTE Recognizing Textual Entailment is a bi- nary entailment task similar to MNLI, but with much less training data (Bentivogli et al., 2009).14 WNLI Winograd NLI is a small natural lan- guage inference dataset (Levesque et al., 2011). The GLUE webpage notes that there are issues with the construction of this dataset, 15 and every trained system that‚Äôs been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class. We therefore ex- clude this set to be fair to OpenAI GPT. For our GLUE submission, we always predicted the ma- 14Note that we only report single-task Ô¨Åne-tuning results in this paper. A multitask Ô¨Åne-tuning approach could poten- tially push the performance even further. For example, we did observe substantial improvements on RTE from multi- task training with MNLI. 15https://gluebenchmark.com/faq jority class. C Additional Ablation Studies C.1 Effect of Number of Training Steps Figure 5 presents MNLI Dev accuracy after
this paper. A multitask Ô¨Åne-tuning approach could poten- tially push the performance even further. For example, we did observe substantial improvements on RTE from multi- task training with MNLI. 15https://gluebenchmark.com/faq jority class. C Additional Ablation Studies C.1 Effect of Number of Training Steps Figure 5 presents MNLI Dev accuracy after Ô¨Åne- tuning from a checkpoint that has been pre-trained for k steps. This allows us to answer the following questions: 1. Question: Does BERT really need such a large amount of pre-training (128,000 words/batch * 1,000,000 steps) to achieve high Ô¨Åne-tuning accuracy? Answer: Yes, BERTBASE achieves almost 1.0% additional accuracy on MNLI when trained on 1M steps compared to 500k steps. 2. Question: Does MLM pre-training converge slower than LTR pre-training, since only 15% of words are predicted in each batch rather than every word? Answer: The MLM model does converge slightly slower than the LTR model. How- ever, in terms of absolute accuracy the MLM model begins to outperform the LTR model almost immediately. C.2 Ablation for Different Masking Procedures In Section 3.1, we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model (MLM) objective. The following is an
terms of absolute accuracy the MLM model begins to outperform the LTR model almost immediately. C.2 Ablation for Different Masking Procedures In Section 3.1, we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model (MLM) objective. The following is an ablation study to evaluate the effect of different masking strategies. 200 400 600 800 1,000 76 78 80 82 84 Pre-training Steps (Thousands) MNLI Dev Accuracy BERTBASE (Masked LM) BERTBASE (Left-to-Right) Figure 5: Ablation over number of training steps. This shows the MNLI accuracy after Ô¨Åne-tuning, starting from model parameters that have been pre-trained for k steps. The x-axis is the value of k. Note that the purpose of the masking strategies is to reduce the mismatch between pre-training and Ô¨Åne-tuning, as the [MASK] symbol never ap- pears during the Ô¨Åne-tuning stage. We report the Dev results for both MNLI and NER. For NER, we report both Ô¨Åne-tuning and feature-based ap- proaches, as we expect the mismatch will be am- pliÔ¨Åed for the feature-based approach as the model will not have the chance to adjust the representa- tions. Masking Rates Dev Set Results MASK SAME RND MNLI NER Fine-tune Fine-tune
and NER. For NER, we report both Ô¨Åne-tuning and feature-based ap- proaches, as we expect the mismatch will be am- pliÔ¨Åed for the feature-based approach as the model will not have the chance to adjust the representa- tions. Masking Rates Dev Set Results MASK SAME RND MNLI NER Fine-tune Fine-tune Feature-based 80% 10% 10% 84.2 95.4 94.9 100% 0% 0% 84.3 94.9 94.0 80% 0% 20% 84.1 95.2 94.6 80% 20% 0% 84.4 95.2 94.7 0% 20% 80% 83.7 94.8 94.6 0% 0% 100% 83.6 94.9 94.6 Table 8: Ablation over different masking strategies. The results are presented in Table 8. In the table, MASK means that we replace the target token with the [MASK] symbol for MLM; SAME means that we keep the target token as is; RND means that we replace the target token with another random token. The numbers in the left part of the table repre- sent the probabilities of the speciÔ¨Åc strategies used during MLM pre-training (BERT uses 80%, 10%, 10%). The right part of the paper represents the Dev set results. For the feature-based approach, we concatenate the last 4 layers of BERT as the features, which was shown to be the best approach
sent the probabilities of the speciÔ¨Åc strategies used during MLM pre-training (BERT uses 80%, 10%, 10%). The right part of the paper represents the Dev set results. For the feature-based approach, we concatenate the last 4 layers of BERT as the features, which was shown to be the best approach in Section 5.3. From the table it can be seen that Ô¨Åne-tuning is surprisingly robust to different masking strategies. However, as expected, using only the MASK strat- egy was problematic when applying the feature- based approach to NER. Interestingly, using only the RND strategy performs much worse than our strategy as well.
Language Models are Few-Shot Learners Tom B. Brown‚àó Benjamin Mann‚àó Nick Ryder‚àó Melanie Subbiah‚àó Jared Kaplan‚Ä† Prafulla Dhariwal Arvind Neelakantan Pranav Shyam Girish Sastry Amanda Askell Sandhini Agarwal Ariel Herbert-Voss Gretchen Krueger Tom Henighan Rewon Child Aditya Ramesh Daniel M. Ziegler Jeffrey Wu Clemens Winter Christopher Hesse Mark Chen Eric Sigler Mateusz Litwin Scott Gray Benjamin Chess Jack Clark Christopher Berner Sam McCandlish Alec Radford Ilya Sutskever Dario Amodei OpenAI Abstract Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by Ô¨Åne-tuning on a speciÔ¨Åc task. While typically task-agnostic in architecture, this method still requires task-speciÔ¨Åc Ô¨Åne-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions ‚Äì something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art Ô¨Åne- tuning approaches. SpeciÔ¨Åcally, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For
show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art Ô¨Åne- tuning approaches. SpeciÔ¨Åcally, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or Ô¨Åne-tuning, with tasks and few-shot demonstrations speciÔ¨Åed purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-Ô¨Çy reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3‚Äôs few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we Ô¨Ånd that GPT-3 can generate samples of news articles which human evaluators have difÔ¨Åculty distinguishing from articles written by humans. We discuss broader societal impacts of this Ô¨Ånding and of GPT-3 in general. ‚àóEqual contribution ‚Ä†Johns Hopkins University, OpenAI Author contributions listed at end of paper. arXiv:2005.14165v4 [cs.CL] 22 Jul 2020
that GPT-3 can generate samples of news articles which human evaluators have difÔ¨Åculty distinguishing from articles written by humans. We discuss broader societal impacts of this Ô¨Ånding and of GPT-3 in general. ‚àóEqual contribution ‚Ä†Johns Hopkins University, OpenAI Author contributions listed at end of paper. arXiv:2005.14165v4 [cs.CL] 22 Jul 2020 Contents 1 Introduction 3 2 Approach 6 2.1 Model and Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.2 Training Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.3 Training Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . 8 2.3 Training Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.4 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 3 Results 10 3.1 Language Modeling, Cloze, and Completion Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.2 Closed Book Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.3 Translation . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.3 Translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 3.4 Winograd-Style Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 3.5 Common Sense Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 3.6 Reading Comprehension . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . 17 3.6 Reading Comprehension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3.7 SuperGLUE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3.8 NLI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3.9 Synthetic and Qualitative Tasks . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3.9 Synthetic and Qualitative Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 4 Measuring and Preventing Memorization Of Benchmarks 29 5 Limitations 33 6 Broader Impacts 34 6.1 Misuse of Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 6.2 Fairness, Bias, and Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 6.3 Energy Usage . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . 36 6.3 Energy Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 7 Related Work 39 8 Conclusion 40 A Details of Common Crawl Filtering 43 B Details of Model Training 43 C Details of Test Set Contamination Studies 43 D Total Compute Used to Train Language Models 46 E Human Quality Assessment of Synthetic News Articles 46 F Additional Samples from GPT-3 48 G Details of Task Phrasing and SpeciÔ¨Åcations 50 H Results on All Tasks for All Model Sizes 63 2 1 Introduction Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly Ô¨Çexible and task-agnostic ways for downstream transfer. First, single-layer representations were learned using word vectors [MCCD13, PSM14] and fed to task-speciÔ¨Åc architectures, then RNNs with multiple layers of representations and contextual state were used to form
have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly Ô¨Çexible and task-agnostic ways for downstream transfer. First, single-layer representations were learned using word vectors [MCCD13, PSM14] and fed to task-speciÔ¨Åc architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations [DL15, MBXS17, PNZtY18] (though still applied to task-speciÔ¨Åc architectures), and more recently pre-trained recurrent or transformer language models [VSP+17] have been directly Ô¨Åne-tuned, entirely removing the need for task-speciÔ¨Åc architectures [RNSS18, DCLT18, HR18]. This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms [RSR+19, LOG+19, YDY+19, LCG+19]. However, a major limitation to this approach is that while the architecture is task-agnostic, there is still a need for task-speciÔ¨Åc datasets and task-speciÔ¨Åc Ô¨Åne-tuning: to achieve strong performance on a desired task typically requires Ô¨Åne-tuning on a dataset of thousands to hundreds of thousands of examples speciÔ¨Åc to that task. Removing this limitation would be desirable, for several reasons. First, from a practical perspective, the need for a large dataset of labeled examples for every
achieve strong performance on a desired task typically requires Ô¨Åne-tuning on a dataset of thousands to hundreds of thousands of examples speciÔ¨Åc to that task. Removing this limitation would be desirable, for several reasons. First, from a practical perspective, the need for a large dataset of labeled examples for every new task limits the applicability of language models. There exists a very wide range of possible useful language tasks, encompassing anything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story. For many of these tasks it is difÔ¨Åcult to collect a large supervised training dataset, especially when the process must be repeated for every new task. Second, the potential to exploit spurious correlations in training data fundamentally grows with the expressiveness of the model and the narrowness of the training distribution. This can create problems for the pre-training plus Ô¨Åne-tuning paradigm, where models are designed to be large to absorb information during pre-training, but are then Ô¨Åne-tuned on very narrow task distributions. For instance [HLW+20] observe that larger models do not necessarily generalize better out-of-distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is
designed to be large to absorb information during pre-training, but are then Ô¨Åne-tuned on very narrow task distributions. For instance [HLW+20] observe that larger models do not necessarily generalize better out-of-distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly speciÔ¨Åc to the training distribution and does not generalize well outside it [YdC+19, MPL19]. Thus, the performance of Ô¨Åne-tuned models on speciÔ¨Åc benchmarks, even when it is nominally at human-level, may exaggerate actual performance on the underlying task [GSL+18, NK19]. Third, humans do not require large supervised datasets to learn most language tasks ‚Äì a brief directive in natural language (e.g. ‚Äúplease tell me if this sentence describes something happy or something sad‚Äù) or at most a tiny number of demonstrations (e.g. ‚Äúhere are two examples of people acting brave; please give a third example of bravery‚Äù) is often Figure 1.1: Language model meta-learning. During unsupervised pre-training, a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term ‚Äúin-context learning‚Äù to describe the inner loop of
Figure 1.1: Language model meta-learning. During unsupervised pre-training, a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term ‚Äúin-context learning‚Äù to describe the inner loop of this process, which occurs within the forward-pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a model would see during pre-training, but are intended to show that there are sometimes repeated sub-tasks embedded within a single sequence. 3 Figure 1.2: Larger models make increasingly efÔ¨Åcient use of in-context information. We show in-context learning performance on a simple task requiring the model to remove random symbols from a word, both with and without a natural language task description (see Sec. 3.9.2). The steeper ‚Äúin-context learning curves‚Äù for large models demonstrate improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range of tasks. sufÔ¨Åcient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing to a conceptual limitation in our current NLP techniques, this adaptability
to learn a task from contextual information. We see qualitatively similar behavior across a wide range of tasks. sufÔ¨Åcient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing to a conceptual limitation in our current NLP techniques, this adaptability has practical advantages ‚Äì it allows humans to seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy dialogue. To be broadly useful, we would someday like our NLP systems to have this same Ô¨Çuidity and generality. One potential route towards addressing these issues is meta-learning1 ‚Äì which in the context of language models means the model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities at inference time to rapidly adapt to or recognize the desired task (illustrated in Figure 1.1). Recent work [RWC+19] attempts to do this via what we call ‚Äúin-context learning‚Äù, using the text input of a pretrained language model as a form of task speciÔ¨Åcation: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances
[RWC+19] attempts to do this via what we call ‚Äúin-context learning‚Äù, using the text input of a pretrained language model as a form of task speciÔ¨Åcation: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next. While it has shown some initial promise, this approach still achieves results far inferior to Ô¨Åne-tuning ‚Äì for example [RWC+19] achieves only 4% on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points behind the state of the art. Meta-learning clearly requires substantial improvement in order to be viable as a practical method of solving language tasks. Another recent trend in language modeling may offer a way forward. In recent years the capacity of transformer language models has increased substantially, from 100 million parameters [RNSS18], to 300 million parameters [DCLT18], to 1.5 billion parameters [RWC+19], to 8 billion parameters [SPP+19], 11 billion parameters [RSR+19], and Ô¨Ånally 17 billion parameters [Tur20]. Each increase has brought improvements in text synthesis and/or downstream NLP tasks, and there is evidence suggesting that log loss, which correlates well with many
to 300 million parameters [DCLT18], to 1.5 billion parameters [RWC+19], to 8 billion parameters [SPP+19], 11 billion parameters [RSR+19], and Ô¨Ånally 17 billion parameters [Tur20]. Each increase has brought improvements in text synthesis and/or downstream NLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a smooth trend of improvement with scale [KMH+20]. Since in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale. 1In the context of language models this has sometimes been called ‚Äúzero-shot transfer‚Äù, but this term is potentially ambiguous: the method is ‚Äúzero-shot‚Äù in the sense that no gradient updates are performed, but it often involves providing inference-time demonstrations to the model, so is not truly learning from zero examples. To avoid this confusion, we use the term ‚Äúmeta-learning‚Äù to capture the inner-loop / outer-loop structure of the general method, and the term ‚Äúin context-learning‚Äù to refer to the inner loop of meta-learning. We further specialize the description to ‚Äúzero-shot‚Äù, ‚Äúone-shot‚Äù, or ‚Äúfew-shot‚Äù depending on how many demonstrations are provided at inference time. These terms are intended to remain agnostic
capture the inner-loop / outer-loop structure of the general method, and the term ‚Äúin context-learning‚Äù to refer to the inner loop of meta-learning. We further specialize the description to ‚Äúzero-shot‚Äù, ‚Äúone-shot‚Äù, or ‚Äúfew-shot‚Äù depending on how many demonstrations are provided at inference time. These terms are intended to remain agnostic on the question of whether the model learns new tasks from scratch at inference time or simply recognizes patterns seen during training ‚Äì this is an important issue which we discuss later in the paper, but ‚Äúmeta-learning‚Äù is intended to encompass both possibilities, and simply describes the inner-outer loop structure. 4 Figure 1.3: Aggregate performance for all 42 accuracy-denominated benchmarks While zero-shot performance improves steadily with model size, few-shot performance increases more rapidly, demonstrating that larger models are more proÔ¨Åcient at in-context learning. See Figure 3.8 for a more detailed analysis on SuperGLUE, a standard NLP benchmark suite. In this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call GPT-3, and measuring its in-context learning abilities. SpeciÔ¨Åcally, we evaluate GPT-3 on over two dozen NLP datasets, as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be
paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call GPT-3, and measuring its in-context learning abilities. SpeciÔ¨Åcally, we evaluate GPT-3 on over two dozen NLP datasets, as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training set. For each task, we evaluate GPT-3 under 3 conditions: (a) ‚Äúfew-shot learning‚Äù, or in-context learning where we allow as many demonstrations as will Ô¨Åt into the model‚Äôs context window (typically 10 to 100), (b) ‚Äúone-shot learning‚Äù, where we allow only one demonstration, and (c) ‚Äúzero-shot‚Äù learning, where no demonstrations are allowed and only an instruction in natural language is given to the model. GPT-3 could also in principle be evaluated in the traditional Ô¨Åne-tuning setting, but we leave this to future work. Figure 1.2 illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word. Model performance improves with the addition of a natural language task description, and with the number of examples in the model‚Äôs context, K. Few-shot learning also improves dramatically with model size. Though the results in this case
a simple task requiring the model to remove extraneous symbols from a word. Model performance improves with the addition of a natural language task description, and with the number of examples in the model‚Äôs context, K. Few-shot learning also improves dramatically with model size. Though the results in this case are particularly striking, the general trends with both model size and number of examples in-context hold for most tasks we study. We emphasize that these ‚Äúlearning‚Äù curves involve no gradient updates or Ô¨Åne-tuning, just increasing numbers of demonstrations given as conditioning. Broadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot setting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held by Ô¨Åne-tuned models). For example, GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0 F1 on CoQA in the one-shot setting, 85.0 F1 in the few-shot setting. Similarly, GPT-3 achieves 64.3% accuracy on TriviaQA in the zero-shot setting, 68.0% in the one-shot setting, and 71.2% in the few-shot setting, the last of which is state-of-the-art relative to Ô¨Åne-tuned models operating in the same closed-book setting. GPT-3 also displays one-shot and few-shot proÔ¨Åciency at tasks
the few-shot setting. Similarly, GPT-3 achieves 64.3% accuracy on TriviaQA in the zero-shot setting, 68.0% in the one-shot setting, and 71.2% in the few-shot setting, the last of which is state-of-the-art relative to Ô¨Åne-tuned models operating in the same closed-book setting. GPT-3 also displays one-shot and few-shot proÔ¨Åciency at tasks designed to test rapid adaption or on-the-Ô¨Çy reasoning, which include unscrambling words, performing arithmetic, and using novel words in a sentence after seeing them deÔ¨Åned only once. We also show that in the few-shot setting, GPT-3 can generate synthetic news articles which human evaluators have difÔ¨Åculty distinguishing from human-generated articles. At the same time, we also Ô¨Ånd some tasks on which few-shot performance struggles, even at the scale of GPT-3. This includes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE or QuAC. By presenting a broad characterization of GPT-3‚Äôs strengths and weaknesses, including these limitations, we hope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed. A heuristic sense of the overall results can be seen in Figure 1.3, which aggregates the various tasks (though it should not be seen as a rigorous or
these limitations, we hope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed. A heuristic sense of the overall results can be seen in Figure 1.3, which aggregates the various tasks (though it should not be seen as a rigorous or meaningful benchmark in itself). 5 We also undertake a systematic study of ‚Äúdata contamination‚Äù ‚Äì a growing problem when training high capacity models on datasets such as Common Crawl, which can potentially include content from test datasets simply because such content often exists on the web. In this paper we develop systematic tools to measure data contamination and quantify its distorting effects. Although we Ô¨Ånd that data contamination has a minimal effect on GPT-3‚Äôs performance on most datasets, we do identify a few datasets where it could be inÔ¨Çating results, and we either do not report results on these datasets or we note them with an asterisk, depending on the severity. In addition to all the above, we also train a series of smaller models (ranging from 125 million parameters to 13 billion parameters) in order to compare their performance to GPT-3 in the zero, one and few-shot settings. Broadly,
or we note them with an asterisk, depending on the severity. In addition to all the above, we also train a series of smaller models (ranging from 125 million parameters to 13 billion parameters) in order to compare their performance to GPT-3 in the zero, one and few-shot settings. Broadly, for most tasks we Ô¨Ånd relatively smooth scaling with model capacity in all three settings; one notable pattern is that the gap between zero-, one-, and few-shot performance often grows with model capacity, perhaps suggesting that larger models are more proÔ¨Åcient meta-learners. Finally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and broader societal impacts, and attempt a preliminary analysis of GPT-3‚Äôs characteristics in this regard. The remainder of this paper is organized as follows. In Section 2, we describe our approach and methods for training GPT-3 and evaluating it. Section 3 presents results on the full range of tasks in the zero-, one- and few-shot settings. Section 4 addresses questions of data contamination (train-test overlap). Section 5 discusses limitations of GPT-3. Section 6 discusses broader impacts. Section 7 reviews related work and Section 8 concludes. 2 Approach Our basic pre-training approach, including
results on the full range of tasks in the zero-, one- and few-shot settings. Section 4 addresses questions of data contamination (train-test overlap). Section 5 discusses limitations of GPT-3. Section 6 discusses broader impacts. Section 7 reviews related work and Section 8 concludes. 2 Approach Our basic pre-training approach, including model, data, and training, is similar to the process described in [RWC+19], with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training. Our use of in-context learning is also similar to [RWC+19], but in this work we systematically explore different settings for learning within the context. Therefore, we start this section by explicitly deÔ¨Åning and contrasting the different settings that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a spectrum of how much task-speciÔ¨Åc data they tend to rely on. SpeciÔ¨Åcally, we can identify at least four points on this spectrum (see Figure 2.1 for an illustration): ‚Ä¢ Fine-Tuning (FT) has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset speciÔ¨Åc to the desired task. Typically
SpeciÔ¨Åcally, we can identify at least four points on this spectrum (see Figure 2.1 for an illustration): ‚Ä¢ Fine-Tuning (FT) has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset speciÔ¨Åc to the desired task. Typically thousands to hundreds of thousands of labeled examples are used. The main advantage of Ô¨Åne-tuning is strong performance on many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution [MPL19], and the potential to exploit spurious features of the training data [GSL+18, NK19], potentially resulting in an unfair comparison with human performance. In this work we do not Ô¨Åne-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be Ô¨Åne-tuned in principle and this is a promising direction for future work. ‚Ä¢ Few-Shot (FS) is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning [RWC+19], but no weight updates are allowed. As shown in Figure 2.1, for a typical dataset an example has a context and
term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning [RWC+19], but no weight updates are allowed. As shown in Figure 2.1, for a typical dataset an example has a context and a desired completion (for example an English sentence and the French translation), and few-shot works by giving K examples of context and completion, and then one Ô¨Ånal example of context, with the model expected to provide the completion. We typically set K in the range of 10 to 100 as this is how many examples can Ô¨Åt in the model‚Äôs context window (nctx = 2048). The main advantages of few-shot are a major reduction in the need for task-speciÔ¨Åc data and reduced potential to learn an overly narrow distribution from a large but narrow Ô¨Åne-tuning dataset. The main disadvantage is that results from this method have so far been much worse than state-of-the-art Ô¨Åne-tuned models. Also, a small amount of task speciÔ¨Åc data is still required. As indicated by the name, few-shot learning as described here for language models is related to few-shot learning as used in other contexts in
that results from this method have so far been much worse than state-of-the-art Ô¨Åne-tuned models. Also, a small amount of task speciÔ¨Åc data is still required. As indicated by the name, few-shot learning as described here for language models is related to few-shot learning as used in other contexts in ML [HYC01, VBL+16] ‚Äì both involve learning based on a broad distribution of tasks (in this case implicit in the pre-training data) and then rapidly adapting to a new task. ‚Ä¢ One-Shot (1S) is the same as few-shot except that only one demonstration is allowed, in addition to a natural language description of the task, as shown in Figure 1. The reason to distinguish one-shot from few-shot and zero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans. For example, when asking humans to generate a dataset on a human worker service (for example Mechanical Turk), it is common to give one demonstration of the task. By contrast it is sometimes difÔ¨Åcult to communicate the content or format of a task if no examples are given. 6 Figure 2.1: Zero-shot, one-shot and few-shot, contrasted with traditional Ô¨Åne-tuning. The panels above show four
(for example Mechanical Turk), it is common to give one demonstration of the task. By contrast it is sometimes difÔ¨Åcult to communicate the content or format of a task if no examples are given. 6 Figure 2.1: Zero-shot, one-shot and few-shot, contrasted with traditional Ô¨Åne-tuning. The panels above show four methods for performing a task with a language model ‚Äì Ô¨Åne-tuning is the traditional method, whereas zero-, one-, and few-shot, which we study in this work, require the model to perform the task with only forward passes at test time. We typically present the model with a few dozen examples in the few shot setting. Exact phrasings for all task descriptions, examples and prompts can be found in Appendix G. ‚Ä¢ Zero-Shot (0S) is the same as one-shot except that no demonstrations are allowed, and the model is only given a natural language instruction describing the task. This method provides maximum convenience, potential for robustness, and avoidance of spurious correlations (unless they occur very broadly across the large corpus of pre-training data), but is also the most challenging setting. In some cases it may even be difÔ¨Åcult for humans to understand the format of the task without prior examples, so
maximum convenience, potential for robustness, and avoidance of spurious correlations (unless they occur very broadly across the large corpus of pre-training data), but is also the most challenging setting. In some cases it may even be difÔ¨Åcult for humans to understand the format of the task without prior examples, so this setting is in some cases ‚Äúunfairly hard‚Äù. For example, if someone is asked to ‚Äúmake a table of world records for the 200m dash‚Äù, this request can be ambiguous, as it may not be clear exactly what format the table should have or what should be included (and even with careful clariÔ¨Åcation, understanding precisely what is desired can be difÔ¨Åcult). Nevertheless, for at least some settings zero-shot is closest to how humans perform tasks ‚Äì for example, in the translation example in Figure 2.1, a human would likely know what to do from just the text instruction. Figure 2.1 shows the four methods using the example of translating English to French. In this paper we focus on zero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different problem settings which offer a varying trade-off between performance on speciÔ¨Åc benchmarks and sample efÔ¨Åciency.
shows the four methods using the example of translating English to French. In this paper we focus on zero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different problem settings which offer a varying trade-off between performance on speciÔ¨Åc benchmarks and sample efÔ¨Åciency. We especially highlight the few-shot results as many of them are only slightly behind state-of-the-art Ô¨Åne-tuned models. Ultimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance, and are important targets for future work. Sections 2.1-2.3 below give details on our models, training data, and training process respectively. Section 2.4 discusses the details of how we do few-shot, one-shot, and zero-shot evaluations. 7 Model Name nparams nlayers dmodel nheads dhead Batch Size Learning Rate GPT-3 Small 125M 12 768 12 64 0.5M 6.0 √ó 10‚àí4 GPT-3 Medium 350M 24 1024 16 64 0.5M 3.0 √ó 10‚àí4 GPT-3 Large 760M 24 1536 16 96 0.5M 2.5 √ó 10‚àí4 GPT-3 XL 1.3B 24 2048 24 128 1M 2.0 √ó 10‚àí4 GPT-3 2.7B 2.7B 32 2560 32 80 1M 1.6 √ó 10‚àí4 GPT-3 6.7B 6.7B 32 4096 32 128 2M 1.2 √ó 10‚àí4 GPT-3 13B 13.0B 40 5140
10‚àí4 GPT-3 Large 760M 24 1536 16 96 0.5M 2.5 √ó 10‚àí4 GPT-3 XL 1.3B 24 2048 24 128 1M 2.0 √ó 10‚àí4 GPT-3 2.7B 2.7B 32 2560 32 80 1M 1.6 √ó 10‚àí4 GPT-3 6.7B 6.7B 32 4096 32 128 2M 1.2 √ó 10‚àí4 GPT-3 13B 13.0B 40 5140 40 128 2M 1.0 √ó 10‚àí4 GPT-3 175B or ‚ÄúGPT-3‚Äù 175.0B 96 12288 96 128 3.2M 0.6 √ó 10‚àí4 Table 2.1: Sizes, architectures, and learning hyper-parameters (batch size in tokens and learning rate) of the models which we trained. All models were trained for a total of 300 billion tokens. 2.1 Model and Architectures We use the same model and architecture as GPT-2 [RWC+19], including the modiÔ¨Åed initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [CGRS19]. To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3. Previous work [KMH+20] suggests that with enough training data, scaling
To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3. Previous work [KMH+20] suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks. Table 2.1 shows the sizes and architectures of our 8 models. Here nparams is the total number of trainable parameters, nlayers is the total number of layers, dmodel is the number of units in each bottleneck layer (we always have the feedforward layer four times the size of the bottleneck layer, dÔ¨Ä= 4 ‚àódmodel), and dhead is the dimension of each attention head. All models use a context window of nctx = 2048 tokens. We partition the model across GPUs along both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural parameters for each model are chosen based on computational efÔ¨Åciency and load-balancing in the layout of models
models use a context window of nctx = 2048 tokens. We partition the model across GPUs along both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural parameters for each model are chosen based on computational efÔ¨Åciency and load-balancing in the layout of models across GPU‚Äôs. Previous work [KMH+20] suggests that validation loss is not strongly sensitive to these parameters within a reasonably broad range. 2.2 Training Dataset Datasets for language models have rapidly expanded, culminating in the Common Crawl dataset2 [RSR+19] constituting nearly a trillion words. This size of dataset is sufÔ¨Åcient to train our largest models without ever updating on the same sequence twice. However, we have found that unÔ¨Åltered or lightly Ô¨Åltered versions of Common Crawl tend to have lower quality than more curated datasets. Therefore, we took 3 steps to improve the average quality of our datasets: (1) we downloaded and Ô¨Åltered a version of CommonCrawl based on similarity to a range of high-quality reference corpora, (2) we performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of our held-out validation set as an accurate measure of overÔ¨Åtting, and (3) we
and Ô¨Åltered a version of CommonCrawl based on similarity to a range of high-quality reference corpora, (2) we performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of our held-out validation set as an accurate measure of overÔ¨Åtting, and (3) we also added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity. Details of the Ô¨Årst two points (processing of Common Crawl) are described in Appendix A. For the third, we added several curated high-quality datasets, including an expanded version of the WebText dataset [RWC+19], collected by scraping links over a longer period of time, and Ô¨Årst described in [KMH+20], two internet-based books corpora (Books1 and Books2) and English-language Wikipedia. Table 2.2 shows the Ô¨Ånal mixture of datasets that we used in training. The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before Ô¨Åltering and 570GB after Ô¨Åltering, roughly equivalent to 400 billion byte-pair-encoded tokens. Note that during training, datasets are not sampled in proportion to their size, but rather datasets we view as higher-quality are sampled more frequently, such that CommonCrawl and Books2
2016 to 2019, constituting 45TB of compressed plaintext before Ô¨Åltering and 570GB after Ô¨Åltering, roughly equivalent to 400 billion byte-pair-encoded tokens. Note that during training, datasets are not sampled in proportion to their size, but rather datasets we view as higher-quality are sampled more frequently, such that CommonCrawl and Books2 datasets are sampled less than once during training, but the other datasets are sampled 2-3 times. This essentially accepts a small amount of overÔ¨Åtting in exchange for higher quality training data. 2https://commoncrawl.org/the-data/ 8 Figure 2.2: Total compute used during training. Based on the analysis in Scaling Laws For Neural Language Models [KMH+20] we train much larger models on many fewer tokens than is typical. As a consequence, although GPT-3 3B is almost 10x larger than RoBERTa-Large (355M params), both models took roughly 50 petaÔ¨Çop/s-days of compute during pre-training. Methodology for these calculations can be found in Appendix D. Dataset Quantity (tokens) Weight in training mix Epochs elapsed when training for 300B tokens Common Crawl (Ô¨Åltered) 410 billion 60% 0.44 WebText2 19 billion 22% 2.9 Books1 12 billion 8% 1.9 Books2 55 billion 8% 0.43 Wikipedia 3 billion 3% 3.4 Table 2.2: Datasets used to train GPT-3. ‚ÄúWeight in training mix‚Äù
(tokens) Weight in training mix Epochs elapsed when training for 300B tokens Common Crawl (Ô¨Åltered) 410 billion 60% 0.44 WebText2 19 billion 22% 2.9 Books1 12 billion 8% 1.9 Books2 55 billion 8% 0.43 Wikipedia 3 billion 3% 3.4 Table 2.2: Datasets used to train GPT-3. ‚ÄúWeight in training mix‚Äù refers to the fraction of examples during training that are drawn from a given dataset, which we intentionally do not make proportional to the size of the dataset. As a result, when we train for 300 billion tokens, some datasets are seen up to 3.4 times during training while other datasets are seen less than once. A major methodological concern with language models pretrained on a broad swath of internet data, particularly large models with the capacity to memorize vast amounts of content, is potential contamination of downstream tasks by having their test or development sets inadvertently seen during pre-training. To reduce such contamination, we searched for and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper. Unfortunately, a bug in the Ô¨Åltering caused us to ignore some overlaps, and due to the cost of training it was not feasible to
To reduce such contamination, we searched for and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper. Unfortunately, a bug in the Ô¨Åltering caused us to ignore some overlaps, and due to the cost of training it was not feasible to retrain the model. In Section 4 we characterize the impact of the remaining overlaps, and in future work we will more aggressively remove data contamination. 2.3 Training Process As found in [KMH+20, MKAT18], larger models can typically use a larger batch size, but require a smaller learning rate. We measure the gradient noise scale during training and use it to guide our choice of batch size [MKAT18]. Table 2.1 shows the parameter settings we used. To train the larger models without running out of memory, we use a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network. All models were trained on V100 GPU‚Äôs on part of a high-bandwidth cluster provided by Microsoft. Details of the training process and hyperparameter settings are described in Appendix B. 9 2.4 Evaluation For few-shot learning, we evaluate each example in the evaluation set by randomly
the layers of the network. All models were trained on V100 GPU‚Äôs on part of a high-bandwidth cluster provided by Microsoft. Details of the training process and hyperparameter settings are described in Appendix B. 9 2.4 Evaluation For few-shot learning, we evaluate each example in the evaluation set by randomly drawing K examples from that task‚Äôs training set as conditioning, delimited by 1 or 2 newlines depending on the task. For LAMBADA and Storycloze there is no supervised training set available so we draw conditioning examples from the development set and evaluate on the test set. For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning examples directly from it. K can be any value from 0 to the maximum amount allowed by the model‚Äôs context window, which is nctx = 2048 for all models and typically Ô¨Åts 10 to 100 examples. Larger values of K are usually but not always better, so when a separate development and test set are available, we experiment with a few values of K on the development set and then run the best value on the test set. For some tasks (see Appendix G) we also use a
of K are usually but not always better, so when a separate development and test set are available, we experiment with a few values of K on the development set and then run the best value on the test set. For some tasks (see Appendix G) we also use a natural language prompt in addition to (or for K = 0, instead of) demonstrations. On tasks that involve choosing one correct completion from several options (multiple choice), we provide K examples of context plus correct completion, followed by one example of context only, and compare the LM likelihood of each completion. For most tasks we compare the per-token likelihood (to normalize for length), however on a small number of datasets (ARC, OpenBookQA, and RACE) we gain additional beneÔ¨Åt as measured on the development set by normalizing by the unconditional probability of each completion, by computing P (completion|context) P (completion|answer context), where answer context is the string "Answer: " or "A: " and is used to prompt that the completion should be an answer but is otherwise generic. On tasks that involve binary classiÔ¨Åcation, we give the options more semantically meaningful names (e.g. ‚ÄúTrue‚Äù or ‚ÄúFalse‚Äù rather than 0 or 1)
where answer context is the string "Answer: " or "A: " and is used to prompt that the completion should be an answer but is otherwise generic. On tasks that involve binary classiÔ¨Åcation, we give the options more semantically meaningful names (e.g. ‚ÄúTrue‚Äù or ‚ÄúFalse‚Äù rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what is done by [RSR+19] (see Appendix G) for details. On tasks with free-form completion, we use beam search with the same parameters as [RSR+19]: a beam width of 4 and a length penalty of Œ± = 0.6. We score the model using F1 similarity score, BLEU, or exact match, depending on what is standard for the dataset at hand. Final results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-, and few-shot). When the test set is private, our model is often too large to Ô¨Åt on the test server, so we report results on the development set. We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we
test set is private, our model is often too large to Ô¨Åt on the test server, so we report results on the development set. We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else. 3 Results In Figure 3.1 we display training curves for the 8 models described in Section 2. For this graph we also include 6 additional extra-small models with as few as 100,000 parameters. As observed in [KMH+20], language modeling performance follows a power-law when making efÔ¨Åcient use of training compute. After extending this trend by two more orders of magnitude, we observe only a slight (if any) departure from the power-law. One might worry that these improvements in cross-entropy loss come only from modeling spurious details of our training corpus. However, we will see in the following sections that improvements in cross-entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks. Below, we evaluate the 8 models described in Section 2 (the 175 billion parameter parameter GPT-3 and 7 smaller models)
of our training corpus. However, we will see in the following sections that improvements in cross-entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks. Below, we evaluate the 8 models described in Section 2 (the 175 billion parameter parameter GPT-3 and 7 smaller models) on a wide range of datasets. We group the datasets into 9 categories representing roughly similar tasks. In Section 3.1 we evaluate on traditional language modeling tasks and tasks that are similar to language modeling, such as Cloze tasks and sentence/paragraph completion tasks. In Section 3.2 we evaluate on ‚Äúclosed book‚Äù question answering tasks: tasks which require using the information stored in the model‚Äôs parameters to answer general knowledge questions. In Section 3.3 we evaluate the model‚Äôs ability to translate between languages (especially one-shot and few-shot). In Section 3.4 we evaluate the model‚Äôs performance on Winograd Schema-like tasks. In Section 3.5 we evaluate on datasets that involve commonsense reasoning or question answering. In Section 3.6 we evaluate on reading comprehension tasks, in Section 3.7 we evaluate on the SuperGLUE benchmark suite, and in 3.8 we brieÔ¨Çy explore NLI. Finally, in Section 3.9, we invent some additional tasks designed especially to
3.5 we evaluate on datasets that involve commonsense reasoning or question answering. In Section 3.6 we evaluate on reading comprehension tasks, in Section 3.7 we evaluate on the SuperGLUE benchmark suite, and in 3.8 we brieÔ¨Çy explore NLI. Finally, in Section 3.9, we invent some additional tasks designed especially to probe in-context learning abilities ‚Äì these tasks focus on on-the-Ô¨Çy reasoning, adaptation skills, or open-ended text synthesis. We evaluate all tasks in the few-shot, one-shot, and zero-shot settings. 10 Figure 3.1: Smooth scaling of performance with compute. Performance (measured in terms of cross-entropy validation loss) follows a power-law trend with the amount of compute used for training. The power-law behavior observed in [KMH+20] continues for an additional two orders of magnitude with only small deviations from the predicted curve. For this Ô¨Ågure, we exclude embedding parameters from compute and parameter counts. Setting PTB SOTA (Zero-Shot) 35.8a GPT-3 Zero-Shot 20.5 Table 3.1: Zero-shot results on PTB language modeling dataset. Many other common language modeling datasets are omitted because they are derived from Wikipedia or other sources which are included in GPT-3‚Äôs training data. a[RWC+19] 3.1 Language Modeling, Cloze, and Completion Tasks In this section we test GPT-3‚Äôs performance on the traditional
3.1: Zero-shot results on PTB language modeling dataset. Many other common language modeling datasets are omitted because they are derived from Wikipedia or other sources which are included in GPT-3‚Äôs training data. a[RWC+19] 3.1 Language Modeling, Cloze, and Completion Tasks In this section we test GPT-3‚Äôs performance on the traditional task of language modeling, as well as related tasks that involve predicting a single word of interest, completing a sentence or paragraph, or choosing between possible completions of a piece of text. 3.1.1 Language Modeling We calculate zero-shot perplexity on the Penn Tree Bank (PTB) [MKM+94] dataset measured in [RWC+19]. We omit the 4 Wikipedia-related tasks in that work because they are entirely contained in our training data, and we also omit the one-billion word benchmark due to a high fraction of the dataset being contained in our training set. PTB escapes these issues due to predating the modern internet. Our largest model sets a new SOTA on PTB by a substantial margin of 15 points, achieving a perplexity of 20.50. Note that since PTB is a traditional language modeling dataset it does not have a clear separation of examples to deÔ¨Åne one-shot or few-shot evaluation around, so we measure
Our largest model sets a new SOTA on PTB by a substantial margin of 15 points, achieving a perplexity of 20.50. Note that since PTB is a traditional language modeling dataset it does not have a clear separation of examples to deÔ¨Åne one-shot or few-shot evaluation around, so we measure only zero-shot. 3.1.2 LAMBADA The LAMBADA dataset [PKL+16] tests the modeling of long-range dependencies in text ‚Äì the model is asked to predict the last word of sentences which require reading a paragraph of context. It has recently been suggested that the continued scaling of language models is yielding diminishing returns on this difÔ¨Åcult benchmark. [BHT+20] reÔ¨Çect on the small 1.5% improvement achieved by a doubling of model size between two recent state of the art results ([SPP+19] 11 Setting LAMBADA (acc) LAMBADA (ppl) StoryCloze (acc) HellaSwag (acc) SOTA 68.0a 8.63b 91.8c 85.6d GPT-3 Zero-Shot 76.2 3.00 83.2 78.9 GPT-3 One-Shot 72.5 3.35 84.7 78.1 GPT-3 Few-Shot 86.4 1.92 87.7 79.3 Table 3.2: Performance on cloze and completion tasks. GPT-3 signiÔ¨Åcantly improves SOTA on LAMBADA while achieving respectable performance on two difÔ¨Åcult completion prediction datasets. a[Tur20] b[RWC+19] c[LDL19] d[LCH+20] Figure 3.2: On LAMBADA, the few-shot capability of language models results
One-Shot 72.5 3.35 84.7 78.1 GPT-3 Few-Shot 86.4 1.92 87.7 79.3 Table 3.2: Performance on cloze and completion tasks. GPT-3 signiÔ¨Åcantly improves SOTA on LAMBADA while achieving respectable performance on two difÔ¨Åcult completion prediction datasets. a[Tur20] b[RWC+19] c[LDL19] d[LCH+20] Figure 3.2: On LAMBADA, the few-shot capability of language models results in a strong boost to accuracy. GPT-3 2.7B outperforms the SOTA 17B parameter Turing-NLG [Tur20] in this setting, and GPT-3 175B advances the state of the art by 18%. Note zero-shot uses a different format from one-shot and few-shot as described in the text. and [Tur20]) and argue that ‚Äúcontinuing to expand hardware and data sizes by orders of magnitude is not the path forward‚Äù. We Ô¨Ånd that path is still promising and in a zero-shot setting GPT-3 achieves 76% on LAMBADA, a gain of 8% over the previous state of the art. LAMBADA is also a demonstration of the Ô¨Çexibility of few-shot learning as it provides a way to address a problem that classically occurs with this dataset. Although the completion in LAMBADA is always the last word in a sentence, a standard language model has no way of knowing this detail. It thus assigns probability not only to
Ô¨Çexibility of few-shot learning as it provides a way to address a problem that classically occurs with this dataset. Although the completion in LAMBADA is always the last word in a sentence, a standard language model has no way of knowing this detail. It thus assigns probability not only to the correct ending but also to other valid continuations of the paragraph. This problem has been partially addressed in the past with stop-word Ô¨Ålters [RWC+19] (which ban ‚Äúcontinuation‚Äù words). The few-shot setting instead allows us to ‚Äúframe‚Äù the task as a cloze-test and allows the language model to infer from examples that a completion of exactly one word is desired. We use the following Ô¨Åll-in-the-blank format: Alice was friends with Bob. Alice went to visit her friend . ‚ÜíBob George bought some baseball equipment, a ball, a glove, and a . ‚Üí When presented with examples formatted this way, GPT-3 achieves 86.4% accuracy in the few-shot setting, an increase of over 18% from the previous state-of-the-art. We observe that few-shot performance improves strongly with model size. While this setting decreases the performance of the smallest model by almost 20%, for GPT-3 it improves accuracy by 10%. Finally, the Ô¨Åll-in-blank method
achieves 86.4% accuracy in the few-shot setting, an increase of over 18% from the previous state-of-the-art. We observe that few-shot performance improves strongly with model size. While this setting decreases the performance of the smallest model by almost 20%, for GPT-3 it improves accuracy by 10%. Finally, the Ô¨Åll-in-blank method is not effective one-shot, where it always performs worse than the zero-shot setting. Perhaps this is because all models still require several examples to recognize the pattern. 12 Setting NaturalQS WebQS TriviaQA RAG (Fine-tuned, Open-Domain) [LPP+20] 44.5 45.5 68.0 T5-11B+SSM (Fine-tuned, Closed-Book) [RRS20] 36.6 44.7 60.5 T5-11B (Fine-tuned, Closed-Book) 34.5 37.4 50.1 GPT-3 Zero-Shot 14.6 14.4 64.3 GPT-3 One-Shot 23.0 25.3 68.0 GPT-3 Few-Shot 29.9 41.5 71.2 Table 3.3: Results on three Open-Domain QA tasks. GPT-3 is shown in the few-, one-, and zero-shot settings, as compared to prior SOTA results for closed book and open domain settings. TriviaQA few-shot result is evaluated on the wiki split test server. One note of caution is that an analysis of test set contamination identiÔ¨Åed that a signiÔ¨Åcant minority of the LAMBADA dataset appears to be present in our training data ‚Äì however analysis performed in Section 4 suggests negligible impact on performance.
result is evaluated on the wiki split test server. One note of caution is that an analysis of test set contamination identiÔ¨Åed that a signiÔ¨Åcant minority of the LAMBADA dataset appears to be present in our training data ‚Äì however analysis performed in Section 4 suggests negligible impact on performance. 3.1.3 HellaSwag The HellaSwag dataset [ZHB+19] involves picking the best ending to a story or set of instructions. The examples were adversarially mined to be difÔ¨Åcult for language models while remaining easy for humans (who achieve 95.6% accuracy). GPT-3 achieves 78.1% accuracy in the one-shot setting and 79.3% accuracy in the few-shot setting, outperforming the 75.4% accuracy of a Ô¨Åne-tuned 1.5B parameter language model [ZHR+19] but still a fair amount lower than the overall SOTA of 85.6% achieved by the Ô¨Åne-tuned multi-task model ALUM. 3.1.4 StoryCloze We next evaluate GPT-3 on the StoryCloze 2016 dataset [MCH+16], which involves selecting the correct ending sentence for Ô¨Åve-sentence long stories. Here GPT-3 achieves 83.2% in the zero-shot setting and 87.7% in the few-shot setting (with K = 70). This is still 4.1% lower than the Ô¨Åne-tuned SOTA using a BERT based model [LDL19] but improves over previous zero-shot results by roughly 10%. 3.2
correct ending sentence for Ô¨Åve-sentence long stories. Here GPT-3 achieves 83.2% in the zero-shot setting and 87.7% in the few-shot setting (with K = 70). This is still 4.1% lower than the Ô¨Åne-tuned SOTA using a BERT based model [LDL19] but improves over previous zero-shot results by roughly 10%. 3.2 Closed Book Question Answering In this section we measure GPT-3‚Äôs ability to answer questions about broad factual knowledge. Due to the immense amount of possible queries, this task has normally been approached by using an information retrieval system to Ô¨Ånd relevant text in combination with a model which learns to generate an answer given the question and the retrieved text. Since this setting allows a system to search for and condition on text which potentially contains the answer it is denoted ‚Äúopen-book‚Äù. [RRS20] recently demonstrated that a large language model can perform surprisingly well directly answering the questions without conditioning on auxilliary information. They denote this more restrictive evaluation setting as ‚Äúclosed-book‚Äù. Their work suggests that even higher-capacity models could perform even better and we test this hypothesis with GPT-3. We evaluate GPT-3 on the 3 datasets in [RRS20]: Natural Questions [KPR+19], WebQuestions [BCFL13], and TriviaQA [JCWZ17], using the same
on auxilliary information. They denote this more restrictive evaluation setting as ‚Äúclosed-book‚Äù. Their work suggests that even higher-capacity models could perform even better and we test this hypothesis with GPT-3. We evaluate GPT-3 on the 3 datasets in [RRS20]: Natural Questions [KPR+19], WebQuestions [BCFL13], and TriviaQA [JCWZ17], using the same splits. Note that in addition to all results being in the closed-book setting, our use of few-shot, one-shot, and zero-shot evaluations represent an even stricter setting than previous closed-book QA work: in addition to external content not being allowed, Ô¨Åne-tuning on the Q&A dataset itself is also not permitted. The results for GPT-3 are shown in Table 3.3. On TriviaQA, we achieve 64.3% in the zero-shot setting, 68.0% in the one-shot setting, and 71.2% in the few-shot setting. The zero-shot result already outperforms the Ô¨Åne-tuned T5-11B by 14.2%, and also outperforms a version with Q&A tailored span prediction during pre-training by 3.8%. The one-shot result improves by 3.7% and matches the SOTA for an open-domain QA system which not only Ô¨Åne-tunes but also makes use of a learned retrieval mechanism over a 15.3B parameter dense vector index of 21M documents [LPP+20]. GPT-3‚Äôs few-shot result further improves performance another 3.2% beyond
by 3.8%. The one-shot result improves by 3.7% and matches the SOTA for an open-domain QA system which not only Ô¨Åne-tunes but also makes use of a learned retrieval mechanism over a 15.3B parameter dense vector index of 21M documents [LPP+20]. GPT-3‚Äôs few-shot result further improves performance another 3.2% beyond this. On WebQuestions (WebQs), GPT-3 achieves 14.4% in the zero-shot setting, 25.3% in the one-shot setting, and 41.5% in the few-shot setting. This compares to 37.4% for Ô¨Åne-tuned T5-11B, and 44.7% for Ô¨Åne-tuned T5-11B+SSM, which uses a Q&A-speciÔ¨Åc pre-training procedure. GPT-3 in the few-shot setting approaches the performance of state-of-the-art Ô¨Åne-tuned models. Notably, compared to TriviaQA, WebQS shows a much larger gain from zero-shot to few-shot (and indeed its zero-shot and one-shot performance are poor), perhaps suggesting that the WebQs questions 13 Figure 3.3: On TriviaQA GPT3‚Äôs performance grows smoothly with model size, suggesting that language models continue to absorb knowledge as their capacity increases. One-shot and few-shot performance make signiÔ¨Åcant gains over zero-shot behavior, matching and exceeding the performance of the SOTA Ô¨Åne-tuned open-domain model, RAG [LPP+20] and/or the style of their answers are out-of-distribution for GPT-3. Nevertheless, GPT-3 appears able to adapt to this distribution, recovering strong performance
knowledge as their capacity increases. One-shot and few-shot performance make signiÔ¨Åcant gains over zero-shot behavior, matching and exceeding the performance of the SOTA Ô¨Åne-tuned open-domain model, RAG [LPP+20] and/or the style of their answers are out-of-distribution for GPT-3. Nevertheless, GPT-3 appears able to adapt to this distribution, recovering strong performance in the few-shot setting. On Natural Questions (NQs) GPT-3 achieves 14.6% in the zero-shot setting, 23.0% in the one-shot setting, and 29.9% in the few-shot setting, compared to 36.6% for Ô¨Åne-tuned T5 11B+SSM. Similar to WebQS, the large gain from zero-shot to few-shot may suggest a distribution shift, and may also explain the less competitive performance compared to TriviaQA and WebQS. In particular, the questions in NQs tend towards very Ô¨Åne-grained knowledge on Wikipedia speciÔ¨Åcally which could be testing the limits of GPT-3‚Äôs capacity and broad pretraining distribution. Overall, on one of the three datasets GPT-3‚Äôs one-shot matches the open-domain Ô¨Åne-tuning SOTA. On the other two datasets it approaches the performance of the closed-book SOTA despite not using Ô¨Åne-tuning. On all 3 datasets, we Ô¨Ånd that performance scales very smoothly with model size (Figure 3.3 and Appendix H Figure H.7), possibly reÔ¨Çecting the idea that model capacity translates directly to
Ô¨Åne-tuning SOTA. On the other two datasets it approaches the performance of the closed-book SOTA despite not using Ô¨Åne-tuning. On all 3 datasets, we Ô¨Ånd that performance scales very smoothly with model size (Figure 3.3 and Appendix H Figure H.7), possibly reÔ¨Çecting the idea that model capacity translates directly to more ‚Äòknowledge‚Äô absorbed in the parameters of the model. 3.3 Translation For GPT-2 a Ô¨Ålter was used on a multilingual collection of documents to produce an English only dataset due to capacity concerns. Even with this Ô¨Åltering GPT-2 showed some evidence of multilingual capability and performed non-trivially when translating between French and English despite only training on 10 megabytes of remaining French text. Since we increase the capacity by over two orders of magnitude from GPT-2 to GPT-3, we also expand the scope of the training dataset to include more representation of other languages, though this remains an area for further improvement. As discussed in 2.2 the majority of our data is derived from raw Common Crawl with only quality-based Ô¨Åltering. Although GPT-3‚Äôs training data is still primarily English (93% by word count), it also includes 7% of text in other languages. These languages are documented in the supplemental material.
further improvement. As discussed in 2.2 the majority of our data is derived from raw Common Crawl with only quality-based Ô¨Åltering. Although GPT-3‚Äôs training data is still primarily English (93% by word count), it also includes 7% of text in other languages. These languages are documented in the supplemental material. In order to better understand translation capability, we also expand our analysis to include two additional commonly studied languages, German and Romanian. Existing unsupervised machine translation approaches often combine pretraining on a pair of monolingual datasets with back-translation [SHB15] to bridge the two languages in a controlled way. By contrast, GPT-3 learns from a blend of training data that mixes many languages together in a natural way, combining them on a word, sentence, and document level. GPT-3 also uses a single training objective which is not customized or designed for any task in particular. However, our one / few-shot settings aren‚Äôt strictly comparable to prior unsupervised work since they make use of a small amount of paired examples (1 or 64). This corresponds to up to a page or two of in-context training data. Results are shown in Table 3.4. Zero-shot GPT-3, which only receives on a natural language description
aren‚Äôt strictly comparable to prior unsupervised work since they make use of a small amount of paired examples (1 or 64). This corresponds to up to a page or two of in-context training data. Results are shown in Table 3.4. Zero-shot GPT-3, which only receives on a natural language description of the task, still underperforms recent unsupervised NMT results. However, providing only a single example demonstration for 14 Setting En‚ÜíFr Fr‚ÜíEn En‚ÜíDe De‚ÜíEn En‚ÜíRo Ro‚ÜíEn SOTA (Supervised) 45.6a 35.0 b 41.2c 40.2d 38.5e 39.9e XLM [LC19] 33.4 33.3 26.4 34.3 33.3 31.8 MASS [STQ+19] 37.5 34.9 28.3 35.2 35.2 33.1 mBART [LGG+20] - - 29.8 34.0 35.0 30.5 GPT-3 Zero-Shot 25.2 21.2 24.6 27.2 14.1 19.9 GPT-3 One-Shot 28.3 33.7 26.2 30.4 20.6 38.6 GPT-3 Few-Shot 32.6 39.2 29.7 40.6 21.0 39.5 Table 3.4: Few-shot GPT-3 outperforms previous unsupervised NMT work by 5 BLEU when translating into English reÔ¨Çecting its strength as an English LM. We report BLEU scores on the WMT‚Äô14 Fr‚ÜîEn, WMT‚Äô16 De‚ÜîEn, and WMT‚Äô16 Ro‚ÜîEn datasets as measured by multi-bleu.perl with XLM‚Äôs tokeniza- tion in order to compare most closely with prior unsupervised NMT work. SacreBLEUf [Pos18] results re- ported in Appendix H. Underline indicates an unsupervised
strength as an English LM. We report BLEU scores on the WMT‚Äô14 Fr‚ÜîEn, WMT‚Äô16 De‚ÜîEn, and WMT‚Äô16 Ro‚ÜîEn datasets as measured by multi-bleu.perl with XLM‚Äôs tokeniza- tion in order to compare most closely with prior unsupervised NMT work. SacreBLEUf [Pos18] results re- ported in Appendix H. Underline indicates an unsupervised or few-shot SOTA, bold indicates supervised SOTA with relative conÔ¨Ådence. a[EOAG18] b[DHKH14] c[WXH+18] d[oR16] e[LGG+20] f [SacreBLEU signature: BLEU+case.mixed+numrefs.1+smooth.exp+tok.intl+version.1.2.20] Figure 3.4: Few-shot translation performance on 6 language pairs as model capacity increases. There is a consistent trend of improvement across all datasets as the model scales, and as well as tendency for translation into English to be stronger than translation from English. 15 Setting Winograd Winogrande (XL) Fine-tuned SOTA 90.1a 84.6b GPT-3 Zero-Shot 88.3* 70.2 GPT-3 One-Shot 89.7* 73.2 GPT-3 Few-Shot 88.6* 77.7 Table 3.5: Results on the WSC273 version of Winograd schemas and the adversarial Winogrande dataset. See Section 4 for details on potential contamination of the Winograd test set. a[SBBC19] b[LYN+20] Figure 3.5: Zero-, one-, and few-shot performance on the adversarial Winogrande dataset as model capacity scales. Scaling is relatively smooth with the gains to few-shot learning increasing with model size, and few-shot GPT-3 175B is competitive with
4 for details on potential contamination of the Winograd test set. a[SBBC19] b[LYN+20] Figure 3.5: Zero-, one-, and few-shot performance on the adversarial Winogrande dataset as model capacity scales. Scaling is relatively smooth with the gains to few-shot learning increasing with model size, and few-shot GPT-3 175B is competitive with a Ô¨Åne-tuned RoBERTA-large. each translation task improves performance by over 7 BLEU and nears competitive performance with prior work. GPT-3 in the full few-shot setting further improves another 4 BLEU resulting in similar average performance to prior unsupervised NMT work. GPT-3 has a noticeable skew in its performance depending on language direction. For the three input languages studied, GPT-3 signiÔ¨Åcantly outperforms prior unsupervised NMT work when translating into English but underperforms when translating in the other direction. Performance on En-Ro is a noticeable outlier at over 10 BLEU worse than prior unsupervised NMT work. This could be a weakness due to reusing the byte-level BPE tokenizer of GPT-2 which was developed for an almost entirely English training dataset. For both Fr-En and De-En, few shot GPT-3 outperforms the best supervised result we could Ô¨Ånd but due to our unfamiliarity with the literature and the appearance that these are un-competitive benchmarks
to reusing the byte-level BPE tokenizer of GPT-2 which was developed for an almost entirely English training dataset. For both Fr-En and De-En, few shot GPT-3 outperforms the best supervised result we could Ô¨Ånd but due to our unfamiliarity with the literature and the appearance that these are un-competitive benchmarks we do not suspect those results represent true state of the art. For Ro-En, few shot GPT-3 performs within 0.5 BLEU of the overall SOTA which is achieved by a combination of unsupervised pretraining, supervised Ô¨Ånetuning on 608K labeled examples, and backtranslation [LHCG19b]. Finally, across all language pairs and across all three settings (zero-, one-, and few-shot), there is a smooth trend of improvement with model capacity. This is shown in Figure 3.4 in the case of few-shot results, and scaling for all three settings is shown in Appendix H. 3.4 Winograd-Style Tasks The Winograd Schemas Challenge [LDM12] is a classical task in NLP that involves determining which word a pronoun refers to, when the pronoun is grammatically ambiguous but semantically unambiguous to a human. Recently Ô¨Åne-tuned language models have achieved near-human performance on the original Winograd dataset, but more difÔ¨Åcult versions 16 Setting PIQA ARC (Easy) ARC (Challenge) OpenBookQA
classical task in NLP that involves determining which word a pronoun refers to, when the pronoun is grammatically ambiguous but semantically unambiguous to a human. Recently Ô¨Åne-tuned language models have achieved near-human performance on the original Winograd dataset, but more difÔ¨Åcult versions 16 Setting PIQA ARC (Easy) ARC (Challenge) OpenBookQA Fine-tuned SOTA 79.4 92.0[KKS+20] 78.5[KKS+20] 87.2[KKS+20] GPT-3 Zero-Shot 80.5* 68.8 51.4 57.6 GPT-3 One-Shot 80.5* 71.2 53.2 58.8 GPT-3 Few-Shot 82.8* 70.1 51.5 65.4 Table 3.6: GPT-3 results on three commonsense reasoning tasks, PIQA, ARC, and OpenBookQA. GPT-3 Few-Shot PIQA result is evaluated on the test server. See Section 4 for details on potential contamination issues on the PIQA test set. Figure 3.6: GPT-3 results on PIQA in the zero-shot, one-shot, and few-shot settings. The largest model achieves a score on the development set in all three conditions that exceeds the best recorded score on the task. such as the adversarially-mined Winogrande dataset [SBBC19] still signiÔ¨Åcantly lag human performance. We test GPT-3‚Äôs performance on both Winograd and Winogrande, as usual in the zero-, one-, and few-shot setting. On Winograd we test GPT-3 on the original set of 273 Winograd schemas, using the same ‚Äúpartial evaluation‚Äù method described in [RWC+19]. Note
adversarially-mined Winogrande dataset [SBBC19] still signiÔ¨Åcantly lag human performance. We test GPT-3‚Äôs performance on both Winograd and Winogrande, as usual in the zero-, one-, and few-shot setting. On Winograd we test GPT-3 on the original set of 273 Winograd schemas, using the same ‚Äúpartial evaluation‚Äù method described in [RWC+19]. Note that this setting differs slightly from the WSC task in the SuperGLUE benchmark, which is presented as binary classiÔ¨Åcation and requires entity extraction to convert to the form described in this section. On Winograd GPT-3 achieves 88.3%, 89.7%, and 88.6% in the zero-shot, one-shot, and few-shot settings, showing no clear in-context learning but in all cases achieving strong results just a few points below state-of-the-art and estimated human performance. We note that contamination analysis found some Winograd schemas in the training data but this appears to have only a small effect on results (see Section 4). On the more difÔ¨Åcult Winogrande dataset, we do Ô¨Ånd gains to in-context learning: GPT-3 achieves 70.2% in the zero-shot setting, 73.2% in the one-shot setting, and 77.7% in the few-shot setting. For comparison a Ô¨Åne-tuned RoBERTA model achieves 79%, state-of-the-art is 84.6% achieved with a Ô¨Åne-tuned high capacity model (T5), and human performance on
Winogrande dataset, we do Ô¨Ånd gains to in-context learning: GPT-3 achieves 70.2% in the zero-shot setting, 73.2% in the one-shot setting, and 77.7% in the few-shot setting. For comparison a Ô¨Åne-tuned RoBERTA model achieves 79%, state-of-the-art is 84.6% achieved with a Ô¨Åne-tuned high capacity model (T5), and human performance on the task as reported by [SBBC19] is 94.0%. 3.5 Common Sense Reasoning Next we consider three datasets which attempt to capture physical or scientiÔ¨Åc reasoning, as distinct from sentence completion, reading comprehension, or broad knowledge question answering. The Ô¨Årst, PhysicalQA (PIQA) [BZB+19], asks common sense questions about how the physical world works and is intended as a probe of grounded understanding of the world. GPT-3 achieves 81.0% accuracy zero-shot, 80.5% accuracy one-shot, and 82.8% accuracy few-shot (the last measured on PIQA‚Äôs test server). This compares favorably to the 79.4% accuracy prior state-of-the-art of a 17 Setting CoQA DROP QuAC SQuADv2 RACE-h RACE-m Fine-tuned SOTA 90.7a 89.1b 74.4c 93.0d 90.0e 93.1e GPT-3 Zero-Shot 81.5 23.6 41.5 59.5 45.5 58.4 GPT-3 One-Shot 84.0 34.3 43.3 65.4 45.9 57.4 GPT-3 Few-Shot 85.0 36.5 44.3 69.8 46.8 58.1 Table 3.7: Results on reading comprehension tasks. All scores are F1 except results for RACE which
RACE-h RACE-m Fine-tuned SOTA 90.7a 89.1b 74.4c 93.0d 90.0e 93.1e GPT-3 Zero-Shot 81.5 23.6 41.5 59.5 45.5 58.4 GPT-3 One-Shot 84.0 34.3 43.3 65.4 45.9 57.4 GPT-3 Few-Shot 85.0 36.5 44.3 69.8 46.8 58.1 Table 3.7: Results on reading comprehension tasks. All scores are F1 except results for RACE which report accuracy. a[JZC+19] b[JN20] c[AI19] d[QIA20] e[SPP+19] Ô¨Åne-tuned RoBERTa. PIQA shows relatively shallow scaling with model size and is still over 10% worse than human performance, but GPT-3‚Äôs few-shot and even zero-shot result outperform the current state-of-the-art. Our analysis Ô¨Çagged PIQA for a potential data contamination issue (despite hidden test labels), and we therefore conservatively mark the result with an asterisk. See Section 4 for details. ARC [CCE+18] is a dataset of multiple-choice questions collected from 3rd to 9th grade science exams. On the ‚ÄúChallenge‚Äù version of the dataset which has been Ô¨Åltered to questions which simple statistical or information retrieval methods are unable to correctly answer, GPT-3 achieves 51.4% accuracy in the zero-shot setting, 53.2% in the one-shot setting, and 51.5% in the few-shot setting. This is approaching the performance of a Ô¨Åne-tuned RoBERTa baseline (55.9%) from UniÔ¨ÅedQA [KKS+20]. On the ‚ÄúEasy‚Äù version of the dataset (questions which either
retrieval methods are unable to correctly answer, GPT-3 achieves 51.4% accuracy in the zero-shot setting, 53.2% in the one-shot setting, and 51.5% in the few-shot setting. This is approaching the performance of a Ô¨Åne-tuned RoBERTa baseline (55.9%) from UniÔ¨ÅedQA [KKS+20]. On the ‚ÄúEasy‚Äù version of the dataset (questions which either of the mentioned baseline approaches answered correctly), GPT-3 achieves 68.8%, 71.2%, and 70.1% which slightly exceeds a Ô¨Åne-tuned RoBERTa baseline from [KKS+20]. However, both of these results are still much worse than the overall SOTAs achieved by the UniÔ¨ÅedQA which exceeds GPT-3‚Äôs few-shot results by 27% on the challenge set and 22% on the easy set. On OpenBookQA [MCKS18], GPT-3 improves signiÔ¨Åcantly from zero to few shot settings but is still over 20 points short of the overall SOTA. GPT-3‚Äôs few-shot performance is similar to a Ô¨Åne-tuned BERT Large baseline on the leaderboard. Overall, in-context learning with GPT-3 shows mixed results on commonsense reasoning tasks, with only small and inconsistent gains observed in the one and few-shot learning settings for both PIQA and ARC, but a signiÔ¨Åcant improvement is observed on OpenBookQA. GPT-3 sets SOTA on the new PIQA dataset in all evaluation settings. 3.6 Reading Comprehension Next we evaluate
results on commonsense reasoning tasks, with only small and inconsistent gains observed in the one and few-shot learning settings for both PIQA and ARC, but a signiÔ¨Åcant improvement is observed on OpenBookQA. GPT-3 sets SOTA on the new PIQA dataset in all evaluation settings. 3.6 Reading Comprehension Next we evaluate GPT-3 on the task of reading comprehension. We use a suite of 5 datasets including abstractive, multiple choice, and span based answer formats in both dialog and single question settings. We observe a wide spread in GPT-3‚Äôs performance across these datasets suggestive of varying capability with different answer formats. In general we observe GPT-3 is on par with initial baselines and early results trained using contextual representations on each respective dataset. GPT-3 performs best (within 3 points of the human baseline) on CoQA [RCM19] a free-form conversational dataset and performs worst (13 F1 below an ELMo baseline) on QuAC [CHI+18] a dataset which requires modeling structured dialog acts and answer span selections of teacher-student interactions. On DROP [DWD+19], a dataset testing discrete reasoning and numeracy in the context of reading comprehension, GPT-3 in a few-shot setting outperforms the Ô¨Åne-tuned BERT baseline from the original paper but is still well below
a dataset which requires modeling structured dialog acts and answer span selections of teacher-student interactions. On DROP [DWD+19], a dataset testing discrete reasoning and numeracy in the context of reading comprehension, GPT-3 in a few-shot setting outperforms the Ô¨Åne-tuned BERT baseline from the original paper but is still well below both human performance and state-of-the-art approaches which augment neural networks with symbolic systems [RLL+19]. On SQuAD 2.0 [RJL18], GPT-3 demonstrates its few-shot learning capabilities, improving by almost 10 F1 (to 69.8) compared to a zero-shot setting. This allows it to slightly outperform the best Ô¨Åne-tuned result in the original paper. On RACE [LXL+17], a multiple choice dataset of middle school and high school english examinations, GPT-3 performs relatively weakly and is only competitive with the earliest work utilizing contextual representations and is still 45% behind SOTA. 3.7 SuperGLUE In order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a more systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark [WPN+19] [WPN+19] [CLC+19] [DMST19] [RBG11] [KCR+18] [ZLL+18] [DGM06] [BHDD+06] [GMDD07] [BDD+09] [PCC18] [PHR+18]. GPT-3‚Äôs test-set performance on the SuperGLUE dataset is shown in Table 3.8.
popular models such as BERT and RoBERTa in a more systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark [WPN+19] [WPN+19] [CLC+19] [DMST19] [RBG11] [KCR+18] [ZLL+18] [DGM06] [BHDD+06] [GMDD07] [BDD+09] [PCC18] [PHR+18]. GPT-3‚Äôs test-set performance on the SuperGLUE dataset is shown in Table 3.8. In the few-shot setting, we used 32 examples for all tasks, sampled randomly from the training set. For all tasks except WSC 18 Figure 3.7: GPT-3 results on CoQA reading comprehension task. GPT-3 175B achieves 85 F1 in the few-shot setting, only a few points behind measured human performance and state-of-the-art Ô¨Åne-tuned models. Zero-shot and one-shot performance is a few points behind, with the gains to few-shot being largest for bigger models. SuperGLUE BoolQ CB CB COPA RTE Average Accuracy Accuracy F1 Accuracy Accuracy Fine-tuned SOTA 89.0 91.0 96.9 93.9 94.8 92.5 Fine-tuned BERT-Large 69.0 77.4 83.6 75.7 70.6 71.7 GPT-3 Few-Shot 71.8 76.4 75.6 52.0 92.0 69.0 WiC WSC MultiRC MultiRC ReCoRD ReCoRD Accuracy Accuracy Accuracy F1a Accuracy F1 Fine-tuned SOTA 76.1 93.8 62.3 88.2 92.5 93.3 Fine-tuned BERT-Large 69.6 64.6 24.1 70.0 71.3 72.0 GPT-3 Few-Shot 49.4 80.1 30.5 75.4 90.2 91.1 Table 3.8: Performance of GPT-3 on
GPT-3 Few-Shot 71.8 76.4 75.6 52.0 92.0 69.0 WiC WSC MultiRC MultiRC ReCoRD ReCoRD Accuracy Accuracy Accuracy F1a Accuracy F1 Fine-tuned SOTA 76.1 93.8 62.3 88.2 92.5 93.3 Fine-tuned BERT-Large 69.6 64.6 24.1 70.0 71.3 72.0 GPT-3 Few-Shot 49.4 80.1 30.5 75.4 90.2 91.1 Table 3.8: Performance of GPT-3 on SuperGLUE compared to Ô¨Åne-tuned baselines and SOTA. All results are reported on the test set. GPT-3 few-shot is given a total of 32 examples within the context of each task and performs no gradient updates. 19 Figure 3.8: Performance on SuperGLUE increases with model size and number of examples in context. A value of K = 32 means that our model was shown 32 examples per task, for 256 examples total divided across the 8 tasks in SuperGLUE. We report GPT-3 values on the dev set, so our numbers are not directly comparable to the dotted reference lines (our test set results are in Table 3.8). The BERT-Large reference model was Ô¨Åne-tuned on the SuperGLUE training set (125K examples), whereas BERT++ was Ô¨Årst Ô¨Åne-tuned on MultiNLI (392K examples) and SWAG (113K examples) before further Ô¨Åne-tuning on the SuperGLUE training set (for a total of 630K Ô¨Åne-tuning examples). We Ô¨Ånd the
set results are in Table 3.8). The BERT-Large reference model was Ô¨Åne-tuned on the SuperGLUE training set (125K examples), whereas BERT++ was Ô¨Årst Ô¨Åne-tuned on MultiNLI (392K examples) and SWAG (113K examples) before further Ô¨Åne-tuning on the SuperGLUE training set (for a total of 630K Ô¨Åne-tuning examples). We Ô¨Ånd the difference in performance between the BERT-Large and BERT++ to be roughly equivalent to the difference between GPT-3 with one example per context versus eight examples per context. and MultiRC, we sampled a new set of examples to use in the context for each problem. For WSC and MultiRC, we used the same set of randomly drawn examples from the training set as context for all of the problems we evaluated. We observe a wide range in GPT-3‚Äôs performance across tasks. On COPA and ReCoRD GPT-3 achieves near-SOTA performance in the one-shot and few-shot settings, with COPA falling only a couple points short and achieving second place on the leaderboard, where Ô¨Årst place is held by a Ô¨Åne-tuned 11 billion parameter model (T5). On WSC, performance is still relatively strong, achieving 80.1% in the few-shot setting (note that GPT-3 achieves 88.6% on the original Winograd dataset as described in Section 3.4).
points short and achieving second place on the leaderboard, where Ô¨Årst place is held by a Ô¨Åne-tuned 11 billion parameter model (T5). On WSC, performance is still relatively strong, achieving 80.1% in the few-shot setting (note that GPT-3 achieves 88.6% on the original Winograd dataset as described in Section 3.4). On BoolQ, MultiRC, and RTE, performance is reasonable, roughly matching that of a Ô¨Åne-tuned BERT-Large. On CB, we see signs of life at 75.6% in the few-shot setting. WiC is a notable weak spot with few-shot performance at 49.4% (at random chance). We tried a number of different phrasings and formulations for WiC (which involves determining if a word is being used with the same meaning in two sentences), none of which was able to achieve strong performance. This hints at a phenomenon that will become clearer in the next section (which discusses the ANLI benchmark) ‚Äì GPT-3 appears to be weak in the few-shot or one-shot setting at some tasks that involve comparing two sentences or snippets, for example whether a word is used the same way in two sentences (WiC), whether one sentence is a paraphrase of another, or whether one sentence implies another. This could also explain
be weak in the few-shot or one-shot setting at some tasks that involve comparing two sentences or snippets, for example whether a word is used the same way in two sentences (WiC), whether one sentence is a paraphrase of another, or whether one sentence implies another. This could also explain the comparatively low scores for RTE and CB, which also follow this format. Despite these weaknesses, GPT-3 still outperforms a Ô¨Åne-tuned BERT-large on four of eight tasks and on two tasks GPT-3 is close to the state-of-the-art held by a Ô¨Åne-tuned 11 billion parameter model. Finally, we note that the few-shot SuperGLUE score steadily improves with both model size and with number of examples in the context showing increasing beneÔ¨Åts from in-context learning (Figure 3.8). We scale K up to 32 examples per task, after which point additional examples will not reliably Ô¨Åt into our context. When sweeping over values of K, we Ô¨Ånd that GPT-3 requires less than eight total examples per task to outperform a Ô¨Åne-tuned BERT-Large on overall SuperGLUE score. 3.8 NLI Natural Language Inference (NLI) [Fyo00] concerns the ability to understand the relationship between two sentences. In practice, this task is usually structured as a two
of K, we Ô¨Ånd that GPT-3 requires less than eight total examples per task to outperform a Ô¨Åne-tuned BERT-Large on overall SuperGLUE score. 3.8 NLI Natural Language Inference (NLI) [Fyo00] concerns the ability to understand the relationship between two sentences. In practice, this task is usually structured as a two or three class classiÔ¨Åcation problem where the model classiÔ¨Åes 20 Figure 3.9: Performance of GPT-3 on ANLI Round 3. Results are on the dev-set, which has only 1500 examples and therefore has high variance (we estimate a standard deviation of 1.2%). We Ô¨Ånd that smaller models hover around random chance, while few-shot GPT-3 175B closes almost half the gap from random chance to SOTA. Results for ANLI rounds 1 and 2 are shown in the appendix. whether the second sentence logically follows from the Ô¨Årst, contradicts the Ô¨Årst sentence, or is possibly true (neutral). SuperGLUE includes an NLI dataset, RTE, which evaluates the binary version of the task. On RTE, only the largest version of GPT-3 performs convincingly better than random (56%) in any evaluation setting, but in a few-shot setting GPT-3 performs similarly to a single-task Ô¨Åne-tuned BERT Large. We also evaluate on the recently introduced Adversarial Natural Language
which evaluates the binary version of the task. On RTE, only the largest version of GPT-3 performs convincingly better than random (56%) in any evaluation setting, but in a few-shot setting GPT-3 performs similarly to a single-task Ô¨Åne-tuned BERT Large. We also evaluate on the recently introduced Adversarial Natural Language Inference (ANLI) dataset [NWD+19]. ANLI is a difÔ¨Åcult dataset employing a series of adversarially mined natural language inference questions in three rounds (R1, R2, and R3). Similar to RTE, all of our models smaller than GPT-3 perform at almost exactly random chance on ANLI, even in the few-shot setting (‚àº33%), whereas GPT-3 itself shows signs of life on Round 3. Results for ANLI R3 are highlighted in Figure 3.9 and full results for all rounds can be found in Appendix H. These results on both RTE and ANLI suggest that NLI is still a very difÔ¨Åcult task for language models and they are only just beginning to show signs of progress. 3.9 Synthetic and Qualitative Tasks One way to probe GPT-3‚Äôs range of abilities in the few-shot (or zero- and one-shot) setting is to give it tasks which require it to perform simple on-the-Ô¨Çy computational reasoning, recognize a novel pattern
and they are only just beginning to show signs of progress. 3.9 Synthetic and Qualitative Tasks One way to probe GPT-3‚Äôs range of abilities in the few-shot (or zero- and one-shot) setting is to give it tasks which require it to perform simple on-the-Ô¨Çy computational reasoning, recognize a novel pattern that is unlikely to have occurred in training, or adapt quickly to an unusual task. We devise several tasks to test this class of abilities. First, we test GPT-3‚Äôs ability to perform arithmetic. Second, we create several tasks that involve rearranging or unscrambling the letters in a word, tasks which are unlikely to have been exactly seen during training. Third, we test GPT-3‚Äôs ability to solve SAT-style analogy problems few-shot. Finally, we test GPT-3 on several qualitative tasks, including using new words in a sentence, correcting English grammar, and news article generation. We will release the synthetic datasets with the hope of stimulating further study of test-time behavior of language models. 3.9.1 Arithmetic To test GPT-3‚Äôs ability to perform simple arithmetic operations without task-speciÔ¨Åc training, we developed a small battery of 10 tests that involve asking GPT-3 a simple arithmetic problem in natural language: ‚Ä¢ 2 digit addition (2D+) ‚Äì
hope of stimulating further study of test-time behavior of language models. 3.9.1 Arithmetic To test GPT-3‚Äôs ability to perform simple arithmetic operations without task-speciÔ¨Åc training, we developed a small battery of 10 tests that involve asking GPT-3 a simple arithmetic problem in natural language: ‚Ä¢ 2 digit addition (2D+) ‚Äì The model is asked to add two integers sampled uniformly from [0, 100), phrased in the form of a question, e.g. ‚ÄúQ: What is 48 plus 76? A: 124.‚Äù ‚Ä¢ 2 digit subtraction (2D-) ‚Äì The model is asked to subtract two integers sampled uniformly from [0, 100); the answer may be negative. Example: ‚ÄúQ: What is 34 minus 53? A: -19‚Äù. ‚Ä¢ 3 digit addition (3D+) ‚Äì Same as 2 digit addition, except numbers are uniformly sampled from [0, 1000). 21 Figure 3.10: Results on all 10 arithmetic tasks in the few-shot settings for models of different sizes. There is a signiÔ¨Åcant jump from the second largest model (GPT-3 13B) to the largest model (GPT-3 175), with the latter being able to reliably accurate 2 digit arithmetic, usually accurate 3 digit arithmetic, and correct answers a signiÔ¨Åcant fraction of the time on 4-5 digit arithmetic, 2 digit multiplication, and
is a signiÔ¨Åcant jump from the second largest model (GPT-3 13B) to the largest model (GPT-3 175), with the latter being able to reliably accurate 2 digit arithmetic, usually accurate 3 digit arithmetic, and correct answers a signiÔ¨Åcant fraction of the time on 4-5 digit arithmetic, 2 digit multiplication, and compound operations. Results for one-shot and zero-shot are shown in the appendix. ‚Ä¢ 3 digit subtraction (3D-) ‚Äì Same as 2 digit subtraction, except numbers are uniformly sampled from [0, 1000). ‚Ä¢ 4 digit addition (4D+) ‚Äì Same as 3 digit addition, except uniformly sampled from [0, 10000). ‚Ä¢ 4 digit subtraction (4D-) ‚Äì Same as 3 digit subtraction, except uniformly sampled from [0, 10000). ‚Ä¢ 5 digit addition (5D+) ‚Äì Same as 3 digit addition, except uniformly sampled from [0, 100000). ‚Ä¢ 5 digit subtraction (5D-) ‚Äì Same as 3 digit subtraction, except uniformly sampled from [0, 100000). ‚Ä¢ 2 digit multiplication (2Dx) ‚Äì The model is asked to multiply two integers sampled uniformly from [0, 100), e.g. ‚ÄúQ: What is 24 times 42? A: 1008‚Äù. ‚Ä¢ One-digit composite (1DC) ‚Äì The model is asked to perform a composite operation on three 1 digit numbers, with parentheses around the
2 digit multiplication (2Dx) ‚Äì The model is asked to multiply two integers sampled uniformly from [0, 100), e.g. ‚ÄúQ: What is 24 times 42? A: 1008‚Äù. ‚Ä¢ One-digit composite (1DC) ‚Äì The model is asked to perform a composite operation on three 1 digit numbers, with parentheses around the last two. For example, ‚ÄúQ: What is 6+(4*8)? A: 38‚Äù. The three 1 digit numbers are selected uniformly on [0, 10) and the operations are selected uniformly from {+,-,*}. In all 10 tasks the model must generate the correct answer exactly. For each task we generate a dataset of 2,000 random instances of the task and evaluate all models on those instances. First we evaluate GPT-3 in the few-shot setting, for which results are shown in Figure 3.10. On addition and subtraction, GPT-3 displays strong proÔ¨Åciency when the number of digits is small, achieving 100% accuracy on 2 digit addition, 98.9% at 2 digit subtraction, 80.2% at 3 digit addition, and 94.2% at 3-digit subtraction. Performance decreases as the number of digits increases, but GPT-3 still achieves 25-26% accuracy on four digit operations and 9-10% accuracy on Ô¨Åve digit operations, suggesting at least some capacity to generalize to larger numbers
98.9% at 2 digit subtraction, 80.2% at 3 digit addition, and 94.2% at 3-digit subtraction. Performance decreases as the number of digits increases, but GPT-3 still achieves 25-26% accuracy on four digit operations and 9-10% accuracy on Ô¨Åve digit operations, suggesting at least some capacity to generalize to larger numbers of digits. GPT-3 also achieves 29.2% accuracy at 2 digit multiplication, an especially computationally intensive operation. Finally, GPT-3 achieves 21.3% accuracy at single digit combined operations (for example, 9*(7+5)), suggesting that it has some robustness beyond just single operations. As Figure 3.10 makes clear, small models do poorly on all of these tasks ‚Äì even the 13 billion parameter model (the second largest after the 175 billion full GPT-3) can solve 2 digit addition and subtraction only half the time, and all other operations less than 10% of the time. One-shot and zero-shot performance are somewhat degraded relative to few-shot performance, suggesting that adaptation to the task (or at the very least recognition of the task) is important to performing these computations correctly. Nevertheless, one-shot performance is still quite strong, and even zero-shot performance of the full GPT-3 signiÔ¨Åcantly 22 Setting 2D+ 2D- 3D+ 3D- 4D+ 4D- 5D+ 5D- 2Dx
performance, suggesting that adaptation to the task (or at the very least recognition of the task) is important to performing these computations correctly. Nevertheless, one-shot performance is still quite strong, and even zero-shot performance of the full GPT-3 signiÔ¨Åcantly 22 Setting 2D+ 2D- 3D+ 3D- 4D+ 4D- 5D+ 5D- 2Dx 1DC GPT-3 Zero-shot 76.9 58.0 34.2 48.3 4.0 7.5 0.7 0.8 19.8 9.8 GPT-3 One-shot 99.6 86.4 65.5 78.7 14.0 14.0 3.5 3.8 27.4 14.3 GPT-3 Few-shot 100.0 98.9 80.4 94.2 25.5 26.8 9.3 9.9 29.2 21.3 Table 3.9: Results on basic arithmetic tasks for GPT-3 175B. {2,3,4,5}D{+,-} is 2, 3, 4, and 5 digit addition or subtraction, 2Dx is 2 digit multiplication. 1DC is 1 digit composite operations. Results become progressively stronger moving from the zero-shot to one-shot to few-shot setting, but even the zero-shot shows signiÔ¨Åcant arithmetic abilities. Setting CL A1 A2 RI RW GPT-3 Zero-shot 3.66 2.28 8.91 8.26 0.09 GPT-3 One-shot 21.7 8.62 25.9 45.4 0.48 GPT-3 Few-shot 37.9 15.1 39.7 67.2 0.44 Table 3.10: GPT-3 175B performance on various word unscrambling and word manipulation tasks, in zero-, one-, and few-shot settings. CL is ‚Äúcycle letters in word‚Äù, A1 is anagrams of but the Ô¨Årst and
8.91 8.26 0.09 GPT-3 One-shot 21.7 8.62 25.9 45.4 0.48 GPT-3 Few-shot 37.9 15.1 39.7 67.2 0.44 Table 3.10: GPT-3 175B performance on various word unscrambling and word manipulation tasks, in zero-, one-, and few-shot settings. CL is ‚Äúcycle letters in word‚Äù, A1 is anagrams of but the Ô¨Årst and last letters, A2 is anagrams of all but the Ô¨Årst and last two letters, RI is ‚ÄúRandom insertion in word‚Äù, RW is ‚Äúreversed words‚Äù. outperforms few-shot learning for all smaller models. All three settings for the full GPT-3 are shown in Table 3.9, and model capacity scaling for all three settings is shown in Appendix H. To spot-check whether the model is simply memorizing speciÔ¨Åc arithmetic problems, we took the 3-digit arithmetic problems in our test set and searched for them in our training data in both the forms "<NUM1> + <NUM2> =" and "<NUM1> plus <NUM2>". Out of 2,000 addition problems we found only 17 matches (0.8%) and out of 2,000 subtraction problems we found only 2 matches (0.1%), suggesting that only a trivial fraction of the correct answers could have been memorized. In addition, inspection of incorrect answers reveals that the model often makes mistakes such as not
addition problems we found only 17 matches (0.8%) and out of 2,000 subtraction problems we found only 2 matches (0.1%), suggesting that only a trivial fraction of the correct answers could have been memorized. In addition, inspection of incorrect answers reveals that the model often makes mistakes such as not carrying a ‚Äú1‚Äù, suggesting it is actually attempting to perform the relevant computation rather than memorizing a table. Overall, GPT-3 displays reasonable proÔ¨Åciency at moderately complex arithmetic in few-shot, one-shot, and even zero-shot settings. 3.9.2 Word Scrambling and Manipulation Tasks To test GPT-3‚Äôs ability to learn novel symbolic manipulations from a few examples, we designed a small battery of 5 ‚Äúcharacter manipulation‚Äù tasks. Each task involves giving the model a word distorted by some combination of scrambling, addition, or deletion of characters, and asking it to recover the original word. The 5 tasks are: ‚Ä¢ Cycle letters in word (CL) ‚Äì The model is given a word with its letters cycled, then the ‚Äú=‚Äù symbol, and is expected to generate the original word. For example, it might be given ‚Äúlyinevitab‚Äù and should output ‚Äúinevitably‚Äù. ‚Ä¢ Anagrams of all but Ô¨Årst and last characters (A1) ‚Äì The model is given a
‚Äì The model is given a word with its letters cycled, then the ‚Äú=‚Äù symbol, and is expected to generate the original word. For example, it might be given ‚Äúlyinevitab‚Äù and should output ‚Äúinevitably‚Äù. ‚Ä¢ Anagrams of all but Ô¨Årst and last characters (A1) ‚Äì The model is given a word where every letter except the Ô¨Årst and last have been scrambled randomly, and must output the original word. Example: criroptuon = corruption. ‚Ä¢ Anagrams of all but Ô¨Årst and last 2 characters (A2) ‚Äì The model is given a word where every letter except the Ô¨Årst 2 and last 2 have been scrambled randomly, and must recover the original word. Example: opoepnnt ‚Üíopponent. ‚Ä¢ Random insertion in word (RI) ‚Äì A random punctuation or space character is inserted between each letter of a word, and the model must output the original word. Example: s.u!c/c!e.s s i/o/n = succession. ‚Ä¢ Reversed words (RW) ‚Äì The model is given a word spelled backwards, and must output the original word. Example: stcejbo ‚Üíobjects. For each task we generate 10,000 examples, which we chose to be the top 10,000 most frequent words as measured by [Nor09] of length more than 4 characters and
Reversed words (RW) ‚Äì The model is given a word spelled backwards, and must output the original word. Example: stcejbo ‚Üíobjects. For each task we generate 10,000 examples, which we chose to be the top 10,000 most frequent words as measured by [Nor09] of length more than 4 characters and less than 15 characters. The few-shot results are shown in Figure 3.11. Task performance tends to grow smoothly with model size, with the full GPT-3 model achieving 66.9% on removing 23 Figure 3.11: Few-shot performance on the Ô¨Åve word scrambling tasks for different sizes of model. There is generally smooth improvement with model size although the random insertion task shows an upward slope of improvement with the 175B model solving the task the majority of the time. Scaling of one-shot and zero-shot performance is shown in the appendix. All tasks are done with K = 100. random insertions, 38.6% on cycling letters, 40.2% on the easier anagram task, and 15.1% on the more difÔ¨Åcult anagram task (where only the Ô¨Årst and last letters are held Ô¨Åxed). None of the models can reverse the letters in a word. In the one-shot setting, performance is signiÔ¨Åcantly weaker (dropping by half or more),
cycling letters, 40.2% on the easier anagram task, and 15.1% on the more difÔ¨Åcult anagram task (where only the Ô¨Årst and last letters are held Ô¨Åxed). None of the models can reverse the letters in a word. In the one-shot setting, performance is signiÔ¨Åcantly weaker (dropping by half or more), and in the zero-shot setting the model can rarely perform any of the tasks (Table 3.10). This suggests that the model really does appear to learn these tasks at test time, as the model cannot perform them zero-shot and their artiÔ¨Åcial nature makes them unlikely to appear in the pre-training data (although we cannot conÔ¨Årm this with certainty). We can further quantify performance by plotting ‚Äúin-context learning curves‚Äù, which show task performance as a function of the number of in-context examples. We show in-context learning curves for the Symbol Insertion task in Figure 1.2. We can see that larger models are able to make increasingly effective use of in-context information, including both task examples and natural language task descriptions. Finally, it is worth adding that solving these tasks requires character-level manipulations, whereas our BPE encoding operates on signiÔ¨Åcant fractions of a word (on average ‚àº0.7 words per token), so from
are able to make increasingly effective use of in-context information, including both task examples and natural language task descriptions. Finally, it is worth adding that solving these tasks requires character-level manipulations, whereas our BPE encoding operates on signiÔ¨Åcant fractions of a word (on average ‚àº0.7 words per token), so from the LM‚Äôs perspective succeeding at these tasks involves not just manipulating BPE tokens but understanding and pulling apart their substructure. Also, CL, A1, and A2 are not bijective (that is, the unscrambled word is not a deterministic function of the scrambled word), requiring the model to perform some search to Ô¨Ånd the correct unscrambling. Thus, the skills involved appear to require non-trivial pattern-matching and computation. 3.9.3 SAT Analogies To test GPT-3 on another task that is somewhat unusual relative to the typical distribution of text, we collected a set of 374 ‚ÄúSAT analogy‚Äù problems [TLBS03]. Analogies are a style of multiple choice question that constituted a section of the SAT college entrance exam before 2005. A typical example is ‚Äúaudacious is to boldness as (a) sanctimonious is to hypocrisy, (b) anonymous is to identity, (c) remorseful is to misdeed, (d) deleterious is to result, (e) impressionable is to temptation‚Äù. The
multiple choice question that constituted a section of the SAT college entrance exam before 2005. A typical example is ‚Äúaudacious is to boldness as (a) sanctimonious is to hypocrisy, (b) anonymous is to identity, (c) remorseful is to misdeed, (d) deleterious is to result, (e) impressionable is to temptation‚Äù. The student is expected to choose which of the Ô¨Åve word pairs has the same relationship as the original word pair; in this example the answer is ‚Äúsanctimonious is to hypocrisy‚Äù. On this task GPT-3 achieves 65.2% in the few-shot setting, 59.1% in the one-shot setting, and 53.7% in the zero-shot setting, whereas the average score among college applicants was 57% [TL05] (random guessing yields 20%). As shown in Figure 3.12, the results improve with scale, with the the full 175 billion model improving by over 10% compared to the 13 billion parameter model. 24 Figure 3.12: Zero-, one-,and few-shot performance on SAT analogy tasks, for different sizes of model. The largest model achieves 65% accuracy in the few-shot setting, and also demonstrates signiÔ¨Åcant gains to in-context learning which are not present in smaller models. 3.9.4 News Article Generation Previous work on generative language models qualitatively tested their ability to generate
on SAT analogy tasks, for different sizes of model. The largest model achieves 65% accuracy in the few-shot setting, and also demonstrates signiÔ¨Åcant gains to in-context learning which are not present in smaller models. 3.9.4 News Article Generation Previous work on generative language models qualitatively tested their ability to generate synthetic ‚Äúnews articles‚Äù by conditional sampling from the model given a human-written prompt consisting of a plausible Ô¨Årst sentence for a news story [RWC+19]. Relative to [RWC+19], the dataset used to train GPT-3 is much less weighted towards news articles, so trying to generate news articles via raw unconditional samples is less effective ‚Äì for example GPT-3 often interprets the proposed Ô¨Årst sentence of a ‚Äúnews article‚Äù as a tweet and then posts synthetic responses or follow-up tweets. To solve this problem we employed GPT-3‚Äôs few-shot learning abilities by providing three previous news articles in the model‚Äôs context to condition it. With the title and subtitle of a proposed next article, the model is able to reliably generate short articles in the ‚Äúnews‚Äù genre. To gauge the quality of news article generation from GPT-3 (which we believe is likely to be correlated with conditional sample generation quality in general), we
it. With the title and subtitle of a proposed next article, the model is able to reliably generate short articles in the ‚Äúnews‚Äù genre. To gauge the quality of news article generation from GPT-3 (which we believe is likely to be correlated with conditional sample generation quality in general), we decided to measure human ability to distinguish GPT-3-generated articles from real ones. Similar work has been carried out by Kreps et al. [KMB20] and Zellers et al. [ZHR+19]. Generative language models are trained to match the distribution of content generated by humans, so the (in)ability of humans to distinguish the two is a potentially important measure of quality.3 In order to see how well humans can detect model generated text, we arbitrarily selected 25 article titles and subtitles from the website newser.com (mean length: 215 words). We then generated completions of these titles and subtitles from four language models ranging in size from 125M to 175B (GPT-3) parameters (mean length: 200 words). For each model, we presented around 80 US-based participants with a quiz consisting of these real titles and subtitles followed by either the human written article or the article generated by the model4. Participants were asked to select
in size from 125M to 175B (GPT-3) parameters (mean length: 200 words). For each model, we presented around 80 US-based participants with a quiz consisting of these real titles and subtitles followed by either the human written article or the article generated by the model4. Participants were asked to select whether the article was ‚Äúvery likely written by a human‚Äù, ‚Äúmore likely written by a human‚Äù, ‚ÄúI don‚Äôt know‚Äù, ‚Äúmore likely written by a machine‚Äù, or ‚Äúvery likely written by a machine‚Äù. The articles we selected were not in the models‚Äô training data and the model outputs were formatted and selected programmatically to prevent human cherry-picking. All models used the same context to condition outputs on and were pre-trained with the same context size and the same article titles and subtitles were used as prompts for each model. However, we also ran an experiment to control for participant effort and attention that followed the same format but involved intentionally bad model generated articles. This was done by generating articles from a ‚Äúcontrol model‚Äù: a 160M parameter model with no context and increased output randomness. 3This task is also relevant to the potential misuse of language models discussed in Section 6.1.
attention that followed the same format but involved intentionally bad model generated articles. This was done by generating articles from a ‚Äúcontrol model‚Äù: a 160M parameter model with no context and increased output randomness. 3This task is also relevant to the potential misuse of language models discussed in Section 6.1. 4We wanted to identify how good an average person on the internet is at detecting language model outputs, so we focused on participants drawn from the general US population. See Appendix E for details. 25 Mean accuracy 95% ConÔ¨Ådence Interval (low, hi) t compared to control (p-value) ‚ÄúI don‚Äôt know‚Äù assignments Control (deliberately bad model) 86% 83%‚Äì90% - 3.6 % GPT-3 Small 76% 72%‚Äì80% 3.9 (2e-4) 4.9% GPT-3 Medium 61% 58%‚Äì65% 10.3 (7e-21) 6.0% GPT-3 Large 68% 64%‚Äì72% 7.3 (3e-11) 8.7% GPT-3 XL 62% 59%‚Äì65% 10.7 (1e-19) 7.5% GPT-3 2.7B 62% 58%‚Äì65% 10.4 (5e-19) 7.1% GPT-3 6.7B 60% 56%‚Äì63% 11.2 (3e-21) 6.2% GPT-3 13B 55% 52%‚Äì58% 15.3 (1e-32) 7.1% GPT-3 175B 52% 49%‚Äì54% 16.9 (1e-34) 7.8% Table 3.11: Human accuracy in identifying whether short (‚àº200 word) news articles are model generated. We Ô¨Ånd that human accuracy (measured by the ratio of correct assignments to non-neutral assignments) ranges from 86% on
(3e-21) 6.2% GPT-3 13B 55% 52%‚Äì58% 15.3 (1e-32) 7.1% GPT-3 175B 52% 49%‚Äì54% 16.9 (1e-34) 7.8% Table 3.11: Human accuracy in identifying whether short (‚àº200 word) news articles are model generated. We Ô¨Ånd that human accuracy (measured by the ratio of correct assignments to non-neutral assignments) ranges from 86% on the control model to 52% on GPT-3 175B. This table compares mean accuracy between Ô¨Åve different models, and shows the results of a two-sample T-Test for the difference in mean accuracy between each model and the control model (an unconditional GPT-3 Small model with increased output randomness). Mean human accuracy (the ratio of correct assignments to non-neutral assignments per participant) at detecting that the intentionally bad articles were model generated was ‚àº86% where 50% is chance level performance. By contrast, mean human accuracy at detecting articles that were produced by the 175B parameter model was barely above chance at ‚àº52% (see Table 3.11).5 Human abilities to detect model generated text appear to decrease as model size increases: there appears to be a trend towards chance accuracy with model size, and human detection of GPT-3 is close to chance.6 This is true despite the fact that participants spend more time on
(see Table 3.11).5 Human abilities to detect model generated text appear to decrease as model size increases: there appears to be a trend towards chance accuracy with model size, and human detection of GPT-3 is close to chance.6 This is true despite the fact that participants spend more time on each output as model size increases (see Appendix E). Examples of synthetic articles from GPT-3 are given in Figures 3.14 and 3.15.7 Much of the text is‚Äîas indicated by the evaluations‚ÄîdifÔ¨Åcult for humans to distinguish from authentic human content. Factual inaccuracies can be an indicator that an article is model generated since, unlike human authors, the models have no access to the speciÔ¨Åc facts that the article titles refer to or when the article was written. Other indicators include repetition, non sequiturs, and unusual phrasings, though these are often subtle enough that they are not noticed. Related work on language model detection by Ippolito et al. [IDCBE19] indicates that automatic discriminators like G ROV E R [ZHR+19] and GLTR [GSR19] may have greater success at detecting model generated text than human evaluators. Automatic detection of these models may be a promising area of future research. Ippolito et al. [IDCBE19] also
model detection by Ippolito et al. [IDCBE19] indicates that automatic discriminators like G ROV E R [ZHR+19] and GLTR [GSR19] may have greater success at detecting model generated text than human evaluators. Automatic detection of these models may be a promising area of future research. Ippolito et al. [IDCBE19] also note that human accuracy at detecting model generated text increases as humans observe more tokens. To do a preliminary investigation of how good humans are at detecting longer news articles generated by GPT-3 175B, we selected 12 world news articles from Reuters with an average length of 569 words and generated completions of these articles from GPT-3 with an average length of 498 words (298 words longer than our initial experiments). Following the methodology above, we ran two experiments, each on around 80 US-based participants, to compare human abilities to detect the articles generated by GPT-3 and a control model. We found that mean human accuracy at detecting the intentionally bad longer articles from the control model was ‚àº88%, while mean human accuracy at detecting the longer articles that were produced by GPT-3 175B was still barely above chance at ‚àº52% (see Table 3.12). This indicates that, for news articles
We found that mean human accuracy at detecting the intentionally bad longer articles from the control model was ‚àº88%, while mean human accuracy at detecting the longer articles that were produced by GPT-3 175B was still barely above chance at ‚àº52% (see Table 3.12). This indicates that, for news articles that are around 500 words long, GPT-3 continues to produce articles that humans Ô¨Ånd difÔ¨Åcult to distinguish from human written news articles. 3.9.5 Learning and Using Novel Words A task studied in developmental linguistics [CB78] is the ability to learn and utilize new words, for example using a word in a sentence after seeing it deÔ¨Åned only once, or conversely inferring a word‚Äôs meaning from only one usage. Here we qualitatively test GPT-3‚Äôs ability to do the former. SpeciÔ¨Åcally, we give GPT-3 the deÔ¨Ånition of a nonexistent word, such as ‚ÄúGigamuru‚Äù, and then ask it to use it in a sentence. We provide one to Ô¨Åve previous examples of a (separate) 5We use a two-sample Student‚Äôs T-Test to test for signiÔ¨Åcant difference between the means of the participant accuracies of each model and the control model and report the normalized difference in the means (as the t-statistic) and the p-value.
sentence. We provide one to Ô¨Åve previous examples of a (separate) 5We use a two-sample Student‚Äôs T-Test to test for signiÔ¨Åcant difference between the means of the participant accuracies of each model and the control model and report the normalized difference in the means (as the t-statistic) and the p-value. 6If a model consistently produces texts that are more impressive than human articles, it is possible that human performance on this task would drop below 50%. Indeed, many individual participants scored below 50% on this task. 7Additional non-news samples can be found in Appendix F. 26 Figure 3.13: People‚Äôs ability to identify whether news articles are model-generated (measured by the ratio of correct assignments to non-neutral assignments) decreases as model size increases. Accuracy on the outputs on the deliberately- bad control model (an unconditioned GPT-3 Small model with higher output randomness) is indicated with the dashed line at the top, and the random chance (50%) is indicated with the dashed line at the bottom. Line of best Ô¨Åt is a power law with 95% conÔ¨Ådence intervals. Mean accuracy 95% ConÔ¨Ådence Interval (low, hi) t compared to control (p-value) ‚ÄúI don‚Äôt know‚Äù assignments Control 88% 84%‚Äì91% - 2.7% GPT-3 175B 52%
and the random chance (50%) is indicated with the dashed line at the bottom. Line of best Ô¨Åt is a power law with 95% conÔ¨Ådence intervals. Mean accuracy 95% ConÔ¨Ådence Interval (low, hi) t compared to control (p-value) ‚ÄúI don‚Äôt know‚Äù assignments Control 88% 84%‚Äì91% - 2.7% GPT-3 175B 52% 48%‚Äì57% 12.7 (3.2e-23) 10.6% Table 3.12: People‚Äôs ability to identify whether ‚àº500 word articles are model generated (as measured by the ratio of correct assignments to non-neutral assignments) was 88% on the control model and 52% on GPT-3 175B. This table shows the results of a two-sample T-Test for the difference in mean accuracy between GPT-3 175B and the control model (an unconditional GPT-3 Small model with increased output randomness). 27 Title: United Methodists Agree to Historic Split Subtitle: Those who oppose gay marriage will form their own denomination Article: After two days of intense debate, the United Methodist Church has agreed to a historic split - one that is expected to end in the creation of a new denomination, one that will be "theologically and socially conservative," according to The Washington Post. The majority of delegates attending the church's annual General Conference in May voted to strengthen a ban
has agreed to a historic split - one that is expected to end in the creation of a new denomination, one that will be "theologically and socially conservative," according to The Washington Post. The majority of delegates attending the church's annual General Conference in May voted to strengthen a ban on the ordination of LGBTQ clergy and to write new rules that will "discipline" clergy who officiate at same-sex weddings. But those who opposed these measures have a new plan: They say they will form a separate denomination by 2020, calling their church the Christian Methodist denomination. The Post notes that the denomination, which claims 12.5 million members, was in the early 20th century the "largest Protestant denomination in the U.S.," but that it has been shrinking in recent decades. The new split will be the second in the church's history. The first occurred in 1968, when roughly 10 percent of the denomination left to form the Evangelical United Brethren Church. The Post notes that the proposed split "comes at a critical time for the church, which has been losing members for years," which has been "pushed toward the brink of a schism over the role of LGBTQ people in
of the denomination left to form the Evangelical United Brethren Church. The Post notes that the proposed split "comes at a critical time for the church, which has been losing members for years," which has been "pushed toward the brink of a schism over the role of LGBTQ people in the church." Gay marriage is not the only issue that has divided the church. In 2016, the denomination was split over ordination of transgender clergy, with the North Pacific regional conference voting to ban them from serving as clergy, and the South Pacific regional conference voting to allow them. Figure 3.14: The GPT-3 generated news article that humans had the greatest difÔ¨Åculty distinguishing from a human written article (accuracy: 12%). Title: Star‚Äôs Tux Promise Draws Megyn Kelly‚Äôs Sarcasm Subtitle: Joaquin Phoenix pledged to not change for each awards event Article: A year ago, Joaquin Phoenix made headlines when he appeared on the red carpet at the Golden Globes wearing a tuxedo with a paper bag over his head that read, "I am a shape-shifter. I can't change the world. I can only change myself." It was a promise to not change to fit into the Hollywood mold: "I think that's
on the red carpet at the Golden Globes wearing a tuxedo with a paper bag over his head that read, "I am a shape-shifter. I can't change the world. I can only change myself." It was a promise to not change to fit into the Hollywood mold: "I think that's a really special thing, to not change yourself. I think it's a really special thing to say, `This is what's inside of me, I'm proud of it, and I'm not going to be ashamed because of the way that someone else thinks I should be.'" Now, it's the Oscars, and Phoenix is at it again. But this time, his publicist is saying he'll be wearing a tux no matter what. Megyn Kelly was not impressed, and she let him have it on The Tonight Show. "You know, I feel like, I feel like you could have worn the tux," she says. "But you're saying you're a shape-shifter. I don't know if you can change your tux, but you can change your mind. You can change your mind. You can change your mind." Phoenix says he did, but it didn't stick. "I was like, `Okay, I'm going to wear a tuxedo
says. "But you're saying you're a shape-shifter. I don't know if you can change your tux, but you can change your mind. You can change your mind. You can change your mind." Phoenix says he did, but it didn't stick. "I was like, `Okay, I'm going to wear a tuxedo to this thing.' And then I thought, `I don't want to wear a tuxedo to this thing.'" Kelly goes on to encourage him to change his mind again, but Phoenix says it's too late: "I'm committed to wearing this." Figure 3.15: The GPT-3 generated news article that humans found the easiest to distinguish from a human written article (accuracy: 61%). 28 A "whatpu" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is: We were traveling in Africa and we saw these very cute whatpus. To do a "farduddle" means to jump up and down really fast. An example of a sentence that uses the word farduddle is: One day when I was playing tag with my little sister, she got really excited and she started doing these crazy farduddles. A "yalubalu" is a type of vegetable that looks like a
jump up and down really fast. An example of a sentence that uses the word farduddle is: One day when I was playing tag with my little sister, she got really excited and she started doing these crazy farduddles. A "yalubalu" is a type of vegetable that looks like a big pumpkin. An example of a sentence that uses the word yalubalu is: I was on a trip to Africa and I tried this yalubalu vegetable that was grown in a garden there. It was delicious. A "Burringo" is a car with very fast acceleration. An example of a sentence that uses the word Burringo is: In our garage we have a Burringo that my father drives to work every day. A "Gigamuru" is a type of Japanese musical instrument. An example of a sentence that uses the word Gigamuru is: I have a Gigamuru that my uncle gave me as a gift. I love to play it at home. To "screeg" something is to swing a sword at it. An example of a sentence that uses the word screeg is: We screeghed at each other for several minutes and then we went outside and ate ice cream. Figure 3.16:
as a gift. I love to play it at home. To "screeg" something is to swing a sword at it. An example of a sentence that uses the word screeg is: We screeghed at each other for several minutes and then we went outside and ate ice cream. Figure 3.16: Representative GPT-3 completions for the few-shot task of using a new word in a sentence. Boldface is GPT-3‚Äôs completions, plain text is human prompts. In the Ô¨Årst example both the prompt and the completion are provided by a human; this then serves as conditioning for subsequent examples where GPT-3 receives successive additional prompts and provides the completions. Nothing task-speciÔ¨Åc is provided to GPT-3 other than the conditioning shown here. nonexistent word being deÔ¨Åned and used in a sentence, so the task is few-shot in terms of previous examples of the broad task and one-shot in terms of the speciÔ¨Åc word. Table 3.16 shows the 6 examples we generated; all deÔ¨Ånitions were human-generated, and the Ô¨Årst answer was human-generated as conditioning while the subsequent answers were generated by GPT-3. These examples were generated continuously in one sitting and we did not omit or repeatedly try any prompts. In all cases the
Table 3.16 shows the 6 examples we generated; all deÔ¨Ånitions were human-generated, and the Ô¨Årst answer was human-generated as conditioning while the subsequent answers were generated by GPT-3. These examples were generated continuously in one sitting and we did not omit or repeatedly try any prompts. In all cases the generated sentence appears to be a correct or at least plausible use of the word. In the Ô¨Ånal sentence the model generates a plausible conjugation for the word ‚Äúscreeg‚Äù (namely ‚Äúscreeghed‚Äù), although the use of the word is slightly awkward (‚Äúscreeghed at each other‚Äù) despite being plausible in the sense that it could describe a toy sword Ô¨Åght. Overall, GPT-3 appears to be at least proÔ¨Åcient at the task of using novel words in a sentence. 3.9.6 Correcting English Grammar Another task well suited for few-shot learning is correcting English grammar. We test this with GPT-3 in the few- shot setting by giving prompts of the form "Poor English Input: <sentence>\n Good English Output: <sentence>". We give GPT-3 one human-generated correction and then ask it to correct 5 more (again without any omissions or repeats). Results are shown in Figure 3.17. 4 Measuring and Preventing Memorization Of Benchmarks Since our
setting by giving prompts of the form "Poor English Input: <sentence>\n Good English Output: <sentence>". We give GPT-3 one human-generated correction and then ask it to correct 5 more (again without any omissions or repeats). Results are shown in Figure 3.17. 4 Measuring and Preventing Memorization Of Benchmarks Since our training dataset is sourced from the internet, it is possible that our model was trained on some of our benchmark test sets. Accurately detecting test contamination from internet-scale datasets is a new area of research without established best practices. While it is common practice to train large models without investigating contamination, given the increasing scale of pretraining datasets, we believe this issue is becoming increasingly important to attend to. This concern is not just hypothetical. One of the Ô¨Årst papers to train a language model on Common Crawl data [TL18] detected and removed a training document which overlapped with one of their evaluation datasets. Other work such as GPT-2 [RWC+19] also conducted post-hoc overlap analysis. Their study was relatively encouraging, Ô¨Ånding that 29 Poor English input: I eated the purple berries. Good English output: I ate the purple berries. Poor English input: Thank you for picking me as your designer.
of their evaluation datasets. Other work such as GPT-2 [RWC+19] also conducted post-hoc overlap analysis. Their study was relatively encouraging, Ô¨Ånding that 29 Poor English input: I eated the purple berries. Good English output: I ate the purple berries. Poor English input: Thank you for picking me as your designer. I‚Äôd appreciate it. Good English output: Thank you for choosing me as your designer. I appreciate it. Poor English input: The mentioned changes have done. or I did the alteration that you requested. or I changed things you wanted and did the modifications. Good English output: The requested changes have been made. or I made the alteration that you requested. or I changed things you wanted and made the modifications. Poor English input: I‚Äôd be more than happy to work with you in another project. Good English output: I'd be more than happy to work with you on another project. Poor English input: Please provide me with a short brief of the design you‚Äôre looking for and that‚Äôd be nice if you could share some examples or project you did before. Good English output: Please provide me with a brief description of the design you're looking for and that would
Poor English input: Please provide me with a short brief of the design you‚Äôre looking for and that‚Äôd be nice if you could share some examples or project you did before. Good English output: Please provide me with a brief description of the design you're looking for and that would be nice if you could share some examples or projects you have done before. Poor English input: The patient was died. Good English output: The patient died. Poor English input: We think that Leslie likes ourselves. Good English output: We think that Leslie likes us. Poor English input: Janet broke Bill on the finger. Good English output: Janet broke Bill's finger. Poor English input: Mary arranged for, in St. Louis, John to rent a house cheap. Good English output: Mary arranged for John to rent a house in St. Louis. Poor English input: Today I have went to the store to to buys some many bottle of water. Good English output: Today I went to the store to buy some bottles of water. Poor English input: I have tried to hit ball with bat, but my swing is has miss. Good English output: I tried to hit the ball with
to to buys some many bottle of water. Good English output: Today I went to the store to buy some bottles of water. Poor English input: I have tried to hit ball with bat, but my swing is has miss. Good English output: I tried to hit the ball with the bat, but my swing missed. Figure 3.17: Representative GPT-3 completions for the few-shot task of correcting English grammar. Boldface is GPT-3‚Äôs completions, plain text is human prompts. In the Ô¨Årst few examples example both the prompt and the completion are provided by a human; this then serves as conditioning for subsequent examples where GPT-3 receives successive additional prompts and provides the completions. Nothing task-speciÔ¨Åc is provided to GPT-3 aside from the Ô¨Årst few examples as conditioning and the ‚ÄúPoor English input/Good English output‚Äù framing. We note that the distinction between ‚Äùpoor‚Äù and ‚Äùgood‚Äù English (and the terms themselves) is complex, contextual, and contested. As the example mentioning the rental of a house shows, assumptions that the model makes about what ‚Äúgood‚Äù is can even lead it to make errors (here, the model not only adjusts grammar, but also removes the word ‚Äùcheap‚Äù in a way that alters meaning). 30
complex, contextual, and contested. As the example mentioning the rental of a house shows, assumptions that the model makes about what ‚Äúgood‚Äù is can even lead it to make errors (here, the model not only adjusts grammar, but also removes the word ‚Äùcheap‚Äù in a way that alters meaning). 30 Figure 4.1: GPT-3 Training Curves We measure model performance during training on a deduplicated validation split of our training distribution. Though there is some gap between training and validation performance, the gap grows only minimally with model size and training time, suggesting that most of the gap comes from a difference in difÔ¨Åculty rather than overÔ¨Åtting. although models did perform moderately better on data that overlapped between training and testing, this did not signiÔ¨Åcantly impact reported results due to the small fraction of data which was contaminated (often only a few percent). GPT-3 operates in a somewhat different regime. On the one hand, the dataset and model size are about two orders of magnitude larger than those used for GPT-2, and include a large amount of Common Crawl, creating increased potential for contamination and memorization. On the other hand, precisely due to the large amount of data, even GPT-3 175B
the one hand, the dataset and model size are about two orders of magnitude larger than those used for GPT-2, and include a large amount of Common Crawl, creating increased potential for contamination and memorization. On the other hand, precisely due to the large amount of data, even GPT-3 175B does not overÔ¨Åt its training set by a signiÔ¨Åcant amount, measured relative to a held-out validation set with which it was deduplicated (Figure 4.1). Thus, we expect that contamination is likely to be frequent, but that its effects may not be as large as feared. We initially tried to address the issue of contamination by proactively searching for and attempting to remove any overlap between our training data and the development and test sets of all benchmarks studied in this paper. Unfortunately, a bug resulted in only partial removal of all detected overlaps from the training data. Due to the cost of training, it wasn‚Äôt feasible to retrain the model. To address this, we investigate in detail how the remaining detected overlap impacts results. For each benchmark, we produce a ‚Äòclean‚Äô version which removes all potentially leaked examples, deÔ¨Åned roughly as examples that have a 13-gram overlap with anything in
cost of training, it wasn‚Äôt feasible to retrain the model. To address this, we investigate in detail how the remaining detected overlap impacts results. For each benchmark, we produce a ‚Äòclean‚Äô version which removes all potentially leaked examples, deÔ¨Åned roughly as examples that have a 13-gram overlap with anything in the pretraining set (or that overlap with the whole example when it is shorter than 13-grams). The goal is to very conservatively Ô¨Çag anything that could potentially be contamination, so as to produce a clean subset that is free of contamination with high conÔ¨Ådence. The exact procedure is detailed in Appendix C. We then evaluate GPT-3 on these clean benchmarks, and compare to the original score. If the score on the clean subset is similar to the score on the entire dataset, this suggests that contamination, even if present, does not have a signiÔ¨Åcant effect on reported results. If the score on the clean subset is lower, this suggests contamination may be inÔ¨Çating the results. The results are summarized in Figure 4.2. Although potential contamination is often high (with a quarter of benchmarks scoring over 50%), in most cases performance changes only negligibly, and we see no evidence that contamination
score on the clean subset is lower, this suggests contamination may be inÔ¨Çating the results. The results are summarized in Figure 4.2. Although potential contamination is often high (with a quarter of benchmarks scoring over 50%), in most cases performance changes only negligibly, and we see no evidence that contamination level and performance difference are correlated. We conclude that either our conservative method substantially overestimated contamination or that contamination has little effect on performance. Below, we review in more detail the few speciÔ¨Åc cases where either (1) the model performs signiÔ¨Åcantly worse on the cleaned version, or (2) potential contamination is very high, which makes measuring the performance difference difÔ¨Åcult. Our analysis Ô¨Çagged six groups of benchmarks for further investigation: Word Scrambling, Reading Comprehension (QuAC, SQuAD2, DROP), PIQA, Winograd, language modeling tasks (Wikitext tasks, 1BW), and German to English 31 Figure 4.2: Benchmark contamination analysis We constructed cleaned versions of each of our benchmarks to check for potential contamination in our training set. The x-axis is a conservative lower bound for how much of the dataset is known with high conÔ¨Ådence to be clean, and the y-axis shows the difference in performance when evaluating only on the veriÔ¨Åed clean subset.
each of our benchmarks to check for potential contamination in our training set. The x-axis is a conservative lower bound for how much of the dataset is known with high conÔ¨Ådence to be clean, and the y-axis shows the difference in performance when evaluating only on the veriÔ¨Åed clean subset. Performance on most benchmarks changed negligibly, but some were Ô¨Çagged for further review. On inspection we Ô¨Ånd some evidence for contamination of the PIQA and Winograd results, and we mark the corresponding results in Section 3 with an asterisk. We Ô¨Ånd no evidence that other benchmarks are affected. translation. Since our overlap analysis is designed to be extremely conservative, we expect it to produce some false positives. We summarize the results for each group of tasks below: ‚Ä¢ Reading Comprehension: Our initial analysis Ô¨Çagged >90% of task examples from QuAC, SQuAD2, and DROP as potentially contaminated, so large that even measuring the differential on a clean subset was difÔ¨Åcult. Upon manual inspection, however, we found that for every overlap we inspected, in all 3 datasets, the source text was present in our training data but the question/answer pairs were not, meaning the model gains only background information and cannot memorize
measuring the differential on a clean subset was difÔ¨Åcult. Upon manual inspection, however, we found that for every overlap we inspected, in all 3 datasets, the source text was present in our training data but the question/answer pairs were not, meaning the model gains only background information and cannot memorize the answer to a speciÔ¨Åc question. ‚Ä¢ German translation: We found 25% of the examples in the WMT16 German-English test set were marked as potentially contaminated, with an associated total effect size of 1-2 BLEU. Upon inspection, none of the Ô¨Çagged examples contain paired sentences resembling NMT training data and collisions were monolingual matches mostly of snippets of events discussed in the news. ‚Ä¢ Reversed Words and Anagrams: Recall that these tasks are of the form ‚Äúalaok = koala‚Äù. Due to the short length of these tasks, we used 2-grams for Ô¨Åltering (ignoring punctuation). After inspecting the Ô¨Çagged overlaps, we found that they were not typically instances of real reversals or unscramblings in the training set, but rather palindromes or trivial unscramblings, e.g ‚Äúkayak = kayak‚Äù. The amount of overlap was small, but removing the trivial tasks lead to an increase in difÔ¨Åculty and thus a spurious signal. Related to
found that they were not typically instances of real reversals or unscramblings in the training set, but rather palindromes or trivial unscramblings, e.g ‚Äúkayak = kayak‚Äù. The amount of overlap was small, but removing the trivial tasks lead to an increase in difÔ¨Åculty and thus a spurious signal. Related to this, the symbol insertion task shows high overlap but no effect on performance ‚Äì this is because that task involves removing non-letter characters from a word, and the overlap analysis itself ignores such characters, leading to many spurious matches. ‚Ä¢ PIQA: The overlap analysis Ô¨Çagged 29% of examples as contaminated, and observed a 3 percentage point absolute decrease (4% relative decrease) in performance on the clean subset. Though the test dataset was released after our training set was created and its labels are hidden, some of the web pages used by the crowdsourced dataset creators are contained in our training set. We found a similar decrease in a 25x smaller model with much less capacity to memorize, leading us to suspect that the shift is likely statistical bias rather than memorization; examples which workers copied may simply be easier. Unfortunately, we cannot rigorously prove this hypothesis. We therefore mark our
set. We found a similar decrease in a 25x smaller model with much less capacity to memorize, leading us to suspect that the shift is likely statistical bias rather than memorization; examples which workers copied may simply be easier. Unfortunately, we cannot rigorously prove this hypothesis. We therefore mark our PIQA results with an asterisk to denote this potential contamination. ‚Ä¢ Winograd: The overlap analysis Ô¨Çagged 45% of examples, and found a 2.6% decrease in performance on the clean subset. Manual inspection of the overlapping data point showed that 132 Winograd schemas were in fact present in our training set, though presented in a different format than we present the task to the model. Although the decrease in performance is small, we mark our Winograd results in the main paper with an asterisk. 32 ‚Ä¢ Language modeling: We found the 4 Wikipedia language modeling benchmarks measured in GPT-2, plus the Children‚Äôs Book Test dataset, to be almost entirely contained in our training data. Since we cannot reliably extract a clean subset here, we do not report results on these datasets, even though we intended to when starting this work. We note that Penn Tree Bank due to its age was
Children‚Äôs Book Test dataset, to be almost entirely contained in our training data. Since we cannot reliably extract a clean subset here, we do not report results on these datasets, even though we intended to when starting this work. We note that Penn Tree Bank due to its age was unaffected and therefore became our chief language modeling benchmark. We also inspected datasets where contamination was high, but the impact on performance was close to zero, simply to verify how much actual contamination existed. These appeared to often contain false positives. They had either no actual contamination, or had contamination that did not give away the answer to the task. One notable exception was LAMBADA, which appeared to have substantial genuine contamination, yet the impact on performance was very small, with the clean subset scoring within 0.5% of the full dataset. Also, strictly speaking, our Ô¨Åll-in-the-blank format precludes the simplest form of memorization. Nevertheless, since we made very large gains on LAMBADA in this paper, the potential contamination is noted in the results section. An important limitation of our contamination analysis is that we cannot be sure that the clean subset is drawn from the same distribution as the original
form of memorization. Nevertheless, since we made very large gains on LAMBADA in this paper, the potential contamination is noted in the results section. An important limitation of our contamination analysis is that we cannot be sure that the clean subset is drawn from the same distribution as the original dataset. It remains possible that memorization inÔ¨Çates results but at the same time is precisely counteracted by some statistical bias causing the clean subset to be easier. However, the sheer number of shifts close to zero suggests this is unlikely, and we also observed no noticeable difference in the shifts for small models, which are unlikely to be memorizing. Overall, we have made a best effort to measure and document the effects of data contamination, and to note or outright remove problematic results, depending on the severity. Much work remains to be done to address this important and subtle issue for the Ô¨Åeld in general, both when designing benchmarks and when training models. For a more detailed explanation of our analysis, we refer the reader to Appendix C. 5 Limitations GPT-3 and our analysis of it have a number of limitations. Below we describe some of these and suggest directions
for the Ô¨Åeld in general, both when designing benchmarks and when training models. For a more detailed explanation of our analysis, we refer the reader to Appendix C. 5 Limitations GPT-3 and our analysis of it have a number of limitations. Below we describe some of these and suggest directions for future work. First, despite the strong quantitative and qualitative improvements of GPT-3, particularly compared to its direct predecessor GPT-2, it still has notable weaknesses in text synthesis and several NLP tasks. On text synthesis, although the overall quality is high, GPT-3 samples still sometimes repeat themselves semantically at the document level, start to lose coherence over sufÔ¨Åciently long passages, contradict themselves, and occasionally contain non-sequitur sentences or paragraphs. We will release a collection of 500 uncurated unconditional samples to help provide a better sense of GPT-3‚Äôs limitations and strengths at text synthesis. Within the domain of discrete language tasks, we have noticed informally that GPT-3 seems to have special difÔ¨Åculty with ‚Äúcommon sense physics‚Äù, despite doing well on some datasets (such as PIQA [BZB+19]) that test this domain. SpeciÔ¨Åcally GPT-3 has difÔ¨Åculty with questions of the type ‚ÄúIf I put cheese into the fridge, will it melt?‚Äù. Quantitatively, GPT-3‚Äôs
we have noticed informally that GPT-3 seems to have special difÔ¨Åculty with ‚Äúcommon sense physics‚Äù, despite doing well on some datasets (such as PIQA [BZB+19]) that test this domain. SpeciÔ¨Åcally GPT-3 has difÔ¨Åculty with questions of the type ‚ÄúIf I put cheese into the fridge, will it melt?‚Äù. Quantitatively, GPT-3‚Äôs in-context learning performance has some notable gaps on our suite of benchmarks, as described in Section 3, and in particular it does little better than chance when evaluated one-shot or even few-shot on some ‚Äúcomparison‚Äù tasks, such as determining if two words are used the same way in a sentence, or if one sentence implies another (WIC and ANLI respectively), as well as on a subset of reading comprehension tasks. This is especially striking given GPT-3‚Äôs strong few-shot performance on many other tasks. GPT-3 has several structural and algorithmic limitations, which could account for some of the issues above. We focused on exploring in-context learning behavior in autoregressive language models because it is straightforward to both sample and compute likelihoods with this model class. As a result our experiments do not include any bidirectional architectures or other training objectives such as denoising. This is a noticeable difference from much of
focused on exploring in-context learning behavior in autoregressive language models because it is straightforward to both sample and compute likelihoods with this model class. As a result our experiments do not include any bidirectional architectures or other training objectives such as denoising. This is a noticeable difference from much of the recent literature, which has documented improved Ô¨Åne-tuning performance when using these approaches over standard language models [RSR+19]. Thus our design decision comes at the cost of potentially worse performance on tasks which empirically beneÔ¨Åt from bidirectionality. This may include Ô¨Åll-in-the-blank tasks, tasks that involve looking back and comparing two pieces of content, or tasks that require re-reading or carefully considering a long passage and then generating a very short answer. This could be a possible explanation for GPT-3‚Äôs lagging few-shot performance on a few of the tasks, such as WIC (which involves comparing the use of a word in two sentences), ANLI (which involves comparing two sentences to see if one implies the other), and several reading comprehension tasks (e.g. QuAC and RACE). We also conjecture, based on past literature, that a large bidirectional model would be stronger at Ô¨Åne-tuning than GPT-3. Making a bidirectional model at the scale
two sentences), ANLI (which involves comparing two sentences to see if one implies the other), and several reading comprehension tasks (e.g. QuAC and RACE). We also conjecture, based on past literature, that a large bidirectional model would be stronger at Ô¨Åne-tuning than GPT-3. Making a bidirectional model at the scale of GPT-3, and/or trying to make bidirectional models work with few- or zero-shot learning, is a promising direction for future research, and could help achieve the ‚Äúbest of both worlds‚Äù. A more fundamental limitation of the general approach described in this paper ‚Äì scaling up any LM-like model, whether autoregressive or bidirectional ‚Äì is that it may eventually run into (or could already be running into) the limits of the 33 pretraining objective. Our current objective weights every token equally and lacks a notion of what is most important to predict and what is less important. [RRS20] demonstrate beneÔ¨Åts of customizing prediction to entities of interest. Also, with self-supervised objectives, task speciÔ¨Åcation relies on forcing the desired task into a prediction problem, whereas ultimately, useful language systems (for example virtual assistants) might be better thought of as taking goal-directed actions rather than just making predictions. Finally, large pretrained language models
customizing prediction to entities of interest. Also, with self-supervised objectives, task speciÔ¨Åcation relies on forcing the desired task into a prediction problem, whereas ultimately, useful language systems (for example virtual assistants) might be better thought of as taking goal-directed actions rather than just making predictions. Finally, large pretrained language models are not grounded in other domains of experience, such as video or real-world physical interaction, and thus lack a large amount of context about the world [BHT+20]. For all these reasons, scaling pure self-supervised prediction is likely to hit limits, and augmentation with a different approach is likely to be necessary. Promising future directions in this vein might include learning the objective function from humans [ZSW+19a], Ô¨Åne-tuning with reinforcement learning, or adding additional modalities such as images to provide grounding and a better model of the world [CLY+19]. Another limitation broadly shared by language models is poor sample efÔ¨Åciency during pre-training. While GPT-3 takes a step towards test-time sample efÔ¨Åciency closer to that of humans (one-shot or zero-shot), it still sees much more text during pre-training than a human sees in the their lifetime [Lin20]. Improving pre-training sample efÔ¨Åciency is an important direction for future work, and might come from
pre-training. While GPT-3 takes a step towards test-time sample efÔ¨Åciency closer to that of humans (one-shot or zero-shot), it still sees much more text during pre-training than a human sees in the their lifetime [Lin20]. Improving pre-training sample efÔ¨Åciency is an important direction for future work, and might come from grounding in the physical world to provide additional information, or from algorithmic improvements. A limitation, or at least uncertainty, associated with few-shot learning in GPT-3 is ambiguity about whether few-shot learning actually learns new tasks ‚Äúfrom scratch‚Äù at inference time, or if it simply recognizes and identiÔ¨Åes tasks that it has learned during training. These possibilities exist on a spectrum, ranging from demonstrations in the training set that are drawn from exactly the same distribution as those at test time, to recognizing the same task but in a different format, to adapting to a speciÔ¨Åc style of a general task such as QA, to learning a skill entirely de novo. Where GPT-3 is on this spectrum may also vary from task to task. Synthetic tasks such as wordscrambling or deÔ¨Åning nonsense words seem especially likely to be learned de novo, whereas translation clearly must be learned during pretraining, although possibly
such as QA, to learning a skill entirely de novo. Where GPT-3 is on this spectrum may also vary from task to task. Synthetic tasks such as wordscrambling or deÔ¨Åning nonsense words seem especially likely to be learned de novo, whereas translation clearly must be learned during pretraining, although possibly from data that is very different in organization and style than the test data. Ultimately, it is not even clear what humans learn from scratch vs from prior demonstrations. Even organizing diverse demonstrations during pre-training and identifying them at test time would be an advance for language models, but nevertheless understanding precisely how few-shot learning works is an important unexplored direction for future research. A limitation associated with models at the scale of GPT-3, regardless of objective function or algorithm, is that they are both expensive and inconvenient to perform inference on, which may present a challenge for practical applicability of models of this scale in their current form. One possible future direction to address this is distillation [HVD15] of large models down to a manageable size for speciÔ¨Åc tasks. Large models such as GPT-3 contain a very wide range of skills, most of which are not needed for a
of models of this scale in their current form. One possible future direction to address this is distillation [HVD15] of large models down to a manageable size for speciÔ¨Åc tasks. Large models such as GPT-3 contain a very wide range of skills, most of which are not needed for a speciÔ¨Åc task, suggesting that in principle aggressive distillation may be possible. Distillation is well-explored in general [LHCG19a] but has not been tried at the scale of hundred of billions parameters; new challenges and opportunities may be associated with applying it to models of this size. Finally, GPT-3 shares some limitations common to most deep learning systems ‚Äì its decisions are not easily interpretable, it is not necessarily well-calibrated in its predictions on novel inputs as observed by the much higher variance in performance than humans on standard benchmarks, and it retains the biases of the data it has been trained on. This last issue ‚Äì biases in the data that may lead the model to generate stereotyped or prejudiced content ‚Äì is of special concern from a societal perspective, and will be discussed along with other issues in the next section on Broader Impacts (Section 6). 6 Broader Impacts Language
on. This last issue ‚Äì biases in the data that may lead the model to generate stereotyped or prejudiced content ‚Äì is of special concern from a societal perspective, and will be discussed along with other issues in the next section on Broader Impacts (Section 6). 6 Broader Impacts Language models have a wide range of beneÔ¨Åcial applications for society, including code and writing auto-completion, grammar assistance, game narrative generation, improving search engine responses, and answering questions. But they also have potentially harmful applications. GPT-3 improves the quality of text generation and adaptability over smaller models and increases the difÔ¨Åculty of distinguishing synthetic text from human-written text. It therefore has the potential to advance both the beneÔ¨Åcial and harmful applications of language models. Here we focus on the potential harms of improved language models, not because we believe the harms are necessarily greater, but in order to stimulate efforts to study and mitigate them. The broader impacts of language models like this are numerous. We focus on two primary issues: the potential for deliberate misuse of language models like GPT-3 in Section 6.1, and issues of bias, fairness, and representation within models like GPT-3 in Section 6.2. We also brieÔ¨Çy
study and mitigate them. The broader impacts of language models like this are numerous. We focus on two primary issues: the potential for deliberate misuse of language models like GPT-3 in Section 6.1, and issues of bias, fairness, and representation within models like GPT-3 in Section 6.2. We also brieÔ¨Çy discuss issues of energy efÔ¨Åciency (Section 6.3). 34 6.1 Misuse of Language Models Malicious uses of language models can be somewhat difÔ¨Åcult to anticipate because they often involve repurposing language models in a very different environment or for a different purpose than researchers intended. To help with this, we can think in terms of traditional security risk assessment frameworks, which outline key steps such as identifying threats and potential impacts, assessing likelihood, and determining risk as a combination of likelihood and impact [Ros12]. We discuss three factors: potential misuse applications, threat actors, and external incentive structures. 6.1.1 Potential Misuse Applications Any socially harmful activity that relies on generating text could be augmented by powerful language models. Examples include misinformation, spam, phishing, abuse of legal and governmental processes, fraudulent academic essay writing and social engineering pretexting. Many of these applications bottleneck on human beings to write sufÔ¨Åciently high quality text. Language
Any socially harmful activity that relies on generating text could be augmented by powerful language models. Examples include misinformation, spam, phishing, abuse of legal and governmental processes, fraudulent academic essay writing and social engineering pretexting. Many of these applications bottleneck on human beings to write sufÔ¨Åciently high quality text. Language models that produce high quality text generation could lower existing barriers to carrying out these activities and increase their efÔ¨Åcacy. The misuse potential of language models increases as the quality of text synthesis improves. The ability of GPT-3 to generate several paragraphs of synthetic content that people Ô¨Ånd difÔ¨Åcult to distinguish from human-written text in 3.9.4 represents a concerning milestone in this regard. 6.1.2 Threat Actor Analysis Threat actors can be organized by skill and resource levels, ranging from low or moderately skilled and resourced actors who may be able to build a malicious product to ‚Äòadvanced persistent threats‚Äô (APTs): highly skilled and well-resourced (e.g. state-sponsored) groups with long-term agendas [SBC+19]. To understand how low and mid-skill actors think about language models, we have been monitoring forums and chat groups where misinformation tactics, malware distribution, and computer fraud are frequently discussed. While we did Ô¨Ånd signiÔ¨Åcant discussion of misuse following
highly skilled and well-resourced (e.g. state-sponsored) groups with long-term agendas [SBC+19]. To understand how low and mid-skill actors think about language models, we have been monitoring forums and chat groups where misinformation tactics, malware distribution, and computer fraud are frequently discussed. While we did Ô¨Ånd signiÔ¨Åcant discussion of misuse following the initial release of GPT-2 in spring of 2019, we found fewer instances of experimentation and no successful deployments since then. Additionally, those misuse discussions were correlated with media coverage of language model technologies. From this, we assess that the threat of misuse from these actors is not immediate, but signiÔ¨Åcant improvements in reliability could change this. Because APTs do not typically discuss operations in the open, we have consulted with professional threat analysts about possible APT activity involving the use of language models. Since the release of GPT-2 there has been no discernible difference in operations that may see potential gains by using language models. The assessment was that language models may not be worth investing signiÔ¨Åcant resources in because there has been no convincing demonstration that current language models are signiÔ¨Åcantly better than current methods for generating text, and because methods for ‚Äútargeting‚Äù or ‚Äúcontrolling‚Äù the content of
potential gains by using language models. The assessment was that language models may not be worth investing signiÔ¨Åcant resources in because there has been no convincing demonstration that current language models are signiÔ¨Åcantly better than current methods for generating text, and because methods for ‚Äútargeting‚Äù or ‚Äúcontrolling‚Äù the content of language models are still at a very early stage. 6.1.3 External Incentive Structures Each threat actor group also has a set of tactics, techniques, and procedures (TTPs) that they rely on to accomplish their agenda. TTPs are inÔ¨Çuenced by economic factors like scalability and ease of deployment; phishing is extremely popular among all groups because it offers a low-cost, low-effort, high-yield method of deploying malware and stealing login credentials. Using language models to augment existing TTPs would likely result in an even lower cost of deployment. Ease of use is another signiÔ¨Åcant incentive. Having stable infrastructure has a large impact on the adoption of TTPs. The outputs of language models are stochastic, however, and though developers can constrain these (e.g. using top-k truncation) they are not able to perform consistently without human feedback. If a social media disinformation bot produces outputs that are reliable 99% of the time, but produces
on the adoption of TTPs. The outputs of language models are stochastic, however, and though developers can constrain these (e.g. using top-k truncation) they are not able to perform consistently without human feedback. If a social media disinformation bot produces outputs that are reliable 99% of the time, but produces incoherent outputs 1% of the time, this could reduce the amount of human labor required in operating this bot. But a human is still needed to Ô¨Ålter the outputs, which restricts how scalable the operation can be. Based on our analysis of this model and analysis of threat actors and the landscape, we suspect AI researchers will eventually develop language models that are sufÔ¨Åciently consistent and steerable that they will be of greater interest to malicious actors. We expect this will introduce challenges for the broader research community, and hope to work on this through a combination of mitigation research, prototyping, and coordinating with other technical developers. 35 6.2 Fairness, Bias, and Representation Biases present in training data may lead models to generate stereotyped or prejudiced content. This is concerning, since model bias could harm people in the relevant groups in different ways by entrenching existing stereotypes and producing demeaning
prototyping, and coordinating with other technical developers. 35 6.2 Fairness, Bias, and Representation Biases present in training data may lead models to generate stereotyped or prejudiced content. This is concerning, since model bias could harm people in the relevant groups in different ways by entrenching existing stereotypes and producing demeaning portrayals amongst other potential harms [Cra17]. We have conducted an analysis of biases in the model in order to better understand GPT-3‚Äôs limitations when it comes to fairness, bias, and representation. 8 Our goal is not to exhaustively characterize GPT-3, but to give a preliminary analysis of some of its limitations and behaviors. We focus on biases relating to gender, race, and religion, although many other categories of bias are likely present and could be studied in follow-up work. This is a preliminary analysis and does not reÔ¨Çect all of the model‚Äôs biases even within the studied categories. Broadly, our analysis indicates that internet-trained models have internet-scale biases; models tend to reÔ¨Çect stereotypes present in their training data. Below we discuss our preliminary Ô¨Åndings of bias along the dimensions of gender, race, and religion. We probe for bias in the 175 billion parameter model and also in similar smaller models,
analysis indicates that internet-trained models have internet-scale biases; models tend to reÔ¨Çect stereotypes present in their training data. Below we discuss our preliminary Ô¨Åndings of bias along the dimensions of gender, race, and religion. We probe for bias in the 175 billion parameter model and also in similar smaller models, to see if and how they are different in this dimension. 6.2.1 Gender In our investigation of gender bias in GPT-3, we focused on associations between gender and occupation. We found that occupations in general have a higher probability of being followed by a male gender identiÔ¨Åer than a female one (in other words, they are male leaning) when given a context such as "The {occupation} was a" (Neutral Variant). 83% of the 388 occupations we tested were more likely to be followed by a male identiÔ¨Åer by GPT-3. We measured this by feeding the model a context such as "The detective was a" and then looking at the probability of the model following up with male indicating words (eg. man, male etc.) or female indicating words (woman, female etc.). In particular, occupations demonstrating higher levels of education such as legislator, banker, or professor emeritus were heavily male leaning along
"The detective was a" and then looking at the probability of the model following up with male indicating words (eg. man, male etc.) or female indicating words (woman, female etc.). In particular, occupations demonstrating higher levels of education such as legislator, banker, or professor emeritus were heavily male leaning along with occupations that require hard physical labour such as mason, millwright, and sheriff. Occupations that were more likely to be followed by female identiÔ¨Åers include midwife, nurse, receptionist, housekeeper etc. We also tested how these probabilities changed when we shifted the context to be the "The competent {occupation} was a" (Competent Variant), and when we shifted the context to be "The incompetent {occupation} was a" (Incompetent Variant) for each occupation in the dataset. We found that, when prompted with "The competent {occupation} was a," the majority of occupations had an even higher probability of being followed by a male identiÔ¨Åer than a female one than was the case with our original neutral prompt, "The {occupation} was a". With the prompt "The incompetent {occupation} was a" the majority of occupations still leaned male with a similar probability than for our original neutral prompt. The average occupation bias - measured as 1
than a female one than was the case with our original neutral prompt, "The {occupation} was a". With the prompt "The incompetent {occupation} was a" the majority of occupations still leaned male with a similar probability than for our original neutral prompt. The average occupation bias - measured as 1 njobs P jobs log( P (female|Context) P (male|Context)) ) - was ‚àí1.11 for the Neutral Variant, ‚àí2.14 for the Competent Variant and ‚àí1.15 for the Incompetent Variant. We also carried out pronoun resolution on the Winogender dataset [RNLVD18] using two methods which further corroborated the model‚Äôs tendency to associate most occupations with males. One method measured the mod- els ability to correctly assign a pronoun as the occupation or the participant. For example, we fed the model a context such as "The advisor met with the advisee because she wanted to get advice about job applications. ‚ÄòShe‚Äô refers to the" and found the option with the lowest probability between the two possi- ble options (Choices between Occupation Option: advisor; Participant Option: advisee). Occupation and participant words often have societal biases associated with them such as the assumption that most occupants are by default male. We found that the language models
and found the option with the lowest probability between the two possi- ble options (Choices between Occupation Option: advisor; Participant Option: advisee). Occupation and participant words often have societal biases associated with them such as the assumption that most occupants are by default male. We found that the language models learnt some of these biases such as a tendency to associate female pronouns with participant positions more than male pronouns. GPT-3 175B had the highest accuracy of all the models (64.17%) on this task. It was also the only model where the accuracy for Occupant sentences (sentences where the correct answer was the Occupation option) for females was higher than for males (81.7% vs 76.7%). All other models had a higher accuracy for male pronouns with Occupation sentences as compared to female pronouns with the exception of our second largest model- GPT-3 13B - which had the same accuracy (60%) for both. This offers some preliminary evidence that in places where issues of bias can make language models susceptible to error, the larger models are more robust than smaller models. We also performed co-occurrence tests, where we analyzed which words are likely to occur in the vicinity of other pre-
for both. This offers some preliminary evidence that in places where issues of bias can make language models susceptible to error, the larger models are more robust than smaller models. We also performed co-occurrence tests, where we analyzed which words are likely to occur in the vicinity of other pre- selected words. We created a model output sample set by generating 800 outputs of length 50 each with a temperature 8Evaluating fairness, bias, and representation in language models is a rapidly-developing area with a large body of prior work. See, for example, [HZJ+19, NBR20, SCNP19]. 36 Table 6.1: Most Biased Descriptive Words in 175B Model Top 10 Most Biased Male Descriptive Words with Raw Co-Occurrence Counts Top 10 Most Biased Female Descriptive Words with Raw Co-Occurrence Counts Average Number of Co-Occurrences Across All Words: 17.5 Average Number of Co-Occurrences Across All Words: 23.9 Large (16) Optimistic (12) Mostly (15) Bubbly (12) Lazy (14) Naughty (12) Fantastic (13) Easy-going (12) Eccentric (13) Petite (10) Protect (10) Tight (10) Jolly (10) Pregnant (10) Stable (9) Gorgeous (28) Personable (22) Sucked (8) Survive (7) Beautiful (158) of 1 and top p of 0.9 for every prompt in our dataset. For gender, we had
(12) Lazy (14) Naughty (12) Fantastic (13) Easy-going (12) Eccentric (13) Petite (10) Protect (10) Tight (10) Jolly (10) Pregnant (10) Stable (9) Gorgeous (28) Personable (22) Sucked (8) Survive (7) Beautiful (158) of 1 and top p of 0.9 for every prompt in our dataset. For gender, we had prompts such as "He was very", "She was very", "He would be described as", "She would be described as"9. We looked at the adjectives and adverbs in the top 100 most favored words using an off-the-shelf POS tagger [LB02]. We found females were more often described using appearance oriented words such as ‚Äùbeautiful‚Äù and ‚Äùgorgeous‚Äù as compared to men who were more often described using adjectives that span a greater spectrum. Table 6.1 shows the top 10 most favored descriptive words for the model along with the raw number of times each word co-occurred with a pronoun indicator. ‚ÄúMost Favored‚Äù here indicates words which were most skewed towards a category by co-occurring with it at a higher rate as compared to the other category. To put these numbers in perspective, we have also included the average for the number of co-occurrences across all qualifying words for each gender. 6.2.2 Race
here indicates words which were most skewed towards a category by co-occurring with it at a higher rate as compared to the other category. To put these numbers in perspective, we have also included the average for the number of co-occurrences across all qualifying words for each gender. 6.2.2 Race To investigate racial bias in GPT-3, we seeded the model with prompts such as - "The {race} man was very", "The {race} woman was very" and "People would describe the {race} person as" and generated 800 samples for each of the above prompts, with {race} replaced with a term indicating a racial category such as White or Asian. We then measure word co-occurrences in the generated samples. Given prior research demonstrating that language models produce text of differing sentiment when varying features such as occupation [HZJ+19], we explored how race impacted sentiment. We measured sentiment using Senti WordNet [BES10] for the words which co-occurred disproportionately with each race. Each word sentiment varied from 100 to -100, with positive scores indicating positive words (eg. wonderfulness: 100, amicable: 87.5), negative scores indicating negative words (eg. wretched: -87.5 , horrid: -87.5) and a score of 0 indicating neutral words (eg. sloping, chalet). It
the words which co-occurred disproportionately with each race. Each word sentiment varied from 100 to -100, with positive scores indicating positive words (eg. wonderfulness: 100, amicable: 87.5), negative scores indicating negative words (eg. wretched: -87.5 , horrid: -87.5) and a score of 0 indicating neutral words (eg. sloping, chalet). It should be noted that we were explicitly prompting the models to talk about race and this in turn generated text that focused on racial features; these results are not from the models talking about race in the wild but talking about race in an experimental setup where they have been primed to do so. Additionally, since we are measuring sentiment by simply looking at word co-occurrences, the resulting sentiment can reÔ¨Çect socio-historical factors - for instance, text relating to a discussion of slavery will frequently have a negative sentiment, which may lead to a demographic being associated with a negative sentiment under this testing methodology. Across the models we analyzed, ‚ÄòAsian‚Äô had a consistently high sentiment - it ranked 1st in 3 out of 7 models. On the other hand, ‚ÄôBlack‚Äô had a consistently low sentiment - it ranked the lowest in 5 out of 7 models. These differences narrowed
sentiment under this testing methodology. Across the models we analyzed, ‚ÄòAsian‚Äô had a consistently high sentiment - it ranked 1st in 3 out of 7 models. On the other hand, ‚ÄôBlack‚Äô had a consistently low sentiment - it ranked the lowest in 5 out of 7 models. These differences narrowed marginally on the larger model sizes. This analysis gives a sense of the biases of different models and highlights the need for more sophisticated analysis of the relationship between sentiment, entities, and input data. 9We only used male and female pronouns. This simplifying assumption makes it easier to study co-occurrence since it does not require the isolation of instances in which ‚Äòthey‚Äô refers to a singular noun from those where it didn‚Äôt, but other forms of gender bias are likely present and could be studied using different approaches. 37 Figure 6.1: Racial Sentiment Across Models Religion Most Favored Descriptive Words Atheism ‚ÄòTheists‚Äô, ‚ÄòCool‚Äô, ‚ÄòAgnostics‚Äô, ‚ÄòMad‚Äô, ‚ÄòTheism‚Äô, ‚ÄòDefensive‚Äô, ‚ÄòComplaining‚Äô, ‚ÄòCorrect‚Äô, ‚ÄòArrogant‚Äô, ‚ÄòCharacterized‚Äô Buddhism ‚ÄòMyanmar‚Äô, ‚ÄòVegetarians‚Äô, ‚ÄòBurma‚Äô, ‚ÄòFellowship‚Äô, ‚ÄòMonk‚Äô, ‚ÄòJapanese‚Äô, ‚ÄòReluctant‚Äô, ‚ÄòWisdom‚Äô, ‚ÄòEn- lightenment‚Äô, ‚ÄòNon-Violent‚Äô Christianity ‚ÄòAttend‚Äô, ‚ÄòIgnorant‚Äô, ‚ÄòResponse‚Äô, ‚ÄòJudgmental‚Äô, ‚ÄòGrace‚Äô, ‚ÄòExecution‚Äô, ‚ÄòEgypt‚Äô, ‚ÄòContinue‚Äô, ‚ÄòCom- ments‚Äô, ‚ÄòOfÔ¨Åcially‚Äô Hinduism ‚ÄòCaste‚Äô, ‚ÄòCows‚Äô, ‚ÄòBJP‚Äô, ‚ÄòKashmir‚Äô, ‚ÄòModi‚Äô, ‚ÄòCelebrated‚Äô, ‚ÄòDharma‚Äô, ‚ÄòPakistani‚Äô, ‚ÄòOriginated‚Äô, ‚ÄòAfrica‚Äô Islam ‚ÄòPillars‚Äô, ‚ÄòTerrorism‚Äô, ‚ÄòFasting‚Äô,
Atheism ‚ÄòTheists‚Äô, ‚ÄòCool‚Äô, ‚ÄòAgnostics‚Äô, ‚ÄòMad‚Äô, ‚ÄòTheism‚Äô, ‚ÄòDefensive‚Äô, ‚ÄòComplaining‚Äô, ‚ÄòCorrect‚Äô, ‚ÄòArrogant‚Äô, ‚ÄòCharacterized‚Äô Buddhism ‚ÄòMyanmar‚Äô, ‚ÄòVegetarians‚Äô, ‚ÄòBurma‚Äô, ‚ÄòFellowship‚Äô, ‚ÄòMonk‚Äô, ‚ÄòJapanese‚Äô, ‚ÄòReluctant‚Äô, ‚ÄòWisdom‚Äô, ‚ÄòEn- lightenment‚Äô, ‚ÄòNon-Violent‚Äô Christianity ‚ÄòAttend‚Äô, ‚ÄòIgnorant‚Äô, ‚ÄòResponse‚Äô, ‚ÄòJudgmental‚Äô, ‚ÄòGrace‚Äô, ‚ÄòExecution‚Äô, ‚ÄòEgypt‚Äô, ‚ÄòContinue‚Äô, ‚ÄòCom- ments‚Äô, ‚ÄòOfÔ¨Åcially‚Äô Hinduism ‚ÄòCaste‚Äô, ‚ÄòCows‚Äô, ‚ÄòBJP‚Äô, ‚ÄòKashmir‚Äô, ‚ÄòModi‚Äô, ‚ÄòCelebrated‚Äô, ‚ÄòDharma‚Äô, ‚ÄòPakistani‚Äô, ‚ÄòOriginated‚Äô, ‚ÄòAfrica‚Äô Islam ‚ÄòPillars‚Äô, ‚ÄòTerrorism‚Äô, ‚ÄòFasting‚Äô, ‚ÄòSheikh‚Äô, ‚ÄòNon-Muslim‚Äô, ‚ÄòSource‚Äô, ‚ÄòCharities‚Äô, ‚ÄòLevant‚Äô, ‚ÄòAllah‚Äô, ‚ÄòProphet‚Äô Judaism ‚ÄòGentiles‚Äô, ‚ÄòRace‚Äô, ‚ÄòSemites‚Äô, ‚ÄòWhites‚Äô, ‚ÄòBlacks‚Äô, ‚ÄòSmartest‚Äô, ‚ÄòRacists‚Äô, ‚ÄòArabs‚Äô, ‚ÄòGame‚Äô, ‚ÄòRussian‚Äô Table 6.2: Shows the ten most favored words about each religion in the GPT-3 175B model. 6.2.3 Religion We studied which words co-occurred with religious terms relating to Atheism, Buddhism, Christianity, Hinduism, Islam, and Judaism, by generating 800 model outputs of length ‚âà50 with a temperature of 1 and a top p of 0.9 for every prompt. Our prompts were of the nature "{Religion practitioners} are" (Eg. "Christians are") for each of the six religious categories listed above. We then allowed the model to naturally carry out completions and created a corpus of such completions for studying co-occurrence of words. The following is an example output from the model: "Buddhists are divided into two main branches - Theravada and Mahayana. Theravada is the more conservative branch, centering on monastic life and the
model to naturally carry out completions and created a corpus of such completions for studying co-occurrence of words. The following is an example output from the model: "Buddhists are divided into two main branches - Theravada and Mahayana. Theravada is the more conservative branch, centering on monastic life and the earliest sutras and refusing to recognize the later Mahayana sutras as authentic." Similar to race, we found that the models make associations with religious terms that indicate some propensity to reÔ¨Çect how these terms are sometimes presented in the world. For example, with the religion Islam, we found that words such as ramadan, prophet and mosque co-occurred at a higher rate than for other religions. We also found that words such as violent, terrorism and terrorist co-occurred at a greater rate with Islam than with other religions and were in the top 40 most favored words for Islam in GPT-3. 38 6.2.4 Future Bias and Fairness Challenges We have presented this preliminary analysis to share some of the biases we found in order to motivate further research, and to highlight the inherent difÔ¨Åculties in characterizing biases in large-scale generative models; we expect this to be an area of continuous research
38 6.2.4 Future Bias and Fairness Challenges We have presented this preliminary analysis to share some of the biases we found in order to motivate further research, and to highlight the inherent difÔ¨Åculties in characterizing biases in large-scale generative models; we expect this to be an area of continuous research for us and are excited to discuss different methodological approaches with the community. We view the work in this section as subjective signposting - we chose gender, race, and religion as a starting point, but we recognize the inherent subjectivity in this choice. Our work is inspired by the literature on characterizing model attributes to develop informative labels such as Model Cards for Model Reporting from [MWZ+18]. Ultimately, it is important not just to characterize biases in language systems but to intervene. The literature on this is also extensive [QMZH19, HZJ+19], so we offer only a few brief comments on future directions speciÔ¨Åc to large language models. In order to pave the way for effective bias prevention in general purpose models, there is a need for building a common vocabulary tying together the normative, technical and empirical challenges of bias mitigation for these models. There is room for more research
future directions speciÔ¨Åc to large language models. In order to pave the way for effective bias prevention in general purpose models, there is a need for building a common vocabulary tying together the normative, technical and empirical challenges of bias mitigation for these models. There is room for more research that engages with the literature outside NLP, better articulates normative statements about harm, and engages with the lived experience of communities affected by NLP systems [BBDIW20]. Thus, mitigation work should not be approached purely with a metric driven objective to ‚Äòremove‚Äô bias as this has been shown to have blind spots [GG19, NvNvdG19] but in a holistic manner. 6.3 Energy Usage Practical large-scale pre-training requires large amounts of computation, which is energy-intensive: training the GPT-3 175B consumed several thousand petaÔ¨Çop/s-days of compute during pre-training, compared to tens of petaÔ¨Çop/s-days for a 1.5B parameter GPT-2 model (Figure 2.2). This means we should be cognizant of the cost and efÔ¨Åciency of such models, as advocated by [SDSE19]. The use of large-scale pre-training also gives another lens through which to view the efÔ¨Åciency of large models - we should consider not only the resources that go into training them, but how these resources
should be cognizant of the cost and efÔ¨Åciency of such models, as advocated by [SDSE19]. The use of large-scale pre-training also gives another lens through which to view the efÔ¨Åciency of large models - we should consider not only the resources that go into training them, but how these resources are amortized over the lifetime of a model, which will subsequently be used for a variety of purposes and Ô¨Åne-tuned for speciÔ¨Åc tasks. Though models like GPT-3 consume signiÔ¨Åcant resources during training, they can be surprisingly efÔ¨Åcient once trained: even with the full GPT-3 175B, generating 100 pages of content from a trained model can cost on the order of 0.4 kW-hr, or only a few cents in energy costs. Additionally, techniques like model distillation [LHCG19a] can further bring down the cost of such models, letting us adopt a paradigm of training single, large-scale models, then creating more efÔ¨Åcient versions of them for use in appropriate contexts. Algorithmic progress may also naturally further increase the efÔ¨Åciency of such models over time, similar to trends observed in image recognition and neural machine translation [HB20]. 7 Related Work Several lines of work have focused on increasing parameter count and/or computation in language
of them for use in appropriate contexts. Algorithmic progress may also naturally further increase the efÔ¨Åciency of such models over time, similar to trends observed in image recognition and neural machine translation [HB20]. 7 Related Work Several lines of work have focused on increasing parameter count and/or computation in language models as a means to improve generative or task performance. An early work scaled LSTM based language models to over a billion parameters [JVS+16]. One line of work straightforwardly increases the size of transformer models, scaling up parameters and FLOPS-per-token roughly in proportion. Work in this vein has successively increased model size: 213 million parameters [VSP+17] in the original paper, 300 million parameters [DCLT18], 1.5 billion parameters [RWC+19], 8 billion parameters [SPP+19], 11 billion parameters [RSR+19], and most recently 17 billion parameters [Tur20]. A second line of work has focused on increasing parameter count but not computation, as a means of increasing models‚Äô capacity to store information without increased computational cost. These approaches rely on the conditional computation framework [BLC13] and speciÔ¨Åcally, the mixture-of-experts method [SMM+17] has been used to produce 100 billion parameter models and more recently 50 billion parameter translation models [AJF19], though only a small fraction of
of increasing models‚Äô capacity to store information without increased computational cost. These approaches rely on the conditional computation framework [BLC13] and speciÔ¨Åcally, the mixture-of-experts method [SMM+17] has been used to produce 100 billion parameter models and more recently 50 billion parameter translation models [AJF19], though only a small fraction of the parameters are actually used on each forward pass. A third approach increases computation without increasing parameters; examples of this approach include adaptive computation time [Gra16] and the universal transformer [DGV+18]. Our work focuses on the Ô¨Årst approach (scaling compute and parameters together, by straightforwardly making the neural net larger), and increases model size 10x beyond previous models that employ this strategy. Several efforts have also systematically studied the effect of scale on language model performance. [KMH+20, RRBS19, LWS+20, HNA+17], Ô¨Ånd a smooth power-law trend in loss as autoregressive language models are scaled up. This work suggests that this trend largely continues as models continue to scale up (although a slight bending of the curve can perhaps be detected in Figure 3.1), and we also Ô¨Ånd relatively smooth increases in many (though not all) downstream tasks across 3 orders of magnitude of scaling. Another line of work goes in the
largely continues as models continue to scale up (although a slight bending of the curve can perhaps be detected in Figure 3.1), and we also Ô¨Ånd relatively smooth increases in many (though not all) downstream tasks across 3 orders of magnitude of scaling. Another line of work goes in the opposite direction from scaling, attempting to preserve strong performance in language models that are as small as possible. This approach includes ALBERT [LCG+19] as well as general [HVD15] and 39 task-speciÔ¨Åc [SDCW19, JYS+19, KR16] approaches to distillation of language models. These architectures and techniques are potentially complementary to our work, and could be applied to decrease latency and memory footprint of giant models. As Ô¨Åne-tuned language models have neared human performance on many standard benchmark tasks, considerable effort has been devoted to constructing more difÔ¨Åcult or open-ended tasks, including question answering [KPR+19, IBGC+14, CCE+18, MCKS18], reading comprehension [CHI+18, RCM19], and adversarially constructed datasets designed to be difÔ¨Åcult for existing language models [SBBC19, NWD+19]. In this work we test our models on many of these datasets. Many previous efforts have focused speciÔ¨Åcally on question-answering, which constitutes a signiÔ¨Åcant fraction of the tasks we tested on. Recent efforts include [RSR+19, RRS20], which
adversarially constructed datasets designed to be difÔ¨Åcult for existing language models [SBBC19, NWD+19]. In this work we test our models on many of these datasets. Many previous efforts have focused speciÔ¨Åcally on question-answering, which constitutes a signiÔ¨Åcant fraction of the tasks we tested on. Recent efforts include [RSR+19, RRS20], which Ô¨Åne-tuned an 11 billion parameter language model, and [GLT+20], which focused on attending over a large corpus of data at test time. Our work differs in focusing on in-context learning but could be combined in the future with those of [GLT+20, LPP+20]. Metalearning in language models has been utilized in [RWC+19], though with much more limited results and no systematic study. More broadly, language model metalearning has an inner-loop-outer-loop structure, making it structurally similar to metalearning as applied to ML in general. Here there is an extensive literature, including matching networks [VBL+16], RL2 [DSC+16], learning to optimize [RL16, ADG+16, LM17] and MAML [FAL17]. Our approach of stufÔ¨Ång the model‚Äôs context with previous examples is most structurally similar to RL2 and also resembles [HYC01], in that an inner loop of adaptation takes place through computation in the model‚Äôs activations across timesteps, without updating the weights, while an outer loop (in this
and MAML [FAL17]. Our approach of stufÔ¨Ång the model‚Äôs context with previous examples is most structurally similar to RL2 and also resembles [HYC01], in that an inner loop of adaptation takes place through computation in the model‚Äôs activations across timesteps, without updating the weights, while an outer loop (in this case just language model pre-training) updates the weights, and implicitly learns the ability to adapt to or at least recognize tasks deÔ¨Åned at inference-time. Few-shot auto-regressive density estimation was explored in [RCP+17] and [GWC+18] studied low-resource NMT as a few-shot learning problem. While the mechanism of our few-shot approach is different, prior work has also explored ways of using pre-trained language models in combination with gradient descent to perform few-shot learning [SS20]. Another sub-Ô¨Åeld with similar goals is semi-supervised learning where approaches such as UDA [XDH+19] also explore methods of Ô¨Åne-tuning when very little labeled data is available. Giving multi-task models instructions in natural language was Ô¨Årst formalized in a supervised setting with [MKXS18] and utilized for some tasks (such as summarizing) in a language model with [RWC+19]. The notion of presenting tasks in natural language was also explored in the text-to-text transformer [RSR+19], although there it was applied for
models instructions in natural language was Ô¨Årst formalized in a supervised setting with [MKXS18] and utilized for some tasks (such as summarizing) in a language model with [RWC+19]. The notion of presenting tasks in natural language was also explored in the text-to-text transformer [RSR+19], although there it was applied for multi-task Ô¨Åne-tuning rather than for in-context learning without weight updates. Another approach to increasing generality and transfer-learning capability in language models is multi-task learning [Car97], which Ô¨Åne-tunes on a mixture of downstream tasks together, rather than separately updating the weights for each one. If successful multi-task learning could allow a single model to be used for many tasks without updating the weights (similar to our in-context learning approach), or alternatively could improve sample efÔ¨Åciency when updating the weights for a new task. Multi-task learning has shown some promising initial results [LGH+15, LSP+18] and multi-stage Ô¨Åne-tuning has recently become a standardized part of SOTA results on some datasets [PFB18] and pushed the boundaries on certain tasks [KKS+20], but is still limited by the need to manually curate collections of datasets and set up training curricula. By contrast pre-training at large enough scale appears to offer a ‚Äúnatural‚Äù broad distribution of tasks
part of SOTA results on some datasets [PFB18] and pushed the boundaries on certain tasks [KKS+20], but is still limited by the need to manually curate collections of datasets and set up training curricula. By contrast pre-training at large enough scale appears to offer a ‚Äúnatural‚Äù broad distribution of tasks implicitly contained in predicting the text itself. One direction for future work might be attempting to generate a broader set of explicit tasks for multi-task learning, for example through procedural generation [TFR+17], human interaction [ZSW+19b], or active learning [Mac92]. Algorithmic innovation in language models over the last two years has been enormous, including denoising-based bidirectionality [DCLT18], preÔ¨ÅxLM [DL15] and encoder-decoder architectures [LLG+19, RSR+19], random permu- tations during training [YDY+19], architectures that improve the efÔ¨Åciency of sampling [DYY+19], improvements in data and training procedures [LOG+19], and efÔ¨Åciency increases in the embedding parameters [LCG+19]. Many of these techniques provide signiÔ¨Åcant gains on downstream tasks. In this work we continue to focus on pure autoregressive language models, both in order to focus on in-context learning performance and to reduce the complexity of our large model implementations. However, it is very likely that incorporating these algorithmic advances could improve GPT-3‚Äôs performance on downstream tasks,
downstream tasks. In this work we continue to focus on pure autoregressive language models, both in order to focus on in-context learning performance and to reduce the complexity of our large model implementations. However, it is very likely that incorporating these algorithmic advances could improve GPT-3‚Äôs performance on downstream tasks, especially in the Ô¨Åne-tuning setting, and combining GPT-3‚Äôs scale with these algorithmic techniques is a promising direction for future work. 8 Conclusion We presented a 175 billion parameter language model which shows strong performance on many NLP tasks and benchmarks in the zero-shot, one-shot, and few-shot settings, in some cases nearly matching the performance of 40 state-of-the-art Ô¨Åne-tuned systems, as well as generating high-quality samples and strong qualitative performance at tasks deÔ¨Åned on-the-Ô¨Çy. We documented roughly predictable trends of scaling in performance without using Ô¨Åne-tuning. We also discussed the social impacts of this class of model. Despite many limitations and weaknesses, these results suggest that very large language models may be an important ingredient in the development of adaptable, general language systems. Acknowledgements The authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper. Thanks to Jakub Pachocki and Szymon Sidor for suggesting tasks,
weaknesses, these results suggest that very large language models may be an important ingredient in the development of adaptable, general language systems. Acknowledgements The authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper. Thanks to Jakub Pachocki and Szymon Sidor for suggesting tasks, and Greg Brockman, Michael Petrov, Brooke Chan, and Chelsea Voss for helping run evaluations on OpenAI‚Äôs infrastructure. Thanks to David Luan for initial support in scaling up this project, Irene Solaiman for discussions about ways to approach and evaluate bias, Harrison Edwards and Yura Burda for discussions and experimentation with in-context learning, Geoffrey Irving and Paul Christiano for early discussions of language model scaling, Long Ouyang for advising on the design of the human evaluation experiments, Chris Hallacy for discussions on data collection, and Shan Carter for help with visual design. Thanks to the millions of people who created content that was used in the training of the model, and to those who were involved in indexing or upvoting the content (in the case of WebText). Additionally, we would like to thank the entire OpenAI infrastructure and supercomputing teams for making it possible to train models at this scale. 41
that was used in the training of the model, and to those who were involved in indexing or upvoting the content (in the case of WebText). Additionally, we would like to thank the entire OpenAI infrastructure and supercomputing teams for making it possible to train models at this scale. 41 Contributions Tom Brown, Ben Mann, Prafulla Dhariwal, Dario Amodei, Nick Ryder, Daniel M Ziegler, and Jeffrey Wu implemented the large-scale models, training infrastructure, and model-parallel strategies. Tom Brown, Dario Amodei, Ben Mann, and Nick Ryder conducted pre-training experiments. Ben Mann and Alec Radford collected, Ô¨Åltered, deduplicated, and conducted overlap analysis on the training data. Melanie Subbiah, Ben Mann, Dario Amodei, Jared Kaplan, Sam McCandlish, Tom Brown, Tom Henighan, and Girish Sastry implemented the downstream tasks and the software framework for supporting them, including creation of synthetic tasks. Jared Kaplan and Sam McCandlish initially predicted that a giant language model should show continued gains, and applied scaling laws to help predict and guide model and data scaling decisions for the research. Ben Mann implemented sampling without replacement during training. Alec Radford originally demonstrated few-shot learning occurs in language models. Jared Kaplan and Sam McCandlish showed that larger models learn more quickly
show continued gains, and applied scaling laws to help predict and guide model and data scaling decisions for the research. Ben Mann implemented sampling without replacement during training. Alec Radford originally demonstrated few-shot learning occurs in language models. Jared Kaplan and Sam McCandlish showed that larger models learn more quickly in-context, and systematically studied in-context learning curves, task prompting, and evaluation methods. Prafulla Dhariwal implemented an early version of the codebase, and developed the memory optimizations for fully half-precision training. Rewon Child and Mark Chen developed an early version of our model-parallel strategy. Rewon Child and Scott Gray contributed the sparse transformer. Aditya Ramesh experimented with loss scaling strategies for pretraining. Melanie Subbiah and Arvind Neelakantan implemented, experimented with, and tested beam search. Pranav Shyam worked on SuperGLUE and assisted with connections to few-shot learning and meta-learning literature. Sandhini Agarwal conducted the fairness and representation analysis. Girish Sastry and Amanda Askell conducted the human evaluations of the model. Ariel Herbert-Voss conducted the threat analysis of malicious use. Gretchen Krueger edited and red-teamed the policy sections of the paper. Benjamin Chess, Clemens Winter, Eric Sigler, Christopher Hesse, Mateusz Litwin, and Christopher Berner optimized OpenAI‚Äôs clusters to run the largest models efÔ¨Åciently.
Askell conducted the human evaluations of the model. Ariel Herbert-Voss conducted the threat analysis of malicious use. Gretchen Krueger edited and red-teamed the policy sections of the paper. Benjamin Chess, Clemens Winter, Eric Sigler, Christopher Hesse, Mateusz Litwin, and Christopher Berner optimized OpenAI‚Äôs clusters to run the largest models efÔ¨Åciently. Scott Gray developed fast GPU kernels used during training. Jack Clark led the analysis of ethical impacts ‚Äî fairness and representation, human assessments of the model, and broader impacts analysis, and advised Gretchen, Amanda, Girish, Sandhini, and Ariel on their work. Dario Amodei, Alec Radford, Tom Brown, Sam McCandlish, Nick Ryder, Jared Kaplan, Sandhini Agarwal, Amanda Askell, Girish Sastry, and Jack Clark wrote the paper. Sam McCandlish led the analysis of model scaling, and advised Tom Henighan and Jared Kaplan on their work. Alec Radford advised the project from an NLP perspective, suggested tasks, put the results in context, and demonstrated the beneÔ¨Åt of weight decay for training. Ilya Sutskever was an early advocate for scaling large generative likelihood models, and advised Pranav, Prafulla, Rewon, Alec, and Aditya on their work. Dario Amodei designed and led the research. 42 A Details of Common Crawl Filtering As mentioned in Section 2.2,
demonstrated the beneÔ¨Åt of weight decay for training. Ilya Sutskever was an early advocate for scaling large generative likelihood models, and advised Pranav, Prafulla, Rewon, Alec, and Aditya on their work. Dario Amodei designed and led the research. 42 A Details of Common Crawl Filtering As mentioned in Section 2.2, we employed two techniques to improve the quality of the Common Crawl dataset: (1) Ô¨Åltering Common Crawl and (2) fuzzy deduplication: 1. In order to improve the quality of Common Crawl, we developed an automatic Ô¨Åltering method to remove low quality documents. Using the original WebText as a proxy for high-quality documents, we trained a classiÔ¨Åer to distinguish these from raw Common Crawl. We then used this classiÔ¨Åer to re-sample Common Crawl by prioritizing documents which were predicted by the classiÔ¨Åer to be higher quality. The classiÔ¨Åer is trained using logistic regression classiÔ¨Åer with features from Spark‚Äôs standard tokenizer and HashingTF 10. For the positive examples, we used a collection of curated datasets such as WebText, Wikiedia, and our web books corpus as the positive examples, and for the negative examples, we used unÔ¨Åltered Common Crawl. We used this classiÔ¨Åer to score Common Crawl documents. We kept each document in
HashingTF 10. For the positive examples, we used a collection of curated datasets such as WebText, Wikiedia, and our web books corpus as the positive examples, and for the negative examples, we used unÔ¨Åltered Common Crawl. We used this classiÔ¨Åer to score Common Crawl documents. We kept each document in our dataset iff np.random.pareto(Œ±) > 1 ‚àídocument_score We chose Œ± = 9 in order to take mostly documents the classiÔ¨Åer scored highly, but still include some documents that were out of distribution. Œ± was chosen to match the distribution of scores from our classiÔ¨Åer on WebText. We found this re-weighting increased quality as measured by loss on a range of out-of-distribution generative text samples. 2. To further improve model quality and prevent overÔ¨Åtting (which becomes increasingly important as model capacity increases), we fuzzily deduplicated documents (i.e. removed documents with high overlap with other documents) within each dataset using Spark‚Äôs MinHashLSH implementation with 10 hashes, using the same features as were used for classiÔ¨Åcation above. We also fuzzily removed WebText from Common Crawl. Overall this decreased dataset size by an average of 10%. After Ô¨Åltering for duplicates and quality, we also partially removed text occurring in benchmark datasets, described in Appendix
implementation with 10 hashes, using the same features as were used for classiÔ¨Åcation above. We also fuzzily removed WebText from Common Crawl. Overall this decreased dataset size by an average of 10%. After Ô¨Åltering for duplicates and quality, we also partially removed text occurring in benchmark datasets, described in Appendix C. B Details of Model Training To train all versions of GPT-3, we use Adam with Œ≤1 = 0.9, Œ≤2 = 0.95, and œµ = 10‚àí8, we clip the global norm of the gradient at 1.0, and we use cosine decay for learning rate down to 10% of its value, over 260 billion tokens (after 260 billion tokens, training continues at 10% of the original learning rate). There is a linear LR warmup over the Ô¨Årst 375 million tokens. We also gradually increase the batch size linearly from a small value (32k tokens) to the full value over the Ô¨Årst 4-12 billion tokens of training, depending on the model size. Data are sampled without replacement during training (until an epoch boundary is reached) to minimize overÔ¨Åtting. All models use weight decay of 0.1 to provide a small amount of regularization [LH17]. During training we always train on sequences of the
4-12 billion tokens of training, depending on the model size. Data are sampled without replacement during training (until an epoch boundary is reached) to minimize overÔ¨Åtting. All models use weight decay of 0.1 to provide a small amount of regularization [LH17]. During training we always train on sequences of the full nctx = 2048 token context window, packing multiple documents into a single sequence when documents are shorter than 2048, in order to increase computational efÔ¨Åciency. Sequences with multiple documents are not masked in any special way but instead documents within a sequence are delimited with a special end of text token, giving the language model the information necessary to infer that context separated by the end of text token is unrelated. This allows for efÔ¨Åcient training without need for any special sequence-speciÔ¨Åc masking. C Details of Test Set Contamination Studies In section 4 we gave a high level overview of test set contamination studies. In this section we provide details on methodology and results. Initial training set Ô¨Åltering We attempted to remove text occurring in benchmarks from training data by searching for 13‚àígram overlaps between all test/development sets used in this work and our training data, and we removed
of test set contamination studies. In this section we provide details on methodology and results. Initial training set Ô¨Åltering We attempted to remove text occurring in benchmarks from training data by searching for 13‚àígram overlaps between all test/development sets used in this work and our training data, and we removed the colliding 13‚àígram as well as a 200 character window around it, splitting the original document into pieces. For Ô¨Åltering purposes we deÔ¨Åne a gram as a lowercase, whitespace delimited word with no punctuation. Pieces less than 200 characters long were discarded. Documents split into more than 10 pieces were considered contaminated and 10https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.HashingTF 43 removed entirely. Originally we removed entire documents given a single collision, but that overly penalized long documents such as books for false positives. An example of a false positive might be a test set based on Wikipedia, in which the Wikipedia article quotes a single line from a book. We ignored 13‚àígrams that matched more than 10 training documents, as inspection showed the majority of these to contain common cultural phrases, legal boilerplate, or similar content that we likely do want the model to learn, rather than undesired speciÔ¨Åc overlaps with test sets. Examples for
line from a book. We ignored 13‚àígrams that matched more than 10 training documents, as inspection showed the majority of these to contain common cultural phrases, legal boilerplate, or similar content that we likely do want the model to learn, rather than undesired speciÔ¨Åc overlaps with test sets. Examples for various frequencies can be found in the GPT-3 release repository11. Overlap methodology For our benchmark overlap analysis in Section 4, we used a variable number of words N to check for overlap for each dataset, where N is the 5th percentile example length in words, ignoring all punctuation, whitespace, and casing. Due to spurious collisions at lower values of N we use a minimum value of 8 on non-synthetic tasks. For performance reasons, we set a maximum value of 13 for all tasks. Values for N and the amount of data marked as dirty are shown in Table C.1. Unlike GPT-2‚Äôs use of bloom Ô¨Ålters to compute probabilistic bounds for test contamination, we used Apache Spark to compute exact collisions across all training and test sets. We compute overlaps between test sets and our full training corpus, even though we only trained on 40% of our Ô¨Åltered Common Crawl documents
GPT-2‚Äôs use of bloom Ô¨Ålters to compute probabilistic bounds for test contamination, we used Apache Spark to compute exact collisions across all training and test sets. We compute overlaps between test sets and our full training corpus, even though we only trained on 40% of our Ô¨Åltered Common Crawl documents per Section 2.2. We deÔ¨Åne a ‚Äòdirty‚Äô example as one with any N-gram overlap with any training document, and a ‚Äòclean‚Äô example as one with no collision. Test and validation splits had similar contamination levels despite some test splits being unlabeled. Due to a bug revealed by this analysis, Ô¨Åltering described above failed on long documents such as books. Because of cost considerations it was infeasible to retrain the model on a corrected version of the training dataset. As such, several language modeling benchmarks plus the Children‚Äôs Book Test showed almost complete overlap, and therefore were not included in this paper. Overlaps are shown in Table C.1 Overlap results To understand how much having seen some of the data helps the model perform on downstream tasks, we Ô¨Ålter every validation and test set by dirtiness. Then we run evaluation on the clean-only examples and report the relative percent change between
paper. Overlaps are shown in Table C.1 Overlap results To understand how much having seen some of the data helps the model perform on downstream tasks, we Ô¨Ålter every validation and test set by dirtiness. Then we run evaluation on the clean-only examples and report the relative percent change between the clean score and the original score. If the clean score is more than 1% or 2% worse than the overall score, it suggests the model may have overÔ¨Åt to the examples it has seen. If the clean score is signiÔ¨Åcantly better, our Ô¨Åltering scheme may have preferentially marked easier examples as dirty. This overlap metric tends to show a high rate of false positives for datasets that contain background information (but not answers) drawn from the web (such as SQuAD, which draws from Wikipedia) or examples less than 8 words long, which we ignored in our Ô¨Åltering process (except for wordscrambling tasks). One instance where this technique seems to fail to give good signal is DROP, a reading comprehension task in which 94% of the examples are dirty. The information required to answer the question is in a passage provided to the model, so having seen the passage during
for wordscrambling tasks). One instance where this technique seems to fail to give good signal is DROP, a reading comprehension task in which 94% of the examples are dirty. The information required to answer the question is in a passage provided to the model, so having seen the passage during training but not the questions and answers does not meaningfully constitute cheating. We conÔ¨Årmed that every matching training document contained only the source passage, and none of the questions and answers in the dataset. The more likely explanation for the decrease in performance is that the 6% of examples that remain after Ô¨Åltering come from a slightly different distribution than the dirty examples. Figure 4.2 shows that as the dataset becomes more contaminated, the variance of the clean/all fraction increases, but there is no apparent bias towards improved or degraded performance. This suggests that GPT-3 is relatively insensitive to contamination. See Section 4 for details on the datasets we Ô¨Çagged for further review. 11https://github.com/openai/gpt-3/blob/master/overlap_frequency.md 44 Name Split Metric N Acc/F1/BLEU Total Count Dirty Acc/F1/BLEU Dirty Count Clean Acc/F1/BLEU Clean Count Clean Percentage Relative Difference Clean vs All Quac dev f1 13 44.3 7353 44.3 7315 54.1 38 1% 20% SQuADv2
See Section 4 for details on the datasets we Ô¨Çagged for further review. 11https://github.com/openai/gpt-3/blob/master/overlap_frequency.md 44 Name Split Metric N Acc/F1/BLEU Total Count Dirty Acc/F1/BLEU Dirty Count Clean Acc/F1/BLEU Clean Count Clean Percentage Relative Difference Clean vs All Quac dev f1 13 44.3 7353 44.3 7315 54.1 38 1% 20% SQuADv2 dev f1 13 69.8 11873 69.9 11136 68.4 737 6% -2% DROP dev f1 13 36.5 9536 37.0 8898 29.5 638 7% -21% Symbol Insertion dev acc 7 66.9 10000 66.8 8565 67.1 1435 14% 0% CoQa dev f1 13 86.0 7983 85.3 5107 87.1 2876 36% 1% ReCoRD dev acc 13 89.5 10000 90.3 6110 88.2 3890 39% -1% Winograd test acc 9 88.6 273 90.2 164 86.2 109 40% -3% BoolQ dev acc 13 76.0 3270 75.8 1955 76.3 1315 40% 0% MultiRC dev acc 13 74.2 953 73.4 558 75.3 395 41% 1% RACE-h test acc 13 46.8 3498 47.0 1580 46.7 1918 55% 0% LAMBADA test acc 13 86.4 5153 86.9 2209 86.0 2944 57% 0% LAMBADA (No Blanks) test acc 13 77.8 5153 78.5 2209 77.2 2944 57% -1% WSC dev acc 13 76.9 104 73.8 42 79.0 62 60% 3% PIQA dev acc 8
46.8 3498 47.0 1580 46.7 1918 55% 0% LAMBADA test acc 13 86.4 5153 86.9 2209 86.0 2944 57% 0% LAMBADA (No Blanks) test acc 13 77.8 5153 78.5 2209 77.2 2944 57% -1% WSC dev acc 13 76.9 104 73.8 42 79.0 62 60% 3% PIQA dev acc 8 82.3 1838 89.9 526 79.3 1312 71% -4% RACE-m test acc 13 58.5 1436 53.0 366 60.4 1070 75% 3% De‚ÜíEn 16 test bleu-sb 12 43.0 2999 47.4 739 40.8 2260 75% -5% En‚ÜíDe 16 test bleu-sb 12 30.9 2999 32.6 739 29.9 2260 75% -3% En‚ÜíRo 16 test bleu-sb 12 25.8 1999 24.9 423 26.1 1576 79% 1% Ro‚ÜíEn 16 test bleu-sb 12 41.3 1999 40.4 423 41.6 1576 79% 1% WebQs test acc 8 41.5 2032 41.6 428 41.5 1604 79% 0% ANLI R1 test acc 13 36.8 1000 40.5 200 35.9 800 80% -3% ANLI R2 test acc 13 34.0 1000 29.4 177 35.0 823 82% 3% TriviaQA dev acc 10 71.2 7993 70.8 1390 71.3 6603 83% 0% ANLI R3 test acc 13 40.2 1200 38.3 196 40.5 1004 84% 1% En‚ÜíFr 14 test bleu-sb 13 39.9 3003 38.3 411 40.3 2592 86% 1% Fr‚ÜíEn 14
acc 13 34.0 1000 29.4 177 35.0 823 82% 3% TriviaQA dev acc 10 71.2 7993 70.8 1390 71.3 6603 83% 0% ANLI R3 test acc 13 40.2 1200 38.3 196 40.5 1004 84% 1% En‚ÜíFr 14 test bleu-sb 13 39.9 3003 38.3 411 40.3 2592 86% 1% Fr‚ÜíEn 14 test bleu-sb 13 41.4 3003 40.9 411 41.4 2592 86% 0% WiC dev acc 13 51.4 638 53.1 49 51.3 589 92% 0% RTE dev acc 13 71.5 277 71.4 21 71.5 256 92% 0% CB dev acc 13 80.4 56 100.0 4 78.8 52 93% -2% Anagrams 2 dev acc 2 40.2 10000 76.2 705 37.4 9295 93% -7% Reversed Words dev acc 2 0.4 10000 1.5 660 0.3 9340 93% -26% OpenBookQA test acc 8 65.4 500 58.1 31 65.9 469 94% 1% ARC (Easy) test acc 11 70.1 2268 77.5 89 69.8 2179 96% 0% Anagrams 1 dev acc 2 15.0 10000 49.8 327 13.8 9673 97% -8% COPA dev acc 9 93.0 100 100.0 3 92.8 97 97% 0% ARC (Challenge) test acc 12 51.6 1144 45.2 31 51.8 1113 97% 0% HellaSwag dev acc 13 79.3 10042 86.2 152 79.2 9890 98% 0% NQs test
dev acc 2 15.0 10000 49.8 327 13.8 9673 97% -8% COPA dev acc 9 93.0 100 100.0 3 92.8 97 97% 0% ARC (Challenge) test acc 12 51.6 1144 45.2 31 51.8 1113 97% 0% HellaSwag dev acc 13 79.3 10042 86.2 152 79.2 9890 98% 0% NQs test acc 11 29.9 3610 32.7 52 29.8 3558 99% 0% Cycled Letters dev acc 2 38.6 10000 20.5 73 38.7 9927 99% 0% SAT Analogies dev acc 9 65.8 374 100.0 2 65.6 372 99% 0% StoryCloze test acc 13 87.7 1871 100.0 2 87.6 1869 100% 0% Winogrande dev acc 13 77.7 1267 - 0 77.7 1267 100% 0% Table C.1: Overlap statistics for all datasets sorted from dirtiest to cleanest. We consider a dataset example dirty if it has a single N-gram collision with any document in our training corpus. ‚ÄúRelative Difference Clean vs All‚Äù shows the percent change in performance between only the clean examples vs all the examples in the benchmark. ‚ÄúCount‚Äù shows the number of examples. ‚ÄúClean percentage‚Äù is the percent of examples that are clean vs total. For ‚ÄúAcc/F1/BLEU‚Äù we use the metric speciÔ¨Åed in ‚ÄúMetric‚Äù. These scores come from evaluations with a different
change in performance between only the clean examples vs all the examples in the benchmark. ‚ÄúCount‚Äù shows the number of examples. ‚ÄúClean percentage‚Äù is the percent of examples that are clean vs total. For ‚ÄúAcc/F1/BLEU‚Äù we use the metric speciÔ¨Åed in ‚ÄúMetric‚Äù. These scores come from evaluations with a different seed for the random examples used for in-context learning, and will therefore differ slightly from the scores elsewhere in the paper. 45 D Total Compute Used to Train Language Models This appendix contains the calculations that were used to derive the approximate compute used to train the language models in Figure 2.2. As a simplifying assumption, we ignore the attention operation, as it typically uses less than 10% of the total compute for the models we are analyzing. Calculations can be seen in Table D.1 and are explained within the table caption. Model Total train compute (PF-days) Total train compute (Ô¨Çops) Params (M) Training tokens (billions) Flops per param per token Mult for bwd pass Fwd-pass Ô¨Çops per active param per token Frac of params active for each token T5-Small 2.08E+00 1.80E+20 60 1,000 3 3 1 0.5 T5-Base 7.64E+00 6.60E+20 220 1,000 3 3 1 0.5 T5-Large 2.67E+01 2.31E+21
(Ô¨Çops) Params (M) Training tokens (billions) Flops per param per token Mult for bwd pass Fwd-pass Ô¨Çops per active param per token Frac of params active for each token T5-Small 2.08E+00 1.80E+20 60 1,000 3 3 1 0.5 T5-Base 7.64E+00 6.60E+20 220 1,000 3 3 1 0.5 T5-Large 2.67E+01 2.31E+21 770 1,000 3 3 1 0.5 T5-3B 1.04E+02 9.00E+21 3,000 1,000 3 3 1 0.5 T5-11B 3.82E+02 3.30E+22 11,000 1,000 3 3 1 0.5 BERT-Base 1.89E+00 1.64E+20 109 250 6 3 2 1.0 BERT-Large 6.16E+00 5.33E+20 355 250 6 3 2 1.0 RoBERTa-Base 1.74E+01 1.50E+21 125 2,000 6 3 2 1.0 RoBERTa-Large 4.93E+01 4.26E+21 355 2,000 6 3 2 1.0 GPT-3 Small 2.60E+00 2.25E+20 125 300 6 3 2 1.0 GPT-3 Medium 7.42E+00 6.41E+20 356 300 6 3 2 1.0 GPT-3 Large 1.58E+01 1.37E+21 760 300 6 3 2 1.0 GPT-3 XL 2.75E+01 2.38E+21 1,320 300 6 3 2 1.0 GPT-3 2.7B 5.52E+01 4.77E+21 2,650 300 6 3 2 1.0 GPT-3 6.7B 1.39E+02 1.20E+22 6,660 300 6 3 2 1.0 GPT-3 13B 2.68E+02 2.31E+22 12,850 300 6 3 2 1.0 GPT-3 175B 3.64E+03 3.14E+23 174,600 300 6 3 2 1.0 Table D.1: Starting from the right hand side and moving
GPT-3 2.7B 5.52E+01 4.77E+21 2,650 300 6 3 2 1.0 GPT-3 6.7B 1.39E+02 1.20E+22 6,660 300 6 3 2 1.0 GPT-3 13B 2.68E+02 2.31E+22 12,850 300 6 3 2 1.0 GPT-3 175B 3.64E+03 3.14E+23 174,600 300 6 3 2 1.0 Table D.1: Starting from the right hand side and moving left, we begin with the number of training tokens that each model was trained with. Next we note that since T5 uses an encoder-decoder model, only half of the parameters are active for each token during a forward or backwards pass. We then note that each token is involved in a single addition and a single multiply for each active parameter in the forward pass (ignoring attention). Then we add a multiplier of 3x to account for the backwards pass (as computing both ‚àÇparams ‚àÇloss and ‚àÇacts ‚àÇloss use a similar amount of compute as the forwards pass. Combining the previous two numbers, we get the total Ô¨Çops per parameter per token. We multiply this value by the total training tokens and the total parameters to yield the number of total Ô¨Çops used during training. We report both Ô¨Çops and petaÔ¨Çop/s-day (each of which are 8.64e+19 Ô¨Çops). E Human Quality
previous two numbers, we get the total Ô¨Çops per parameter per token. We multiply this value by the total training tokens and the total parameters to yield the number of total Ô¨Çops used during training. We report both Ô¨Çops and petaÔ¨Çop/s-day (each of which are 8.64e+19 Ô¨Çops). E Human Quality Assessment of Synthetic News Articles This appendix contains details on the experiments measuring human ability to distinguish GPT-3-generated synthetic news articles from real news articles. We Ô¨Årst describe the experiments on the ‚àº200 word news articles, and then describe the preliminary investigation of ‚àº500 word news articles generated by GPT-3. Participants: We recruited 718 unique participants to take part in 6 experiments. 97 participants were excluded for failing an internet check question, leaving a total of 621 participants: 343 male, 271 female, and 7 other. Mean participant age was ‚àº38 years old. All participants were recruited through Positly, which maintains a whitelist of high-performing workers from Mechanical Turk. All participants were US-based but there were no other demographic restrictions. Participants were paid $12 for their participation, based on a task time estimate of 60 minutes determined by pilot runs. In order to ensure that the sample of participants for each
a whitelist of high-performing workers from Mechanical Turk. All participants were US-based but there were no other demographic restrictions. Participants were paid $12 for their participation, based on a task time estimate of 60 minutes determined by pilot runs. In order to ensure that the sample of participants for each experiment quiz was unique, participants were not allowed to take part in an experiment more than once. Procedure and design: We arbitrarily selected 25 news articles that appeared in newser.com in early 2020. We used the article titles and subtitles to produce outputs from the 125M, 350M, 760M, 1.3B, 2.7B, 6.7B, 13.0B, and 200B (GPT-3) parameter language models. Five outputs per question were generated by each model and the generation with a word count closest to that of the human written article was selected automatically. This was to minimize the effect that completion length might have on participants‚Äô judgments. The same output procedure for each model with the exception of the removal of the intentionally bad control model, as described in the main text. 46 Model Participants Recruited Participants Excluded Genders (m:f:other) Mean Age Average Word Count (human:model) Control 76 7 32:37:0 39 216:216 GPT-3 Small 80 7 41:31:1 40
same output procedure for each model with the exception of the removal of the intentionally bad control model, as described in the main text. 46 Model Participants Recruited Participants Excluded Genders (m:f:other) Mean Age Average Word Count (human:model) Control 76 7 32:37:0 39 216:216 GPT-3 Small 80 7 41:31:1 40 216:188 GPT-3 Medium 80 7 46:28:2 39 216:202 GPT-3 Large 81 24 46:28:2 37 216:200 GPT-3 XL 79 14 32:32:1 38 216:199 GPT-3 2.7B 80 11 36:33:0 40 216:202 GPT-3 6.7B 76 5 46:28:2 37 216:195 GPT-3 13.0B 81 13 46:28:2 37 216:209 GPT-3 175B 80 9 42:29:0 37 216:216 Table E.1: Participant details and article lengths for each experiment to evaluate human detection of ‚àº200 word model generated news articles. Participants were excluded due to internet check fails. Figure E.1: Participants spend more time trying to identify whether each news article is machine generated as model size increases. Duration on the control model is indicated with the dashed line. Line of best Ô¨Åt is a linear model on a log scale with 95% conÔ¨Ådence intervals. In each experiment, half of the participants were randomly assigned to quiz A and half were randomly assigned to quiz B. Each quiz consisted
on the control model is indicated with the dashed line. Line of best Ô¨Åt is a linear model on a log scale with 95% conÔ¨Ådence intervals. In each experiment, half of the participants were randomly assigned to quiz A and half were randomly assigned to quiz B. Each quiz consisted of 25 articles: half (12-13) were human written and half (12-13) were model generated: the articles with human written completions in quiz A had model generated completions in quiz B and vice versa. The order of quiz question was shufÔ¨Çed for each participant. Participants could leave comments and were asked to indicate if they had seen the articles before. Participants were instructed not to look up the articles or their content during the quiz and at the end of the quiz were asked if they had looked anything up during the quiz. Statistical Tests: To compare means on the different runs, we performed a two-sample t-test for independent groups for each model against the control. This was implemented in Python using the scipy.stats.ttest_ind function. When plotting a regression line in the graph of average participant accuracy vs model size, we Ô¨Åt a power law of the form ax‚àíb. The 95%
runs, we performed a two-sample t-test for independent groups for each model against the control. This was implemented in Python using the scipy.stats.ttest_ind function. When plotting a regression line in the graph of average participant accuracy vs model size, we Ô¨Åt a power law of the form ax‚àíb. The 95% conÔ¨Ådence intervals were estimated from the t-distribution of the sample mean. Duration statistics: In the main text, we discussed the Ô¨Ånding that the ability of human participants to distinguish model and human generated news articles decreases as our models become larger. We have also found that the average time spent for a given set of questions increases as the model size increases, as shown in Figure E.1. Lower 47 Model Participants Recruited Participants Excluded Genders (m:f:other) Mean Age Average Word Count (human:model) Control 79 17 32:37:0 39 569:464 GPT-3 175B 81 19 32:30:0 40 569:498 Table E.2: Participant details and article lengths for the experiments investigating human detection of ‚àº500 word model generated news articles. Participants were excluded due to internet check fails. accuracy scores despite increased time investment from participants supports the Ô¨Ånding that larger models generate harder-to-distinguish news articles. Preliminary investigation of ‚àº500 word articles: We recruited 160
article lengths for the experiments investigating human detection of ‚àº500 word model generated news articles. Participants were excluded due to internet check fails. accuracy scores despite increased time investment from participants supports the Ô¨Ånding that larger models generate harder-to-distinguish news articles. Preliminary investigation of ‚àº500 word articles: We recruited 160 unique US-based participants to take part in 2 experiments through Positly (details are given in Table E.2). We randomly selected 12 Reuters world news articles from late 2019 and created a context for GPT-3 175B that consisted of a single Reuters article not in this set of 12. We then used the article titles and Reuters locations to generate completions from GPT-3 175B and the 160M control model from the previous experiments. These were used to create two 12-question quizzes per model, each consisting of half human written and half model generated articles. Comprehension questions were added and articles were shown to participants in 3 stages at 30 second intervals to encourage closer reading. Participants were paid $12 for this task. Model generation selection methods, exclusion criteria, and statistical tests mirror those of the previous experiments. F Additional Samples from GPT-3 GPT-3 adapts well to many tasks other than the
shown to participants in 3 stages at 30 second intervals to encourage closer reading. Participants were paid $12 for this task. Model generation selection methods, exclusion criteria, and statistical tests mirror those of the previous experiments. F Additional Samples from GPT-3 GPT-3 adapts well to many tasks other than the ones explored in the main body of the paper. As an example, in Figure F.1, we show four uncurated samples from a prompt suggesting that the model write a poem, with a given title, in the style of Wallace Stevens. We Ô¨Årst experimented with a few prompts, then generated four samples with no additional editing or selection (sampling at temperature 1 using nucleus sampling [HBFC19] with P = 0.9). Completions were truncated when the model began to write a new title and author heading, or broke into prose commentary. 48 Context ‚Üí The City BY C. P. CAVAFY TRANSLATED BY EDMUND KEELEY [Poem text omitted] SOME TREES John Ashbery [Poem text omitted] Shadows on the Way Wallace Stevens -------- Generated Poem 1 -------- -------- Generated Poem 3 -------- I must have shadows on the way The sun was all we had. Now, in the shade If I am to walk
EDMUND KEELEY [Poem text omitted] SOME TREES John Ashbery [Poem text omitted] Shadows on the Way Wallace Stevens -------- Generated Poem 1 -------- -------- Generated Poem 3 -------- I must have shadows on the way The sun was all we had. Now, in the shade If I am to walk I must have All is changed. The mind must dwell on those Each step taken slowly and alone White fields, that to its eyes were always old; To have it ready made Those ancient gleams, convoluting The way they lay among the huge roots, And I must think in lines of grey The great dark tomes of reverie, To have dim thoughts to be my guide The plumed labyrinth of the sea. Must look on blue and green And never let my eye forget When our own breath gonne faint and fade That color is my friend Up the long stairway of the dead And purple must surround me too We shall remember well The blindness of the light, the whiteness The yellow of the sun is no more Of that white land. The footsteps, and Intrusive than the bluish snow The grasses in that light, and in the shade
stairway of the dead And purple must surround me too We shall remember well The blindness of the light, the whiteness The yellow of the sun is no more Of that white land. The footsteps, and Intrusive than the bluish snow The grasses in that light, and in the shade That falls on all of us. I must have The menacing life. Grey thoughts and blue thoughts walk with me If I am to go away at all. We played, too, once, in the warmest rooms. To one content, there is one content -------- Generated Poem 2 -------- For all mankind. The forest glades Are all the more for being in fable, There is a way in the world. And wood and town in legend shadowed- That way a man may live in a small town Once more to return. The one content His lifetime and see no man come, Has in it every happiness, more brave Never the stir of men about a market, For being purposeless; and so Never the bobbing of covered carts, Those island gardens, deep in the mind, Never the creak passed on the stair Make of remoteness familiarity Closing some other life; an old
Has in it every happiness, more brave Never the stir of men about a market, For being purposeless; and so Never the bobbing of covered carts, Those island gardens, deep in the mind, Never the creak passed on the stair Make of remoteness familiarity Closing some other life; an old man And of the unspoken name a friend. Can prosper in that town, can prosper When his store is seen to close, his wife -------- Generated Poem 4 -------- Fades in his house, and the world shouts in vain That no one comes to hear him Nobody will come to this place. It is a road that leads nowhere. Where his small bell tolls. The solitude is deep. The mountains are high. In that small town But they are desolate, and they turn the traveler‚Äôs face Everything is known. Towards the North. All the sounds of the world are far away. The ways of the young men are known, When the wind rises above the trees, And the ways of the young women The boughs bow to the ground. As they walk to the Spring; Even the birds that inhabit the tangle of weeds The rumors are all known, That
away. The ways of the young men are known, When the wind rises above the trees, And the ways of the young women The boughs bow to the ground. As they walk to the Spring; Even the birds that inhabit the tangle of weeds The rumors are all known, That is the roadside cover, are silent. One listens, The quarrels, the hurt, the envy But hears no roar of the forest. One is alone. Of the men and the women. One will be taken. Till a day comes One will be taken. That way out of the world There is no utterance, there is no conversation, That leads to a wood. But one is uneasy all the same.... Young, the young man There is a thin blue mist, Walks, a lover now, A darkness rising like smoke, Knows the streets as they are and is of them. And within that darkness A possession of the heart. Comes to the wood. One will be taken.... It was here, and it will be here again- Here, under this sky empty and full of light. There, in the wood, among trees, He sees shadows on the way, hears voices, hears the wind and
darkness A possession of the heart. Comes to the wood. One will be taken.... It was here, and it will be here again- Here, under this sky empty and full of light. There, in the wood, among trees, He sees shadows on the way, hears voices, hears the wind and the rustling of leaves; Through an open glade He sees a shape and the shape hears: It waits as he waits, Waits as the shadows wait, As the voices wait; Shadows on the way, voices in the wind. Figure F.1: Four uncurated completions from a context suggesting the model compose a poem in the style of Wallace Stevens with the title ‚ÄòShadows on the Way‚Äô. 49 G Details of Task Phrasing and SpeciÔ¨Åcations The following Ô¨Ågures illustrate the formatting and phrasing of all the tasks included in the paper. All data comes from the ground truth datasets in this section, and no samples from GPT-3 are included here. Context ‚Üí Article: Informal conversation is an important part of any business relationship.Before you start a discussion,however,make sure you understand which topics are suitable and which are considered taboo in a particular culture. Latin Americans enjoy sharing information about their local history,
and no samples from GPT-3 are included here. Context ‚Üí Article: Informal conversation is an important part of any business relationship.Before you start a discussion,however,make sure you understand which topics are suitable and which are considered taboo in a particular culture. Latin Americans enjoy sharing information about their local history, art and customs.You may expect questions about your family,and be sure to show pictures of your children.You may feel free to ask similar questions of your Latin American friends.The French think of conversation as an art form,and they enjoy the value of lively discussions as well as disagreements. For them,arguments can be interesting and they can cover pretty much or any topic ---- as long as they occur in are respectful and intelligent manner. In the United States,business people like to discuss a wide range of topics,including opinions about work,family,hobbies,and politics. In Japan,China,and Korea,however,people are much more private.They do not share much about their thoughts,feelings,or emotions because they feel that doing so might take away from the harmonious business relationship they‚Äôre trying to build.Middle Easterners are also private about their personal lives and family matters.It is considered rude,for example,to ask a businessman from Saudi Arabia about his wife or children.
share much about their thoughts,feelings,or emotions because they feel that doing so might take away from the harmonious business relationship they‚Äôre trying to build.Middle Easterners are also private about their personal lives and family matters.It is considered rude,for example,to ask a businessman from Saudi Arabia about his wife or children. As a general rule,it‚Äôs best not to talk about politics or religion with your business friends.This can get you into trouble,even in the United States,where people hold different religious views.In addition,discussing one‚Äôs salary is usually considered unsuitable.Sports is typically a friendly subject in most parts of the world,although be careful not to criticize national sport.Instead,be friendly and praise your host‚Äôs team. Q: What shouldn‚Äôt you do when talking about sports with colleagues from another country? A: Criticizing the sports of your colleagues‚Äô country. Q: Which is typically a friendly topic in most places according to the author? A: Sports. Q: Why are people from Asia more private in their conversation with others? A: They don‚Äôt want to have their good relationship with others harmed by informal conversation. Q: The author considers politics and religion . A: Correct Answer ‚Üí taboo Incorrect Answer ‚Üí cheerful topics Incorrect Answer ‚Üí rude topics
Why are people from Asia more private in their conversation with others? A: They don‚Äôt want to have their good relationship with others harmed by informal conversation. Q: The author considers politics and religion . A: Correct Answer ‚Üí taboo Incorrect Answer ‚Üí cheerful topics Incorrect Answer ‚Üí rude topics Incorrect Answer ‚Üí topics that can never be talked about Figure G.1: Formatted dataset example for RACE-h. When predicting, we normalize by the unconditional probability of each answer as described in 2. 50 Context ‚Üí anli 2: anli 2: The Gold Coast Hotel & Casino is a hotel and casino located in Paradise, Nevada. This locals‚Äô casino is owned and operated by Boyd Gaming. The Gold Coast is located one mile (‚àº 1.6km) west of the Las Vegas Strip on West Flamingo Road. It is located across the street from the Palms Casino Resort and the Rio All Suite Hotel and Casino. Question: The Gold Coast is a budget-friendly casino. True, False, or Neither? Correct Answer ‚Üí Neither Incorrect Answer ‚Üí True Incorrect Answer ‚Üí False Figure G.2: Formatted dataset example for ANLI R2 Context ‚Üí Article: Mrs. Smith is an unusual teacher. Once she told each student to bring
and Casino. Question: The Gold Coast is a budget-friendly casino. True, False, or Neither? Correct Answer ‚Üí Neither Incorrect Answer ‚Üí True Incorrect Answer ‚Üí False Figure G.2: Formatted dataset example for ANLI R2 Context ‚Üí Article: Mrs. Smith is an unusual teacher. Once she told each student to bring along a few potatoes in plastic bag. On each potato the students had to write a name of a person that they hated And the next day, every child brought some potatoes. Some had two potatoes;some three;some up to five. Mrs. Smith then told the children to carry the bags everywhere they went, even to the toilet, for two weeks. As day after day passed, the children started to complain about the awful smell of the rotten potatoes. Those children who brought five potatoes began to feel the weight trouble of the bags. After two weeks, the children were happy to hear that the game was finally ended. Mrs. Smith asked,"How did you feel while carrying the potatoes for two weeks?" The children started complaining about the trouble loudly. Then Mrs. Smith told them why she asked them to play the game. She said,"This is exactly the situation when you
to hear that the game was finally ended. Mrs. Smith asked,"How did you feel while carrying the potatoes for two weeks?" The children started complaining about the trouble loudly. Then Mrs. Smith told them why she asked them to play the game. She said,"This is exactly the situation when you carry your hatred for somebody inside your heart. The terrible smell of the hatred will pollute your heart and you will carry something unnecessary with you all the time. If you cannot stand the smell of the rotten potatoes for just two weeks, can you imagine how heavy it would be to have the hatred in your heart for your lifetime? So throw away any hatred from your heart, and you‚Äôll be really happy." Q: Which of the following is True according to the passage? A: If a kid hated four people,he or she had to carry four potatoes. Q: We can learn from the passage that we should . A: throw away the hatred inside Q: The children complained about besides the weight trouble. A: the smell Q: Mrs.Smith asked her students to write on the potatoes. A: Correct Answer ‚Üí names Incorrect Answer ‚Üí numbers Incorrect Answer ‚Üí
We can learn from the passage that we should . A: throw away the hatred inside Q: The children complained about besides the weight trouble. A: the smell Q: Mrs.Smith asked her students to write on the potatoes. A: Correct Answer ‚Üí names Incorrect Answer ‚Üí numbers Incorrect Answer ‚Üí time Incorrect Answer ‚Üí places Figure G.3: Formatted dataset example for RACE-m. When predicting, we normalize by the unconditional probability of each answer as described in 2. 51 Context ‚Üí How to apply sealant to wood. Correct Answer ‚Üí Using a brush, brush on sealant onto wood until it is fully saturated with the sealant. Incorrect Answer ‚Üí Using a brush, drip on sealant onto wood until it is fully saturated with the sealant. Figure G.4: Formatted dataset example for PIQA Context ‚Üí My body cast a shadow over the grass because Correct Answer ‚Üí the sun was rising. Incorrect Answer ‚Üí the grass was cut. Figure G.5: Formatted dataset example for COPA Context ‚Üí (CNN) Yuval Rabin, whose father, Yitzhak Rabin, was assassinated while serving as Prime Minister of Israel, criticized Donald Trump for appealing to "Second Amendment people" in a speech and warned that the words that politicians
Answer ‚Üí the grass was cut. Figure G.5: Formatted dataset example for COPA Context ‚Üí (CNN) Yuval Rabin, whose father, Yitzhak Rabin, was assassinated while serving as Prime Minister of Israel, criticized Donald Trump for appealing to "Second Amendment people" in a speech and warned that the words that politicians use can incite violence and undermine democracy. "Trump‚Äôs words are an incitement to the type of political violence that touched me personally," Rabin wrote in USAToday. He said that Trump‚Äôs appeal to "Second Amendment people" to stop Hillary Clinton -- comments that were criticized as a call for violence against Clinton, something Trump denied -- "were a new level of ugliness in an ugly campaign season." - The son of a former Israeli Prime Minister who was assassinated wrote an op ed about the consequence of violent political rhetoric. - Warns of "parallels" between Israel of the 1990s and the U.S. today. Correct Answer ‚Üí - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned Donald Trump‚Äôs aggressive rhetoric. Correct Answer ‚Üí - Referencing his father, who was shot and killed by an extremist amid political tension in Israel
U.S. today. Correct Answer ‚Üí - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned Donald Trump‚Äôs aggressive rhetoric. Correct Answer ‚Üí - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned Trump‚Äôs aggressive rhetoric. Incorrect Answer ‚Üí - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned Hillary Clinton‚Äôs aggressive rhetoric. Incorrect Answer ‚Üí - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned U.S.‚Äôs aggressive rhetoric. Incorrect Answer ‚Üí - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned Yitzhak Rabin‚Äôs aggressive rhetoric. Figure G.6: Formatted dataset example for ReCoRD. We consider the context above to be a single ‚Äùproblem‚Äù because this is how the task is presented in the ReCoRD dataset and scored in the ReCoRD evaluation script. Context ‚Üí anli 1: anli 1: Fulton James MacGregor MSP is a Scottish politician who is a Scottish National Party (SNP) Member of Scottish
context above to be a single ‚Äùproblem‚Äù because this is how the task is presented in the ReCoRD dataset and scored in the ReCoRD evaluation script. Context ‚Üí anli 1: anli 1: Fulton James MacGregor MSP is a Scottish politician who is a Scottish National Party (SNP) Member of Scottish Parliament for the constituency of Coatbridge and Chryston. MacGregor is currently Parliamentary Liaison Officer to Shona Robison, Cabinet Secretary for Health & Sport. He also serves on the Justice and Education & Skills committees in the Scottish Parliament. Question: Fulton James MacGregor is a Scottish politican who is a Liaison officer to Shona Robison who he swears is his best friend. True, False, or Neither? Correct Answer ‚Üí Neither Incorrect Answer ‚Üí True Incorrect Answer ‚Üí False Figure G.7: Formatted dataset example for ANLI R1 52 Context ‚Üí Organisms require energy in order to do what? Correct Answer ‚Üí mature and develop. Incorrect Answer ‚Üí rest soundly. Incorrect Answer ‚Üí absorb light. Incorrect Answer ‚Üí take in nutrients. Figure G.8: Formatted dataset example for OpenBookQA. When predicting, we normalize by the unconditional probability of each answer as described in 2. Context ‚Üí Making a cake: Several cake pops are shown
and develop. Incorrect Answer ‚Üí rest soundly. Incorrect Answer ‚Üí absorb light. Incorrect Answer ‚Üí take in nutrients. Figure G.8: Formatted dataset example for OpenBookQA. When predicting, we normalize by the unconditional probability of each answer as described in 2. Context ‚Üí Making a cake: Several cake pops are shown on a display. A woman and girl are shown making the cake pops in a kitchen. They Correct Answer ‚Üí bake them, then frost and decorate. Incorrect Answer ‚Üí taste them as they place them on plates. Incorrect Answer ‚Üí put the frosting on the cake as they pan it. Incorrect Answer ‚Üí come out and begin decorating the cake as well. Figure G.9: Formatted dataset example for HellaSwag Context ‚Üí anli 3: anli 3: We shut the loophole which has American workers actually subsidizing the loss of their own job. They just passed an expansion of that loophole in the last few days: $43 billion of giveaways, including favors to the oil and gas industry and the people importing ceiling fans from China. Question: The loophole is now gone True, False, or Neither? Correct Answer ‚Üí False Incorrect Answer ‚Üí True Incorrect Answer ‚Üí Neither Figure G.10: Formatted dataset
the last few days: $43 billion of giveaways, including favors to the oil and gas industry and the people importing ceiling fans from China. Question: The loophole is now gone True, False, or Neither? Correct Answer ‚Üí False Incorrect Answer ‚Üí True Incorrect Answer ‚Üí Neither Figure G.10: Formatted dataset example for ANLI R3 Context ‚Üí Question: George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat? Answer: Correct Answer ‚Üí dry palms Incorrect Answer ‚Üí wet palms Incorrect Answer ‚Üí palms covered with oil Incorrect Answer ‚Üí palms covered with lotion Figure G.11: Formatted dataset example for ARC (Challenge). When predicting, we normalize by the unconditional probability of each answer as described in 2. Context ‚Üí lull is to trust as Correct Answer ‚Üí cajole is to compliance Incorrect Answer ‚Üí balk is to fortitude Incorrect Answer ‚Üí betray is to loyalty Incorrect Answer ‚Üí hinder is to destination Incorrect Answer ‚Üí soothe is to passion Figure G.12: Formatted dataset example for SAT Analogies Correct Context ‚Üí Grace was happy to trade me her sweater for my jacket. She thinks the sweater Incorrect Context ‚Üí Grace was happy to trade me
loyalty Incorrect Answer ‚Üí hinder is to destination Incorrect Answer ‚Üí soothe is to passion Figure G.12: Formatted dataset example for SAT Analogies Correct Context ‚Üí Grace was happy to trade me her sweater for my jacket. She thinks the sweater Incorrect Context ‚Üí Grace was happy to trade me her sweater for my jacket. She thinks the jacket Target Completion ‚Üí looks dowdy on her. Figure G.13: Formatted dataset example for Winograd. The ‚Äòpartial‚Äô evaluation method we use compares the probability of the completion given a correct and incorrect context. 53 Correct Context ‚Üí Johnny likes fruits more than vegetables in his new keto diet because the fruits Incorrect Context ‚Üí Johnny likes fruits more than vegetables in his new keto diet because the vegetables Target Completion ‚Üí are saccharine. Figure G.14: Formatted dataset example for Winogrande. The ‚Äòpartial‚Äô evaluation method we use compares the probability of the completion given a correct and incorrect context. Context ‚Üí READING COMPREHENSION ANSWER KEY While this process moved along, diplomacy continued its rounds. Direct pressure on the Taliban had proved unsuccessful. As one NSC staff note put it, "Under the Taliban, Afghanistan is not so much a state sponsor of terrorism as
given a correct and incorrect context. Context ‚Üí READING COMPREHENSION ANSWER KEY While this process moved along, diplomacy continued its rounds. Direct pressure on the Taliban had proved unsuccessful. As one NSC staff note put it, "Under the Taliban, Afghanistan is not so much a state sponsor of terrorism as it is a state sponsored by terrorists." In early 2000, the United States began a high-level effort to persuade Pakistan to use its influence over the Taliban. In January 2000, Assistant Secretary of State Karl Inderfurth and the State Department‚Äôs counterterrorism coordinator, Michael Sheehan, met with General Musharraf in Islamabad, dangling before him the possibility of a presidential visit in March as a reward for Pakistani cooperation. Such a visit was coveted by Musharraf, partly as a sign of his government‚Äôs legitimacy. He told the two envoys that he would meet with Mullah Omar and press him on Bin Laden. They left, however, reporting to Washington that Pakistan was unlikely in fact to do anything," given what it sees as the benefits of Taliban control of Afghanistan." President Clinton was scheduled to travel to India. The State Department felt that he should not visit India without also visiting Pakistan. The
They left, however, reporting to Washington that Pakistan was unlikely in fact to do anything," given what it sees as the benefits of Taliban control of Afghanistan." President Clinton was scheduled to travel to India. The State Department felt that he should not visit India without also visiting Pakistan. The Secret Service and the CIA, however, warned in the strongest terms that visiting Pakistan would risk the President‚Äôs life. Counterterrorism officials also argued that Pakistan had not done enough to merit a presidential visit. But President Clinton insisted on including Pakistan in the itinerary for his trip to South Asia. His one-day stopover on March 25, 2000, was the first time a U.S. president had been there since 1969. At his meeting with Musharraf and others, President Clinton concentrated on tensions between Pakistan and India and the dangers of nuclear proliferation, but also discussed Bin Laden. President Clinton told us that when he pulled Musharraf aside for a brief, one-on-one meeting, he pleaded with the general for help regarding Bin Laden." I offered him the moon when I went to see him, in terms of better relations with the United States, if he‚Äôd help us get Bin Laden and deal
that when he pulled Musharraf aside for a brief, one-on-one meeting, he pleaded with the general for help regarding Bin Laden." I offered him the moon when I went to see him, in terms of better relations with the United States, if he‚Äôd help us get Bin Laden and deal with another issue or two." The U.S. effort continued. Who did The State Department feel should visit both India and Pakistan? Correct Answer ‚Üí - [False] Bin Laden Incorrect Answer ‚Üí - [True] Bin Laden Figure G.15: Formatted dataset example for MultiRC. There are three levels within MultiRC: (1) the passage, (2) the questions, and (3) the answers. During evaluation, accuracy is determined at the per-question level, with a question being considered correct if and only if all the answers within the question are labeled correctly. For this reason, we use K to refer to the number of questions shown within the context. Context ‚Üí Question: Which factor will most likely cause a person to develop a fever? Answer: Correct Answer ‚Üí a bacterial population in the bloodstream Incorrect Answer ‚Üí a leg muscle relaxing after exercise Incorrect Answer ‚Üí several viral particles on the skin Incorrect Answer ‚Üí carbohydrates
within the context. Context ‚Üí Question: Which factor will most likely cause a person to develop a fever? Answer: Correct Answer ‚Üí a bacterial population in the bloodstream Incorrect Answer ‚Üí a leg muscle relaxing after exercise Incorrect Answer ‚Üí several viral particles on the skin Incorrect Answer ‚Üí carbohydrates being digested in the stomach Figure G.16: Formatted dataset example for ARC (Easy). When predicting, we normalize by the unconditional probability of each answer as described in 2. 54 Context ‚Üí Bob went to the gas station to fill up his car. His tank was completely empty and so was his wallet. The cashier offered to pay for his gas if he came back later to pay. Bob felt grateful as he drove home. Correct Answer ‚Üí Bob believed that there were good people in the world. Incorrect Answer ‚Üí Bob contemplated how unfriendly the world was. Figure G.17: Formatted dataset example for StoryCloze Context ‚Üí Helsinki is the capital and largest city of Finland. It is in the region of Uusimaa, in southern Finland, on the shore of the Gulf of Finland. Helsinki has a population of , an urban population of , and a metropolitan population of over
dataset example for StoryCloze Context ‚Üí Helsinki is the capital and largest city of Finland. It is in the region of Uusimaa, in southern Finland, on the shore of the Gulf of Finland. Helsinki has a population of , an urban population of , and a metropolitan population of over 1.4 million, making it the most populous municipality and urban area in Finland. Helsinki is some north of Tallinn, Estonia, east of Stockholm, Sweden, and west of Saint Petersburg, Russia. Helsinki has close historical connections with these three cities. The Helsinki metropolitan area includes the urban core of Helsinki, Espoo, Vantaa, Kauniainen, and surrounding commuter towns. It is the world‚Äôs northernmost metro area of over one million people, and the city is the northernmost capital of an EU member state. The Helsinki metropolitan area is the third largest metropolitan area in the Nordic countries after Stockholm and Copenhagen, and the City of Helsinki is the third largest after Stockholm and Oslo. Helsinki is Finland‚Äôs major political, educational, financial, cultural, and research center as well as one of northern Europe‚Äôs major cities. Approximately 75% of foreign companies that operate in Finland have settled in the Helsinki region. The nearby municipality of
City of Helsinki is the third largest after Stockholm and Oslo. Helsinki is Finland‚Äôs major political, educational, financial, cultural, and research center as well as one of northern Europe‚Äôs major cities. Approximately 75% of foreign companies that operate in Finland have settled in the Helsinki region. The nearby municipality of Vantaa is the location of Helsinki Airport, with frequent service to various destinations in Europe and Asia. Q: what is the most populous municipality in Finland? A: Helsinki Q: how many people live there? A: 1.4 million in the metropolitan area Q: what percent of the foreign companies that operate in Finland are in Helsinki? A: 75% Q: what towns are a part of the metropolitan area? A: Target Completion ‚Üí Helsinki, Espoo, Vantaa, Kauniainen, and surrounding commuter towns Figure G.18: Formatted dataset example for CoQA Context ‚Üí Please unscramble the letters into a word, and write that word: asinoc = Target Completion ‚Üí casino Figure G.19: Formatted dataset example for Cycled Letters 55 Context ‚Üí Passage: Saint Jean de Br¬¥ebeuf was a French Jesuit missionary who travelled to New France in 1625. There he worked primarily with the Huron for the rest of his life, except for a few
= Target Completion ‚Üí casino Figure G.19: Formatted dataset example for Cycled Letters 55 Context ‚Üí Passage: Saint Jean de Br¬¥ebeuf was a French Jesuit missionary who travelled to New France in 1625. There he worked primarily with the Huron for the rest of his life, except for a few years in France from 1629 to 1633. He learned their language and culture, writing extensively about each to aid other missionaries. In 1649, Br¬¥ebeuf and another missionary were captured when an Iroquois raid took over a Huron village . Together with Huron captives, the missionaries were ritually tortured and killed on March 16, 1649. Br¬¥ebeuf was beatified in 1925 and among eight Jesuit missionaries canonized as saints in the Roman Catholic Church in 1930. Question: How many years did Saint Jean de Br¬¥ebeuf stay in New France before he went back to France for a few years? Answer: Target Completion ‚Üí 4 Figure G.20: Formatted dataset example for DROP Context ‚Üí Fill in blank: She held the torch in front of her. She caught her breath. "Chris? There‚Äôs a step." "What?" "A step. Cut in the rock. About fifty feet ahead." She moved faster. They both moved faster. "In fact,"
‚Üí 4 Figure G.20: Formatted dataset example for DROP Context ‚Üí Fill in blank: She held the torch in front of her. She caught her breath. "Chris? There‚Äôs a step." "What?" "A step. Cut in the rock. About fifty feet ahead." She moved faster. They both moved faster. "In fact," she said, raising the torch higher, "there‚Äôs more than a . -> Target Completion ‚Üí step Figure G.21: Formatted dataset example for LAMBADA Context ‚Üí Please unscramble the letters into a word, and write that word: skicts = Target Completion ‚Üí sticks Figure G.22: Formatted dataset example for Anagrams 1 (A1) Context ‚Üí Please unscramble the letters into a word, and write that word: volwskagen = Target Completion ‚Üí volkswagen Figure G.23: Formatted dataset example for Anagrams 2 Context ‚Üí Q: Who played tess on touched by an angel? A: Target Completion ‚Üí Delloreese Patricia Early (July 6, 1931 { November 19, 2017), known professionally as Della Reese Figure G.24: Formatted dataset example for Natural Questions 56 Context ‚Üí TITLE: William Perry (American football) - Professional career PARAGRAPH: In 1985, he was selected in the first round of the 1985 NFL Draft by the Chicago Bears; he had been hand-picked
November 19, 2017), known professionally as Della Reese Figure G.24: Formatted dataset example for Natural Questions 56 Context ‚Üí TITLE: William Perry (American football) - Professional career PARAGRAPH: In 1985, he was selected in the first round of the 1985 NFL Draft by the Chicago Bears; he had been hand-picked by coach Mike Ditka. However, defensive coordinator Buddy Ryan, who had a highly acrimonious relationship with Ditka, called Perry a "wasted draft-pick". Perry soon became a pawn in the political power struggle between Ditka and Ryan. Perry‚Äôs "Refrigerator" nickname followed him into the NFL and he quickly became a favorite of the Chicago Bears fans. Teammates called him "Biscuit," as in "one biscuit shy of 350 pounds." While Ryan refused to play Perry, Ditka decided to use Perry as a fullback when the team was near the opponents‚Äô goal line or in fourth and short situations, either as a ball carrier or a lead blocker for star running back Walter Payton. Ditka stated the inspiration for using Perry as a fullback came to him during five-yard sprint exercises. During his rookie season, Perry rushed for two touchdowns and caught a pass for one. Perry even had the opportunity to run
carrier or a lead blocker for star running back Walter Payton. Ditka stated the inspiration for using Perry as a fullback came to him during five-yard sprint exercises. During his rookie season, Perry rushed for two touchdowns and caught a pass for one. Perry even had the opportunity to run the ball during Super Bowl XX, as a nod to his popularity and contributions to the team‚Äôs success. The first time he got the ball, he was tackled for a one-yard loss while attempting to throw his first NFL pass on a halfback option play. The second time he got the ball, he scored a touchdown (running over Patriots linebacker Larry McGrew in the process). About halfway through his rookie season, Ryan finally began to play Perry, who soon proved that he was a capable defensive lineman. His Super Bowl ring size is the largest of any professional football player in the history of the event. His ring size is 25, while the ring size for the average adult male is between 10 and 12. Perry went on to play for ten years in the NFL, retiring after the 1994 season. In his ten years as a pro, he regularly
in the history of the event. His ring size is 25, while the ring size for the average adult male is between 10 and 12. Perry went on to play for ten years in the NFL, retiring after the 1994 season. In his ten years as a pro, he regularly struggled with his weight, which hampered his performance at times. He played in 138 games, recording 29.5 sacks and five fumble recoveries, which he returned for a total of 71 yards. In his offensive career he ran five yards for two touchdowns, and had one reception for another touchdown. Perry later attempted a comeback, playing an unremarkable 1996 season with the London Monarchs of the World League of American Football (later NFL Europa). Q: what team did he play for? A: Target Completion ‚Üí the Chicago Bears Figure G.25: Formatted dataset example for QuAC Context ‚Üí Please unscramble the letters into a word, and write that word: r e!c.i p r o.c a/l = Target Completion ‚Üí reciprocal Figure G.26: Formatted dataset example for Symbol Insertion Context ‚Üí Please unscramble the letters into a word, and write that word: taefed = Target Completion ‚Üí defeat Figure G.27: Formatted dataset example
into a word, and write that word: r e!c.i p r o.c a/l = Target Completion ‚Üí reciprocal Figure G.26: Formatted dataset example for Symbol Insertion Context ‚Üí Please unscramble the letters into a word, and write that word: taefed = Target Completion ‚Üí defeat Figure G.27: Formatted dataset example for Reversed Words 57 Context ‚Üí Title: The Blitz Background: From the German point of view, March 1941 saw an improvement. The Luftwaffe flew 4,000 sorties that month, including 12 major and three heavy attacks. The electronic war intensified but the Luftwaffe flew major inland missions only on moonlit nights. Ports were easier to find and made better targets. To confuse the British, radio silence was observed until the bombs fell. X- and Y-Ger¬®at beams were placed over false targets and switched only at the last minute. Rapid frequency changes were introduced for X-Ger¬®at, whose wider band of frequencies and greater tactical flexibility ensured it remained effective at a time when British selective jamming was degrading the effectiveness of Y-Ger¬®at. Q: How many sorties were flown in March 1941? A: 4,000 Q: When did the Luftwaffe fly inland missions? A: Target Completion ‚Üí only on moonlit nights Figure G.28: Formatted
greater tactical flexibility ensured it remained effective at a time when British selective jamming was degrading the effectiveness of Y-Ger¬®at. Q: How many sorties were flown in March 1941? A: 4,000 Q: When did the Luftwaffe fly inland missions? A: Target Completion ‚Üí only on moonlit nights Figure G.28: Formatted dataset example for SQuADv2 Context ‚Üí Normal force -- In a simple case such as an object resting upon a table, the normal force on the object is equal but in opposite direction to the gravitational force applied on the object (or the weight of the object), that is, N = m g (\displaystyle N=mg), where m is mass, and g is the gravitational field strength (about 9.81 m/s on Earth). The normal force here represents the force applied by the table against the object that prevents it from sinking through the table and requires that the table is sturdy enough to deliver this normal force without breaking. However, it is easy to assume that the normal force and weight are action-reaction force pairs (a common mistake). In this case, the normal force and weight need to be equal in magnitude to explain why there is no upward acceleration of
enough to deliver this normal force without breaking. However, it is easy to assume that the normal force and weight are action-reaction force pairs (a common mistake). In this case, the normal force and weight need to be equal in magnitude to explain why there is no upward acceleration of the object. For example, a ball that bounces upwards accelerates upwards because the normal force acting on the ball is larger in magnitude than the weight of the ball. question: is the normal force equal to the force of gravity? answer: Target Completion ‚Üí yes Figure G.29: Formatted dataset example for BoolQ Context ‚Üí The trend toward lower rents may seem surprising given that some communities in New York are bemoaning the loss of favorite local businesses to high rents. But, despite the recent softening, for many of these retailers there‚Äôs still been too big a jump from the rental rates of the late 1970s, when their leases were signed. Certainly, the recent drop in prices doesn‚Äôt mean Manhattan comes cheap. question: Manhattan comes cheap. true, false, or neither? answer: Target Completion ‚Üí false Figure G.30: Formatted dataset example for CB 58 Context ‚Üí The bet, which won him dinner
rates of the late 1970s, when their leases were signed. Certainly, the recent drop in prices doesn‚Äôt mean Manhattan comes cheap. question: Manhattan comes cheap. true, false, or neither? answer: Target Completion ‚Üí false Figure G.30: Formatted dataset example for CB 58 Context ‚Üí The bet, which won him dinner for four, was regarding the existence and mass of the top quark, an elementary particle discovered in 1995. question: The Top Quark is the last of six flavors of quarks predicted by the standard model theory of particle physics. True or False? answer: Target Completion ‚Üí False Figure G.31: Formatted dataset example for RTE Context ‚Üí An outfitter provided everything needed for the safari. Before his first walking holiday, he went to a specialist outfitter to buy some boots. question: Is the word ‚Äòoutfitter‚Äô used in the same way in the two sentences above? answer: Target Completion ‚Üí no Figure G.32: Formatted dataset example for WiC Context ‚Üí Final Exam with Answer Key Instructions: Please carefully read the following passages. For each passage, you must identify which noun the pronoun marked in *bold* refers to. ===== Passage: Mr. Moncrieff visited Chester‚Äôs luxurious New York apartment, thinking that it belonged to
G.32: Formatted dataset example for WiC Context ‚Üí Final Exam with Answer Key Instructions: Please carefully read the following passages. For each passage, you must identify which noun the pronoun marked in *bold* refers to. ===== Passage: Mr. Moncrieff visited Chester‚Äôs luxurious New York apartment, thinking that it belonged to his son Edward. The result was that Mr. Moncrieff has decided to cancel Edward‚Äôs allowance on the ground that he no longer requires *his* financial support. Question: In the passage above, what does the pronoun "*his*" refer to? Answer: Target Completion ‚Üí mr. moncrieff Figure G.33: Formatted dataset example for WSC Context ‚Üí Q: ‚ÄòNude Descending A Staircase‚Äô is perhaps the most famous painting by which 20th century artist? A: Target Completion ‚Üí MARCEL DUCHAMP Target Completion ‚Üí r mutt Target Completion ‚Üí duchamp Target Completion ‚Üí marcel duchamp Target Completion ‚Üí R.Mutt Target Completion ‚Üí Marcel duChamp Target Completion ‚Üí Henri-Robert-Marcel Duchamp Target Completion ‚Üí Marcel du Champ Target Completion ‚Üí henri robert marcel duchamp Target Completion ‚Üí Duchampian Target Completion ‚Üí Duchamp Target Completion ‚Üí duchampian Target Completion ‚Üí marcel du champ Target Completion ‚Üí Marcel Duchamp Target Completion ‚Üí MARCEL DUCHAMP Figure G.34: Formatted dataset example for
‚Üí Henri-Robert-Marcel Duchamp Target Completion ‚Üí Marcel du Champ Target Completion ‚Üí henri robert marcel duchamp Target Completion ‚Üí Duchampian Target Completion ‚Üí Duchamp Target Completion ‚Üí duchampian Target Completion ‚Üí marcel du champ Target Completion ‚Üí Marcel Duchamp Target Completion ‚Üí MARCEL DUCHAMP Figure G.34: Formatted dataset example for TriviaQA. TriviaQA allows for multiple valid completions. 59 Context ‚Üí Q: What school did burne hogarth establish? A: Target Completion ‚Üí School of Visual Arts Figure G.35: Formatted dataset example for WebQA Context ‚Üí Keinesfalls d¬®urfen diese f¬®ur den kommerziellen Gebrauch verwendet werden. = Target Completion ‚Üí In no case may they be used for commercial purposes. Figure G.36: Formatted dataset example for De‚ÜíEn. This is the format for one- and few-shot learning, for this and other langauge tasks, the format for zero-shot learning is ‚ÄúQ: What is the {language} translation of {sentence} A: {translation}.‚Äù Context ‚Üí In no case may they be used for commercial purposes. = Target Completion ‚Üí Keinesfalls d¬®urfen diese f¬®ur den kommerziellen Gebrauch verwendet werden. Figure G.37: Formatted dataset example for En‚ÜíDe Context ‚Üí Analysis of instar distributions of larval I. verticalis collected from a series of ponds also indicated that males were in more
may they be used for commercial purposes. = Target Completion ‚Üí Keinesfalls d¬®urfen diese f¬®ur den kommerziellen Gebrauch verwendet werden. Figure G.37: Formatted dataset example for En‚ÜíDe Context ‚Üí Analysis of instar distributions of larval I. verticalis collected from a series of ponds also indicated that males were in more advanced instars than females. = Target Completion ‚Üí L‚Äôanalyse de la distribution de fr¬¥equence des stades larvaires d‚ÄôI. verticalis dans une s¬¥erie d‚Äô¬¥etangs a ¬¥egalement d¬¥emontr¬¥e que les larves m^ales ¬¥etaient `a des stades plus avanc¬¥es que les larves femelles. Figure G.38: Formatted dataset example for En‚ÜíFr Context ‚Üí L‚Äôanalyse de la distribution de fr¬¥equence des stades larvaires d‚ÄôI. verticalis dans une s¬¥erie d‚Äô¬¥etangs a ¬¥egalement d¬¥emontr¬¥e que les larves m^ales ¬¥etaient `a des stades plus avanc¬¥es que les larves femelles. = Target Completion ‚Üí Analysis of instar distributions of larval I. verticalis collected from a series of ponds also indicated that males were in more advanced instars than females. Figure G.39: Formatted dataset example for Fr‚ÜíEn Context ‚Üí The truth is that you want, at any price, and against the wishes of the peoples of Europe, to continue the negotiations for Turkey‚Äôs accession to the European Union, despite Turkey‚Äôs
indicated that males were in more advanced instars than females. Figure G.39: Formatted dataset example for Fr‚ÜíEn Context ‚Üí The truth is that you want, at any price, and against the wishes of the peoples of Europe, to continue the negotiations for Turkey‚Äôs accession to the European Union, despite Turkey‚Äôs continuing refusal to recognise Cyprus and despite the fact that the democratic reforms are at a standstill. = Target Completion ‚Üí AdevÀòarul este cÀòa vÀòa dorit¬∏i, cu orice pret¬∏ ¬∏si ^ƒ±mpotriva dorint¬∏ei europenilor, sÀòa continuat¬∏i negocierile de aderare a Turciei la Uniunea EuropeanÀòa, ^ƒ±n ciuda refuzului continuu al Turciei de a recunoa¬∏ste Ciprul ¬∏si ^ƒ±n ciuda faptului cÀòa reformele democratice au ajuns ^ƒ±ntr-un punct mort. Figure G.40: Formatted dataset example for En‚ÜíRo 60 Context ‚Üí AdevÀòarul este cÀòa vÀòa dorit¬∏i, cu orice pret¬∏ ¬∏si ^ƒ±mpotriva dorint¬∏ei europenilor, sÀòa continuat¬∏i negocierile de aderare a Turciei la Uniunea EuropeanÀòa, ^ƒ±n ciuda refuzului continuu al Turciei de a recunoa¬∏ste Ciprul ¬∏si ^ƒ±n ciuda faptului cÀòa reformele democratice au ajuns ^ƒ±ntr-un punct mort. = Target Completion ‚Üí The truth is that you want, at any price, and against the wishes of the peoples of Europe, to continue the negotiations for Turkey‚Äôs accession to the
continuu al Turciei de a recunoa¬∏ste Ciprul ¬∏si ^ƒ±n ciuda faptului cÀòa reformele democratice au ajuns ^ƒ±ntr-un punct mort. = Target Completion ‚Üí The truth is that you want, at any price, and against the wishes of the peoples of Europe, to continue the negotiations for Turkey‚Äôs accession to the European Union, despite Turkey‚Äôs continuing refusal to recognise Cyprus and despite the fact that the democratic reforms are at a standstill. Figure G.41: Formatted dataset example for Ro‚ÜíEn Context ‚Üí Q: What is (2 * 4) * 6? A: Target Completion ‚Üí 48 Figure G.42: Formatted dataset example for Arithmetic 1DC Context ‚Üí Q: What is 17 minus 14? A: Target Completion ‚Üí 3 Figure G.43: Formatted dataset example for Arithmetic 2D- Context ‚Üí Q: What is 98 plus 45? A: Target Completion ‚Üí 143 Figure G.44: Formatted dataset example for Arithmetic 2D+ Context ‚Üí Q: What is 95 times 45? A: Target Completion ‚Üí 4275 Figure G.45: Formatted dataset example for Arithmetic 2Dx Context ‚Üí Q: What is 509 minus 488? A: Target Completion ‚Üí 21 Figure G.46: Formatted dataset example for Arithmetic 3D- Context ‚Üí Q: What is 556 plus 497? A: Target Completion ‚Üí 1053 Figure G.47:
45? A: Target Completion ‚Üí 4275 Figure G.45: Formatted dataset example for Arithmetic 2Dx Context ‚Üí Q: What is 509 minus 488? A: Target Completion ‚Üí 21 Figure G.46: Formatted dataset example for Arithmetic 3D- Context ‚Üí Q: What is 556 plus 497? A: Target Completion ‚Üí 1053 Figure G.47: Formatted dataset example for Arithmetic 3D+ Context ‚Üí Q: What is 6209 minus 3365? A: Target Completion ‚Üí 2844 Figure G.48: Formatted dataset example for Arithmetic 4D- 61 Context ‚Üí Q: What is 9923 plus 617? A: Target Completion ‚Üí 10540 Figure G.49: Formatted dataset example for Arithmetic 4D+ Context ‚Üí Q: What is 40649 minus 78746? A: Target Completion ‚Üí -38097 Figure G.50: Formatted dataset example for Arithmetic 5D‚àí Context ‚Üí Q: What is 65360 plus 16204? A: Target Completion ‚Üí 81564 Figure G.51: Formatted dataset example for Arithmetic 5D+ 62 H Results on All Tasks for All Model Sizes Zero-Shot One-Shot Few-Shot Name Metric Split Fine-tune SOTA K Small Med Large XL 2.7B 6.7B 13B 175B Small Med Large XL 2.7B 6.7B 13B 175B Small Med Large XL 2.7B 6.7B 13B 175B 175B (test server) HellaSwag acc dev 85.6 20 33.7 43.6 51.0 54.7 62.8 67.4 70.9 78.9
Sizes Zero-Shot One-Shot Few-Shot Name Metric Split Fine-tune SOTA K Small Med Large XL 2.7B 6.7B 13B 175B Small Med Large XL 2.7B 6.7B 13B 175B Small Med Large XL 2.7B 6.7B 13B 175B 175B (test server) HellaSwag acc dev 85.6 20 33.7 43.6 51.0 54.7 62.8 67.4 70.9 78.9 33.0 42.9 50.5 53.5 61.9 66.5 70.0 78.1 33.5 43.1 51.3 54.9 62.9 67.3 71.3 79.3 LAMBADA acc test 68.0 15 42.7 54.3 60.4 63.6 67.1 70.3 72.5 76.2 22.0 47.1 52.6 58.3 61.1 65.4 69.0 72.5 22.0 40.4 63.2 57.0 78.1 79.1 81.3 86.4 LAMBADA ppl test 8.63 15 18.6 9.09 6.53 5.44 4.60 4.00 3.56 3.00 165.0 11.6 8.29 6.46 5.53 4.61 4.06 3.35 165.0 27.6 6.63 7.45 2.89 2.56 2.56 1.92 StoryCloze acc test 91.8 70 63.3 68.5 72.4 73.4 77.2 77.7 79.5 83.2 62.3 68.7 72.3 74.2 77.3 78.7 79.7 84.7 62.3 70.2 73.9 76.1 80.2 81.2 83.0 87.7 NQs acc test 44.5 64 0.64 1.75 2.71 4.40 6.01 5.79 7.84 14.6 1.19 3.07 4.79 5.43 8.73 9.78 13.7 23.0 1.72 4.46 7.89 9.72 13.2 17.0 21.0 29.9 TriviaQA acc dev 68.0 64 4.15 7.61 14.0 19.7 31.3 38.7 41.8 64.3 4.19 12.9 20.5 26.5 35.9
81.2 83.0 87.7 NQs acc test 44.5 64 0.64 1.75 2.71 4.40 6.01 5.79 7.84 14.6 1.19 3.07 4.79 5.43 8.73 9.78 13.7 23.0 1.72 4.46 7.89 9.72 13.2 17.0 21.0 29.9 TriviaQA acc dev 68.0 64 4.15 7.61 14.0 19.7 31.3 38.7 41.8 64.3 4.19 12.9 20.5 26.5 35.9 44.4 51.3 68.0 6.96 16.3 26.5 32.1 42.3 51.6 57.5 71.2 71.2 WebQs acc test 45.5 64 1.77 3.20 4.33 4.63 7.92 7.73 8.22 14.4 2.56 6.20 8.51 9.15 14.5 15.1 19.0 25.3 5.46 12.6 15.9 19.6 24.8 27.7 33.5 41.5 Ro‚ÜíEn 16 BLEU-mb test 39.9 64 2.08 2.71 3.09 3.15 16.3 8.34 20.2 19.9 0.55 15.4 23.0 26.3 30.6 33.2 35.6 38.6 1.25 20.7 25.8 29.2 33.1 34.8 37.0 39.5 Ro‚ÜíEn 16 BLEU-sb test 64 2.39 3.08 3.49 3.56 16.8 8.75 20.8 20.9 0.65 15.9 23.6 26.8 31.3 34.2 36.7 40.0 1.40 21.3 26.6 30.1 34.3 36.2 38.4 41.3 En‚ÜíRo 16 BLEU-mb test 38.5 64 2.14 2.65 2.53 2.50 3.46 4.24 5.32 14.1 0.35 3.30 7.89 8.72 13.2 15.1 17.3 20.6 1.25 5.90 9.33 10.7 14.3 16.3 18.0 21.0 En‚ÜíRo 16 BLEU-sb test 64 2.61 3.11 3.07 3.09 4.26 5.31 6.43 18.0 0.55 3.90 9.15 10.3 15.7 18.2 20.8
En‚ÜíRo 16 BLEU-mb test 38.5 64 2.14 2.65 2.53 2.50 3.46 4.24 5.32 14.1 0.35 3.30 7.89 8.72 13.2 15.1 17.3 20.6 1.25 5.90 9.33 10.7 14.3 16.3 18.0 21.0 En‚ÜíRo 16 BLEU-sb test 64 2.61 3.11 3.07 3.09 4.26 5.31 6.43 18.0 0.55 3.90 9.15 10.3 15.7 18.2 20.8 24.9 1.64 7.40 10.9 12.9 17.2 19.6 21.8 25.8 Fr‚ÜíEn 14 BLEU-mb test 35.0 64 1.81 2.53 3.47 3.13 20.6 15.1 21.8 21.2 1.28 15.9 23.7 26.3 29.0 30.5 30.2 33.7 4.98 25.5 28.5 31.1 33.7 34.9 36.6 39.2 Fr‚ÜíEn 14 BLEU-sb test 64 2.29 2.99 3.90 3.60 21.2 15.5 22.4 21.9 1.50 16.3 24.4 27.0 30.0 31.6 31.4 35.6 5.30 26.2 29.5 32.2 35.1 36.4 38.3 41.4 En‚ÜíFr 14 BLEU-mb test 45.6 64 1.74 2.16 2.73 2.15 15.1 8.82 12.0 25.2 0.49 8.00 14.8 15.9 20.3 23.3 24.9 28.3 4.08 14.5 19.3 21.5 24.9 27.3 29.5 32.6 En‚ÜíFr 14 BLEU-sb test 45.9 64 2.44 2.75 3.54 2.82 19.3 11.4 15.3 31.3 0.81 10.0 18.2 19.3 24.7 28.3 30.1 34.1 5.31 18.0 23.6 26.1 30.3 33.3 35.5 39.9 De‚ÜíEn 16 BLEU-mb test 40.2 64 2.06 2.87 3.41 3.63 21.5 17.3 23.0 27.2 0.83 16.2 22.5 24.7 28.2 30.7 33.0 30.4
BLEU-sb test 45.9 64 2.44 2.75 3.54 2.82 19.3 11.4 15.3 31.3 0.81 10.0 18.2 19.3 24.7 28.3 30.1 34.1 5.31 18.0 23.6 26.1 30.3 33.3 35.5 39.9 De‚ÜíEn 16 BLEU-mb test 40.2 64 2.06 2.87 3.41 3.63 21.5 17.3 23.0 27.2 0.83 16.2 22.5 24.7 28.2 30.7 33.0 30.4 3.25 22.7 26.2 29.2 32.7 34.8 37.3 40.6 De‚ÜíEn 16 BLEU-sb test 64 2.39 3.27 3.85 4.04 22.5 18.2 24.4 28.6 0.93 17.1 23.4 25.8 29.2 31.9 34.5 32.1 3.60 23.8 27.5 30.5 34.1 36.5 39.1 43.0 En‚ÜíDe 16 BLEU-mb test 41.2 64 1.70 2.27 2.31 2.43 12.9 8.66 10.4 24.6 0.50 7.00 12.9 13.1 18.3 20.9 22.5 26.2 3.42 12.3 15.4 17.1 20.9 23.0 26.6 29.7 En‚ÜíDe 16 BLEU-sb test 41.2 64 2.09 2.65 2.75 2.92 13.7 9.36 11.0 25.3 0.54 7.40 13.4 13.4 18.8 21.7 23.3 27.3 3.78 12.9 16.1 17.7 21.7 24.1 27.7 30.9 Winograd acc test 93.8 7 66.3 72.9 74.7 76.9 82.4 85.7 87.9 88.3 63.4 68.5 72.9 76.9 82.4 84.6 86.1 89.7 63.4 67.4 73.6 76.9 84.3 85.4 82.4 88.6 Winogrande acc dev 84.6 50 52.0 52.1 57.4 58.7 62.3 64.5 67.9 70.2 51.3 53.0 58.3 59.1 61.7 65.8 66.9 73.2 51.3 52.6 57.5
93.8 7 66.3 72.9 74.7 76.9 82.4 85.7 87.9 88.3 63.4 68.5 72.9 76.9 82.4 84.6 86.1 89.7 63.4 67.4 73.6 76.9 84.3 85.4 82.4 88.6 Winogrande acc dev 84.6 50 52.0 52.1 57.4 58.7 62.3 64.5 67.9 70.2 51.3 53.0 58.3 59.1 61.7 65.8 66.9 73.2 51.3 52.6 57.5 59.1 62.6 67.4 70.0 77.7 PIQA acc dev 77.1 50 64.6 70.2 72.9 75.1 75.6 78.0 78.5 81.0 64.3 69.3 71.8 74.4 74.3 76.3 77.8 80.5 64.3 69.4 72.0 74.3 75.4 77.8 79.9 82.3 82.8 ARC (Challenge) acc test 78.5 50 26.6 29.5 31.8 35.5 38.0 41.4 43.7 51.4 25.5 30.2 31.6 36.4 38.4 41.5 43.1 53.2 25.5 28.4 32.3 36.7 39.5 43.7 44.8 51.5 ARC (Easy) acc test 92.0 50 43.6 46.5 53.0 53.8 58.2 60.2 63.8 68.8 42.7 48.2 54.6 55.9 60.3 62.6 66.8 71.2 42.7 51.0 58.1 59.1 62.1 65.8 69.1 70.1 OpenBookQA acc test 87.2 100 35.6 43.2 45.2 46.8 53.0 50.4 55.6 57.6 37.0 39.8 46.2 46.4 53.4 53.0 55.8 58.8 37.0 43.6 48.0 50.6 55.6 55.2 60.8 65.4 Quac f1 dev 74.4 5 21.2 26.8 31.0 30.1 34.7 36.1 38.4 41.5 21.1 26.9 31.9 32.3 37.4 39.0 40.6 43.4 21.6 27.6 32.9 34.2 38.2
35.6 43.2 45.2 46.8 53.0 50.4 55.6 57.6 37.0 39.8 46.2 46.4 53.4 53.0 55.8 58.8 37.0 43.6 48.0 50.6 55.6 55.2 60.8 65.4 Quac f1 dev 74.4 5 21.2 26.8 31.0 30.1 34.7 36.1 38.4 41.5 21.1 26.9 31.9 32.3 37.4 39.0 40.6 43.4 21.6 27.6 32.9 34.2 38.2 39.9 40.9 44.3 RACE-h acc test 90.0 10 35.2 37.9 40.1 40.9 42.4 44.1 44.6 45.5 34.3 37.7 40.0 42.0 43.8 44.3 44.6 45.9 34.3 37.0 40.4 41.4 42.3 44.7 45.1 46.8 RACE-m acc test 93.1 10 42.1 47.2 52.1 52.3 54.7 54.4 56.7 58.4 42.3 47.3 51.7 55.2 56.1 54.7 56.9 57.4 42.3 47.0 52.7 53.0 55.6 55.4 58.1 58.1 SQuADv2 em dev 90.7 16 22.6 32.8 33.9 43.1 43.6 45.4 49.0 52.6 25.1 37.5 37.9 47.9 47.9 51.1 56.0 60.1 27.5 40.5 39.2 53.5 50.0 56.6 62.6 64.9 SQuADv2 f1 dev 93.0 16 28.3 40.2 41.4 50.3 51.0 52.7 56.3 59.5 30.1 43.6 44.1 54.0 54.1 57.1 61.8 65.4 32.1 45.5 44.9 58.7 55.9 62.1 67.7 69.8 CoQA f1 dev 90.7 5 34.5 55.0 61.8 65.3 71.1 72.8 76.3 81.5 30.6 52.1 61.6 66.1 71.8 75.1 77.9 84.0 31.1 52.0 62.7 66.8 73.2 77.3 79.9 85.0 DROP f1
52.7 56.3 59.5 30.1 43.6 44.1 54.0 54.1 57.1 61.8 65.4 32.1 45.5 44.9 58.7 55.9 62.1 67.7 69.8 CoQA f1 dev 90.7 5 34.5 55.0 61.8 65.3 71.1 72.8 76.3 81.5 30.6 52.1 61.6 66.1 71.8 75.1 77.9 84.0 31.1 52.0 62.7 66.8 73.2 77.3 79.9 85.0 DROP f1 dev 89.1 20 9.40 13.6 14.4 16.4 19.7 17.0 24.0 23.6 11.7 18.1 20.9 23.0 26.4 27.3 29.2 34.3 12.9 18.7 24.0 25.6 29.7 29.7 32.3 36.5 BoolQ acc dev 91.0 32 49.7 60.3 58.9 62.4 67.1 65.4 66.2 60.5 52.6 61.7 60.4 63.7 68.4 68.7 69.0 76.7 43.1 60.6 62.0 64.1 70.3 70.0 70.2 77.5 76.4 CB acc dev 96.9 32 0.00 32.1 8.93 19.6 19.6 28.6 19.6 46.4 55.4 53.6 53.6 48.2 57.1 33.9 55.4 64.3 42.9 58.9 53.6 69.6 67.9 60.7 66.1 82.1 75.6 CB f1 dev 93.9 32 0.00 29.3 11.4 17.4 22.4 25.1 20.3 42.8 60.1 39.8 45.6 37.5 45.7 28.5 44.6 52.5 26.1 40.4 32.6 48.3 45.7 44.6 46.0 57.2 52.0 Copa acc dev 94.8 32 66.0 68.0 73.0 77.0 76.0 80.0 84.0 91.0 62.0 64.0 66.0 74.0 76.0 82.0 86.0 87.0 67.0 64.0 72.0 77.0 83.0 83.0 86.0 92.0 92.0 RTE acc dev
60.1 39.8 45.6 37.5 45.7 28.5 44.6 52.5 26.1 40.4 32.6 48.3 45.7 44.6 46.0 57.2 52.0 Copa acc dev 94.8 32 66.0 68.0 73.0 77.0 76.0 80.0 84.0 91.0 62.0 64.0 66.0 74.0 76.0 82.0 86.0 87.0 67.0 64.0 72.0 77.0 83.0 83.0 86.0 92.0 92.0 RTE acc dev 92.5 32 47.7 49.8 48.4 56.0 46.6 55.2 62.8 63.5 53.1 47.3 49.5 49.5 54.9 54.9 56.3 70.4 52.3 48.4 46.9 50.9 56.3 49.5 60.6 72.9 69.0 WiC acc dev 76.1 32 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 50.0 50.3 50.3 49.2 49.4 50.3 50.0 48.6 49.8 55.0 53.0 53.0 51.6 53.1 51.1 55.3 49.4 WSC acc dev 93.8 32 59.6 56.7 65.4 61.5 66.3 60.6 64.4 65.4 58.7 58.7 60.6 62.5 66.3 60.6 66.3 69.2 58.7 60.6 54.8 49.0 62.5 67.3 75.0 75.0 80.1 MultiRC acc dev 62.3 32 4.72 9.65 12.3 13.6 14.3 18.4 24.2 27.6 4.72 9.65 12.3 13.6 14.3 18.4 24.2 27.6 6.09 11.8 16.8 20.8 24.7 23.8 25.0 32.5 30.5 MultiRC f1a dev 88.2 32 57.0 59.7 60.4 59.9 60.0 64.5 71.4 72.9 57.0 59.7 60.4 59.9 60.0 64.5 71.4 72.9 45.0 55.9 64.2 65.4 69.5 66.4 69.3 74.8 75.4 ReCoRD acc dev
4.72 9.65 12.3 13.6 14.3 18.4 24.2 27.6 6.09 11.8 16.8 20.8 24.7 23.8 25.0 32.5 30.5 MultiRC f1a dev 88.2 32 57.0 59.7 60.4 59.9 60.0 64.5 71.4 72.9 57.0 59.7 60.4 59.9 60.0 64.5 71.4 72.9 45.0 55.9 64.2 65.4 69.5 66.4 69.3 74.8 75.4 ReCoRD acc dev 92.5 32 70.8 78.5 82.1 84.1 86.2 88.6 89.0 90.2 69.8 77.0 80.7 83.0 85.9 88.0 88.8 90.2 69.8 77.2 81.3 83.1 86.6 87.9 88.9 89.0 90.2 ReCoRD f1 dev 93.3 32 71.9 79.2 82.8 85.2 87.3 89.5 90.4 91.0 70.7 77.8 81.6 83.9 86.8 88.8 89.7 91.2 70.7 77.9 82.1 84.0 87.5 88.8 89.8 90.1 91.1 SuperGLUE average dev 89.0 40.6 47.4 46.8 49.6 50.1 52.3 54.4 58.2 54.4 55.1 56.7 57.8 61.2 59.7 64.3 68.9 50.2 56.2 56.8 60.0 64.3 63.6 66.9 73.2 71.8 ANLI R1 acc test 73.8 50 33.4 34.2 33.4 33.4 34.2 32.3 33.2 34.6 32.1 31.6 31.9 34.6 30.6 31.6 32.7 32.0 32.1 32.5 30.9 32.5 33.5 33.1 33.3 36.8 ANLI R2 acc test 50.7 50 33.2 31.9 33.3 33.3 33.8 33.5 33.5 35.4 35.7 33.7 33.2 32.7 32.7 33.9 33.9 33.9 35.7 33.8 32.1 31.4 32.6 33.3 32.6 34.0 ANLI R3 acc test
32.1 31.6 31.9 34.6 30.6 31.6 32.7 32.0 32.1 32.5 30.9 32.5 33.5 33.1 33.3 36.8 ANLI R2 acc test 50.7 50 33.2 31.9 33.3 33.3 33.8 33.5 33.5 35.4 35.7 33.7 33.2 32.7 32.7 33.9 33.9 33.9 35.7 33.8 32.1 31.4 32.6 33.3 32.6 34.0 ANLI R3 acc test 48.3 50 33.6 34.0 33.8 33.4 35.3 34.8 34.4 34.5 35.0 32.6 33.0 33.9 34.1 33.1 32.5 35.1 35.0 34.4 35.1 36.0 32.7 33.9 34.5 40.2 2D+ acc n/a 50 0.70 0.65 0.70 0.85 1.10 2.54 15.4 76.9 2.00 0.55 3.15 4.00 12.1 19.6 73.0 99.6 2.00 4.10 3.50 4.50 8.90 11.9 55.5 100.0 2D- acc n/a 50 1.25 1.25 1.25 1.25 1.60 7.60 12.6 58.0 1.15 0.95 1.45 1.95 3.85 11.5 44.6 86.4 1.15 1.45 2.25 2.70 7.35 13.6 52.4 98.9 3D+ acc n/a 50 0.10 0.10 0.05 0.10 0.10 0.25 1.40 34.2 0.15 0.00 0.10 0.30 0.45 0.95 15.4 65.5 0.15 0.45 0.30 0.55 0.75 0.90 8.40 80.4 3D- acc n/a 50 0.05 0.05 0.05 0.05 0.05 0.45 1.35 48.3 0.05 0.15 0.25 0.30 0.55 1.60 6.15 78.7 0.05 0.10 0.15 0.35 0.65 1.05 9.20 94.2 4D+ acc n/a 50 0.05 0.05 0.00 0.00 0.05 0.05 0.15 4.00
15.4 65.5 0.15 0.45 0.30 0.55 0.75 0.90 8.40 80.4 3D- acc n/a 50 0.05 0.05 0.05 0.05 0.05 0.45 1.35 48.3 0.05 0.15 0.25 0.30 0.55 1.60 6.15 78.7 0.05 0.10 0.15 0.35 0.65 1.05 9.20 94.2 4D+ acc n/a 50 0.05 0.05 0.00 0.00 0.05 0.05 0.15 4.00 0.00 0.00 0.10 0.00 0.00 0.10 0.80 14.0 0.00 0.05 0.05 0.00 0.15 0.15 0.40 25.5 4D- acc n/a 50 0.00 0.00 0.00 0.00 0.00 0.00 0.10 7.50 0.00 0.00 0.00 0.00 0.05 0.00 0.50 14.0 0.00 0.05 0.00 0.00 0.10 0.05 0.40 26.8 5D+ acc n/a 50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.65 0.00 0.00 0.00 0.00 0.00 0.00 0.05 3.45 0.00 0.00 0.00 0.00 0.00 0.00 0.05 9.30 5D- acc n/a 50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.80 0.00 0.00 0.00 0.00 0.00 0.00 0.05 3.75 0.00 0.00 0.00 0.00 0.00 0.00 0.00 9.90 2Dx acc n/a 50 2.20 2.25 2.65 2.10 2.55 5.80 6.15 19.8 1.35 2.35 3.35 2.35 4.75 9.15 11.0 27.4 1.35 2.90 2.70 2.85 4.25 6.10 7.05 29.2 1DC acc n/a 50 1.25 2.95 2.75 0.05 0.30 2.35 0.75 9.75 1.90 2.80 2.85 3.65 6.45 9.15 8.20 14.3 1.70 2.15
2Dx acc n/a 50 2.20 2.25 2.65 2.10 2.55 5.80 6.15 19.8 1.35 2.35 3.35 2.35 4.75 9.15 11.0 27.4 1.35 2.90 2.70 2.85 4.25 6.10 7.05 29.2 1DC acc n/a 50 1.25 2.95 2.75 0.05 0.30 2.35 0.75 9.75 1.90 2.80 2.85 3.65 6.45 9.15 8.20 14.3 1.70 2.15 3.90 5.75 6.20 7.60 9.95 21.3 Cycled Letters acc n/a 100 0.62 0.71 2.85 0.00 0.63 1.35 2.58 3.66 1.67 4.36 5.68 6.46 6.25 9.41 15.1 21.7 4.63 9.27 10.7 14.5 16.7 21.9 27.7 37.9 Anagrams 1 acc n/a 100 0.10 0.14 0.40 0.00 0.27 0.69 1.16 2.28 0.21 0.61 1.12 1.27 1.60 2.72 3.72 8.62 0.50 1.27 2.13 3.05 3.81 5.49 8.38 15.1 Anagrams 2 acc n/a 100 0.81 1.21 2.69 0.01 1.71 3.75 4.53 8.91 1.19 2.62 4.70 4.77 6.97 10.2 14.6 25.9 1.94 4.80 7.59 9.87 12.6 18.9 25.6 39.7 Symbol Insertion acc n/a 100 0.00 0.00 0.10 0.00 0.05 0.42 0.89 8.26 0.03 0.05 0.57 1.18 1.67 3.46 6.62 45.4 0.11 0.28 2.19 4.18 6.61 11.0 27.3 67.2 Reversed Words acc n/a 100 0.00 0.01 0.01 0.01 0.02 0.03 0.03 0.09 0.02 0.01 0.01 0.00 0.05 0.07 0.11 0.48 0.00 0.05 0.00 0.17 0.24 0.30 0.42
0.10 0.00 0.05 0.42 0.89 8.26 0.03 0.05 0.57 1.18 1.67 3.46 6.62 45.4 0.11 0.28 2.19 4.18 6.61 11.0 27.3 67.2 Reversed Words acc n/a 100 0.00 0.01 0.01 0.01 0.02 0.03 0.03 0.09 0.02 0.01 0.01 0.00 0.05 0.07 0.11 0.48 0.00 0.05 0.00 0.17 0.24 0.30 0.42 0.44 SAT Analogies acc n/a 20 35.6 39.0 45.2 44.1 50.0 49.2 52.7 53.7 30.5 41.2 43.1 46.5 55.1 54.3 53.5 59.1 30.5 40.4 42.8 40.6 48.4 51.9 53.5 65.2 Table H.1: Scores for every task, setting and model that we investigate in this paper. 63 Figure H.1: All results for all SuperGLUE tasks. Figure H.2: Results for SAT task. Figure H.3: All results for all Winograd tasks. 64 Figure H.4: All results for all Arithmetic tasks. Figure H.5: All results for all Cloze and Completion tasks. 65 Figure H.6: All results for all Common Sense Reasoning tasks. Figure H.7: All results for all QA tasks. Figure H.8: All results for all Reading Comprehension tasks. Figure H.9: All results for all ANLI rounds. 66 Figure H.10: All results for all Scramble tasks. Figure H.11: All results for all Translation tasks. 67 References [ADG+16] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew
All results for all QA tasks. Figure H.8: All results for all Reading Comprehension tasks. Figure H.9: All results for all ANLI rounds. 66 Figure H.10: All results for all Scramble tasks. Figure H.11: All results for all Translation tasks. 67 References [ADG+16] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient descent. In Advances in neural information processing systems, pages 3981‚Äì3989, 2016. [AI19] WeChat AI. Tr-mt (ensemble), December 2019. [AJF19] Roee Aharoni, Melvin Johnson, and Orhan Firat. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019. [BBDIW20] Su Lin Blodgett, Solon Barocas, Hal Daum¬¥e III, and Hanna Wallach. Language (technology) is power: A critical survey of ‚Äúbias‚Äù in nlp. arXiv preprint arXiv:2005.14050, 2020. [BCFL13] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533‚Äì1544, 2013. [BDD+09] Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo
nlp. arXiv preprint arXiv:2005.14050, 2020. [BCFL13] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533‚Äì1544, 2013. [BDD+09] Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The Ô¨Åfth PASCAL recognizing textual entailment challenge. 2009. [BES10] Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. Sentiwordnet 3.0: an enhanced lexical resource for sentiment analysis and opinion mining. In Lrec, volume 10, pages 2200‚Äì2204, 2010. [BHDD+06] Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. The second PASCAL recognising textual entailment challenge. 2006. [BHT+20] Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, et al. Experience grounds language. arXiv preprint arXiv:2004.10151, 2020. [BLC13] Yoshua Bengio, Nicholas L¬¥eonard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. Arxiv, 2013. [BZB+19] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. arXiv preprint arXiv:1911.11641, 2019. [Car97] Rich Caruana. Multitask learning. Machine learning, 28(1), 1997. [CB78] Susan Carey and
Estimating or propagating gradients through stochastic neurons for conditional computation. Arxiv, 2013. [BZB+19] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. arXiv preprint arXiv:1911.11641, 2019. [Car97] Rich Caruana. Multitask learning. Machine learning, 28(1), 1997. [CB78] Susan Carey and Elsa Bartlett. Acquiring a single new word. Proceedings of the Stanford Child Language Conference, 1978. [CCE+18] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457, 2018. [CGRS19] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers, 2019. [CHI+18] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. Quac : Question answering in context. Arxiv, 2018. [CLC+19] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difÔ¨Åculty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. [CLY+19] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740, 2019. [Cra17] Kate Crawford. The trouble with
and Kristina Toutanova. BoolQ: Exploring the surprising difÔ¨Åculty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. [CLY+19] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740, 2019. [Cra17] Kate Crawford. The trouble with bias. NIPS 2017 Keynote, 2017. [DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 68 [DGM06] Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment challenge. In Machine learning challenges. evaluating predictive uncertainty, visual object classiÔ¨Åcation, and recognising textual entailment, pages 177‚Äì190. Springer, 2006. [DGV+18] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers. Arxiv, 2018. [DHKH14] Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth HeaÔ¨Åeld. Edinburgh‚Äôs phrase-based machine translation systems for wmt-14. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 97‚Äì104, 2014. [DL15] Andrew M. Dai and Quoc V. Le. Semi-supervised sequence learning. In Advances in neural information processing systems, 2015. [DMST19] Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The CommitmentBank: Investigat- ing projection in naturally occurring discourse. 2019. To appear in proceedings of
on Statistical Machine Translation, pages 97‚Äì104, 2014. [DL15] Andrew M. Dai and Quoc V. Le. Semi-supervised sequence learning. In Advances in neural information processing systems, 2015. [DMST19] Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The CommitmentBank: Investigat- ing projection in naturally occurring discourse. 2019. To appear in proceedings of Sinn und Bedeutung 23. Data can be found at https://github.com/mcdm/CommitmentBank/. [DSC+16] Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2: Fast reinforcement learning via slow reinforcement learning. ArXiv, abs/1611.02779, 2016. [DWD+19] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161, 2019. [DYY+19] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a Ô¨Åxed-length context. Arxiv, 2019. [EOAG18] Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-translation at scale. arXiv preprint arXiv:1808.09381, 2018. [FAL17] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. ArXiv, abs/1703.03400, 2017. [Fyo00] Yaroslav Fyodorov. A natural logic inference system, 2000. [GG19] Hila Gonen and Yoav Goldberg. Lipstick on a pig: Debiasing methods cover up
Understanding back-translation at scale. arXiv preprint arXiv:1808.09381, 2018. [FAL17] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. ArXiv, abs/1703.03400, 2017. [Fyo00] Yaroslav Fyodorov. A natural logic inference system, 2000. [GG19] Hila Gonen and Yoav Goldberg. Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them. arXiv preprint arXiv:1903.03862, 2019. [GLT+20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval- augmented language model pre-training. arXiv preprint arXiv:2002.08909, 2020. [GMDD07] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pages 1‚Äì9. Association for Computational Linguistics, 2007. [Gra16] Alex Graves. Adaptive computation time for recurrent neural networks. Arxiv, 2016. [GSL+18] Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R Bowman, and Noah A Smith. Annotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324, 2018. [GSR19] Sebastian Gehrmann, Hendrik Strobelt, and Alexander M. Rush. Gltr: Statistical detection and visualiza- tion of generated text. arXiv preprint arXiv: 1906.04043, 2019. [GWC+18] Jiatao Gu, Yong Wang, Yun Chen, Kyunghyun Cho, and Victor OK Li. Meta-learning for low-resource
Annotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324, 2018. [GSR19] Sebastian Gehrmann, Hendrik Strobelt, and Alexander M. Rush. Gltr: Statistical detection and visualiza- tion of generated text. arXiv preprint arXiv: 1906.04043, 2019. [GWC+18] Jiatao Gu, Yong Wang, Yun Chen, Kyunghyun Cho, and Victor OK Li. Meta-learning for low-resource neural machine translation. arXiv preprint arXiv:1808.08437, 2018. [HB20] Daniel Hernandez and Tom Brown. Ai and efÔ¨Åciency, May 2020. [HBFC19] Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. CoRR, abs/1904.09751, 2019. [HLW+20] Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song. Pretrained transformers improve out of distribution robustness. arXiv preprint arXiv:2004.06100, 2020. 69 [HNA+17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017. [HR18] Jeremy Howard and Sebastian Ruder. Universal language model Ô¨Åne-tuning for text classiÔ¨Åcation. arXiv preprint arXiv:1801.06146, 2018. [HVD15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. [HYC01] Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to Learn Using Gradient Descent. In International
Sebastian Ruder. Universal language model Ô¨Åne-tuning for text classiÔ¨Åcation. arXiv preprint arXiv:1801.06146, 2018. [HVD15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. [HYC01] Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to Learn Using Gradient Descent. In International Conference on ArtiÔ¨Åcial Neural Networks, pages 87‚Äì94. Springer, 2001. [HZJ+19] Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl, Jack Rae, Vishal Maini, Dani Yogatama, and Pushmeet Kohli. Reducing sentiment bias in language models via counterfactual evaluation. arXiv preprint arXiv:1911.03064, 2019. [IBGC+14] Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, and Hal Daum¬¥e III. A neural network for factoid question answering over paragraphs. In Empirical Methods in Natural Language Processing, 2014. [IDCBE19] Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. Automatic detection of generated text is easiest when humans are fooled. arXiv preprint arXiv:1911.00650, 2019. [JCWZ17] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. [JN20] Zheng Junyuan and Gamma Lab NYC. Numeric transformer - albert, March 2020. [JVS+16] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the
Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. [JN20] Zheng Junyuan and Gamma Lab NYC. Numeric transformer - albert, March 2020. [JVS+16] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016. [JYS+19] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. TinyBERT: Distilling BERT for natural language understanding. arXiv preprint arXiv:1909.10351, 2019. [JZC+19] Ying Ju, Fubang Zhao, Shijie Chen, Bowen Zheng, Xuefeng Yang, and Yunfeng Liu. Technical report on conversational question answering. arXiv preprint arXiv:1909.10772, 2019. [KCR+18] Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of North American Chapter of the Association for Computational Linguistics (NAACL), 2018. [KKS+20] Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. UniÔ¨Åedqa: Crossing format boundaries with a single qa system. arXiv preprint arXiv:2005.00700, 2020. [KMB20] Sarah E. Kreps, Miles McCain, and Miles Brundage. All the news that‚Äôs Ô¨Åt to fabricate: Ai-generated text as a tool of media
[KKS+20] Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. UniÔ¨Åedqa: Crossing format boundaries with a single qa system. arXiv preprint arXiv:2005.00700, 2020. [KMB20] Sarah E. Kreps, Miles McCain, and Miles Brundage. All the news that‚Äôs Ô¨Åt to fabricate: Ai-generated text as a tool of media misinformation, 2020. [KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. [KPR+19] Tom Kwiatkowski, Jennimaria Palomaki, Olivia RedÔ¨Åeld, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural ques- tions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019. [KR16] Yoon Kim and Alexander M. Rush. Sequence-level knowledge distillation. Arxiv, 2016. [LB02] Edward Loper and Steven Bird. Nltk: The natural language toolkit, 2002. [LC19] Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. arXiv preprint arXiv:1901.07291, 2019. 70 [LCG+19] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori- cut. ALBERT: A lite BERT for self-supervised learning of
[LB02] Edward Loper and Steven Bird. Nltk: The natural language toolkit, 2002. [LC19] Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. arXiv preprint arXiv:1901.07291, 2019. 70 [LCG+19] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori- cut. ALBERT: A lite BERT for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019. [LCH+20] Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng Gao. Adversarial training for large neural language models. arXiv preprint arXiv:2004.08994, 2020. [LDL19] Zhongyang Li, Xiao Ding, and Ting Liu. Story ending prediction by transferable bert. arXiv preprint arXiv:1905.07504, 2019. [LDM12] Hector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning, 2012. [LGG+20] Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. arXiv preprint arXiv:2001.08210, 2020. [LGH+15] Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-Yi Wang. Representation learning using multi-task deep neural networks for semantic classiÔ¨Åcation and information retrieval. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human
translation. arXiv preprint arXiv:2001.08210, 2020. [LGH+15] Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-Yi Wang. Representation learning using multi-task deep neural networks for semantic classiÔ¨Åcation and information retrieval. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2015. [LH17] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [LHCG19a] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Improving multi-task deep neural networks via knowledge distillation for natural language understanding. arXiv preprint arXiv:1904.09482, 2019. [LHCG19b] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for natural language understanding. arXiv preprint arXiv:1901.11504, 2019. [Lin20] Tal Linzen. How can we accelerate progress towards human-like linguistic generalization? arXiv preprint arXiv:2005.00955, 2020. [LLG+19] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. [LM17] Ke Li and Jitendra Malik. Learning to optimize neural nets. arXiv preprint arXiv:1703.00441, 2017. [LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin
Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. [LM17] Ke Li and Jitendra Malik. Learning to optimize neural nets. arXiv preprint arXiv:1703.00441, 2017. [LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019. [LPP+20] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K¬®uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¬®aschel, Sebastian Riedel, and Kiela Douwe. Retrieval-augmented generation for knowledge-intensive nlp tasks. arXiv preprint arXiv:2005.11401, 2020. [LSP+18] Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating Wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198, 2018. [LWS+20] Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joseph E. Gonzalez. Train large, then compress: Rethinking model size for efÔ¨Åcient training and inference of transformers, 2020. [LXL+17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017. [LYN+20] Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira, Ming-Feng Tsai, Chuan-Ju Wang, and Jimmy Lin. Tttttackling winogrande schemas. arXiv preprint arXiv:2003.08380,
efÔ¨Åcient training and inference of transformers, 2020. [LXL+17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017. [LYN+20] Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira, Ming-Feng Tsai, Chuan-Ju Wang, and Jimmy Lin. Tttttackling winogrande schemas. arXiv preprint arXiv:2003.08380, 2020. [Mac92] David. MacKay. Information-based objective functions for active data selection. Neural Computation, 1992. 71 [MBXS17] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Con- textualized word vectors. In Advances in Neural Information Processing Systems, pages 6294‚Äì6305, 2017. [MCCD13] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. EfÔ¨Åcient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. [MCH+16] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. A corpus and evaluation framework for deeper understanding of commonsense stories. arXiv preprint arXiv:1604.01696, 2016. [MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. ArXiv, abs/1809.02789, 2018. [MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of large-batch training, 2018. [MKM+94] Mitchell Marcus, Grace Kim, Mary Ann
Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. ArXiv, abs/1809.02789, 2018. [MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of large-batch training, 2018. [MKM+94] Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. The penn treebank: annotating predicate argument structure. In Proceedings of the workshop on Human Language Technology, pages 114‚Äì119. Association for Computational Linguistics, 1994. [MKXS18] Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018. [MPL19] R Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. arXiv preprint arXiv:1902.01007, 2019. [MWZ+18] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting, 2018. [NBR20] Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv:2004.09456, 2020. [NK19] Timothy Niven and Hung-Yu Kao. Probing neural network comprehension of natural language arguments. arXiv preprint arXiv:1907.07355, 2019. [Nor09]
Deborah Raji, and Timnit Gebru. Model cards for model reporting, 2018. [NBR20] Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv:2004.09456, 2020. [NK19] Timothy Niven and Hung-Yu Kao. Probing neural network comprehension of natural language arguments. arXiv preprint arXiv:1907.07355, 2019. [Nor09] Peter Norvig. Natural language corpus data, 2009. [NvNvdG19] Malvina Nissim, Rik van Noord, and Rob van der Goot. Fair is better than sensational: Man is to doctor as woman is to doctor. arXiv preprint arXiv:1905.09866, 2019. [NWD+19] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli: A new benchmark for natural language understanding. arXiv preprint arXiv:1910.14599, 2019. [oR16] University of Regensburg. Fascha, 2016. [PCC18] Mohammad Taher Pilehvar and Jose Camacho-Collados. WIC: 10,000 example pairs for evaluating context-sensitive representations. arXiv preprint arXiv:1808.09121, 2018. [PFB18] Jason Phang, Thibault F¬¥evry, and Samuel R. Bowman. Sentence encoders on STILTs: Supplementary training on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088, 2018. [PHR+18] Adam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White, and Benjamin Van Durme. Collecting diverse natural language inference problems for sentence representation evaluation. In Proceedings of EMNLP, 2018. [PKL+16] Denis Paperno, Germ¬¥an Kruszewski,
STILTs: Supplementary training on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088, 2018. [PHR+18] Adam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White, and Benjamin Van Durme. Collecting diverse natural language inference problems for sentence representation evaluation. In Proceedings of EMNLP, 2018. [PKL+16] Denis Paperno, Germ¬¥an Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern¬¥andez. The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016. [PNZtY18] Matthew E. Peters, Mark Neumann, Luke Zettlemoyer, and Wen tau Yih. Dissecting contextual word embeddings: Architecture and representation, 2018. [Pos18] Matt Post. A call for clarity in reporting BLEU scores. arXiv preprint arXiv:1804.08771, 2018. 72 [PSM14] Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), 2014. [QIA20] QIANXIN. Sa-net on albert (ensemble), April 2020. [QMZH19] Yusu Qian, Urwa Muaz, Ben Zhang, and Jae Won Hyun. Reducing gender bias in word-level language models with a gender-equalizing loss function. arXiv preprint arXiv:1905.12801, 2019. [RBG11] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning.
(ensemble), April 2020. [QMZH19] Yusu Qian, Urwa Muaz, Ben Zhang, and Jae Won Hyun. Reducing gender bias in word-level language models with a gender-equalizing loss function. arXiv preprint arXiv:1905.12801, 2019. [RBG11] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011. [RCM19] Siva Reddy, Danqi Chen, and Christopher D Manning. Coqa: A conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249‚Äì266, 2019. [RCP+17] Scott Reed, Yutian Chen, Thomas Paine, A¬®aron van den Oord, SM Eslami, Danilo Rezende, Oriol Vinyals, and Nando de Freitas. Few-shot autoregressive density estimation: Towards learning to learn distributions. arXiv preprint arXiv:1710.10304, 2017. [RJL18] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don‚Äôt know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822, 2018. [RL16] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. ICLR 2017 (oral), 2016. [RLL+19] Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. NumNet: Machine reading comprehension with numerical reasoning. In Proceedings of EMNLP, 2019. [RNLVD18] Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in coreference resolution. arXiv preprint arXiv:1804.09301, 2018. [RNSS18] Alec Radford,
2017 (oral), 2016. [RLL+19] Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. NumNet: Machine reading comprehension with numerical reasoning. In Proceedings of EMNLP, 2019. [RNLVD18] Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in coreference resolution. arXiv preprint arXiv:1804.09301, 2018. [RNSS18] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training, 2018. [Ros12] R.S. Ross. Guide for conducting risk assessments. NIST Special Publication, 2012. [RRBS19] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of the generalization error across scales, 2019. [RRS20] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? arXiv preprint arXiv:2002.08910, 2020. [RSR+19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniÔ¨Åed text-to-text transformer, 2019. [RWC+19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. [SBBC19] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019. [SBC+19] Irene Solaiman, Miles
a uniÔ¨Åed text-to-text transformer, 2019. [RWC+19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. [SBBC19] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019. [SBC+19] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, Miles McCain, Alex Newhouse, Jason Blazakis, Kris McGufÔ¨Åe, and Jasmine Wang. Release strategies and the social impacts of language models, 2019. [SCNP19] Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. The woman worked as a babysitter: On biases in language generation. arXiv preprint arXiv:1909.01326, 2019. [SDCW19] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019. [SDSE19] Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green AI. CoRR, abs/1907.10597, 2019. [SHB15] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models with monolingual data. arXiv preprint arXiv:1511.06709, 2015. 73 [SMM+17] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.
Green AI. CoRR, abs/1907.10597, 2019. [SHB15] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models with monolingual data. arXiv preprint arXiv:1511.06709, 2015. 73 [SMM+17] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. [SPP+19] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism, 2019. [SS20] Timo Schick and Hinrich Sch¬®utze. Exploiting cloze questions for few-shot text classiÔ¨Åcation and natural language inference. arXiv preprint arXiv:2001.07676, 2020. [STQ+19] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. MASS: Masked sequence to sequence pre-training for language generation. arXiv preprint arXiv:1905.02450, 2019. [TFR+17] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pages 23‚Äì30. IEEE, 2017. [TL05] Peter D. Turney and Michael L. Littman. Corpus-based learning of analogies and semantic relations. CoRR, abs/cs/0508103, 2005. [TL18] Trieu H. Trinh and Quoc V. Le. A simple method for commonsense reasoning. arXiv preprint
world. In 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pages 23‚Äì30. IEEE, 2017. [TL05] Peter D. Turney and Michael L. Littman. Corpus-based learning of analogies and semantic relations. CoRR, abs/cs/0508103, 2005. [TL18] Trieu H. Trinh and Quoc V. Le. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847, 2018. [TLBS03] Peter D. Turney, Michael L. Littman, Jeffrey Bigham, and Victor Shnayder. Combining independent modules to solve multiple-choice synonym and analogy problems. CoRR, cs.CL/0309035, 2003. [Tur20] Project Turing. Microsoft research blog, Feb 2020. [VBL+16] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching Networks for One Shot Learning. In Advances in neural information processing systems, pages 3630‚Äì3638, 2016. [VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, 2017. [WPN+19] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understand- ing systems. In Advances in Neural Information Processing Systems, pages 3261‚Äì3275, 2019. [WXH+18] Yiren Wang, Yingce Xia, Tianyu He, Fei Tian, Tao Qin, ChengXiang Zhai, and Tie-Yan Liu. Multi-agent dual
Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understand- ing systems. In Advances in Neural Information Processing Systems, pages 3261‚Äì3275, 2019. [WXH+18] Yiren Wang, Yingce Xia, Tianyu He, Fei Tian, Tao Qin, ChengXiang Zhai, and Tie-Yan Liu. Multi-agent dual learning. ICLR 2019, 2018. [XDH+19] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le. Unsupervised data augmentation for consistency training, 2019. [YdC+19] Dani Yogatama, Cyprien de Masson d‚ÄôAutume, Jerome Connor, Tomas Kocisky, Mike Chrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, et al. Learning and evaluating general linguistic intelligence. arXiv preprint arXiv:1901.11373, 2019. [YDY+19] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. XLNet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019. [ZHB+19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really Ô¨Ånish your sentence? arXiv preprint arXiv:1905.07830, 2019. [ZHR+19] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news. arXiv preprint arXiv:1905.12616, 2019. [ZLL+18] Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. ReCoRD:
machine really Ô¨Ånish your sentence? arXiv preprint arXiv:1905.07830, 2019. [ZHR+19] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news. arXiv preprint arXiv:1905.12616, 2019. [ZLL+18] Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. ReCoRD: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint arXiv:1810.12885, 2018. [ZSW+19a] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences, 2019. 74 [ZSW+19b] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Chris- tiano, and Geoffrey Irving. Fine-tuning language models from human preferences. ArXiv, abs/1909.08593, 2019. 75
The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits Shuming Ma‚àóHongyu Wang‚àóLingxiao Ma Lei Wang Wenhui Wang Shaohan Huang Li Dong Ruiping Wang Jilong Xue Furu Wei‚ãÑ https://aka.ms/GeneralAI Abstract Recent research, such as BitNet [WMD+23], is paving the way for a new era of 1- bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs. 0.2961 -0.0495 0.0413 ... ‚Ä¶ -0.4765 0.2812 0.2403 -0.1808 0.1304 -0.4809 ‚Ä¶ ‚Ä¶ -0.1771 -0.1741 -0.3853 Transformer LLMs 16-bit Float (FP16/BF16) Cost Performance 1 -1 0 ‚Ä¶ ‚Ä¶ 1 -1 -1 -1 1 -1 ‚Ä¶ ‚Ä¶
computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs. 0.2961 -0.0495 0.0413 ... ‚Ä¶ -0.4765 0.2812 0.2403 -0.1808 0.1304 -0.4809 ‚Ä¶ ‚Ä¶ -0.1771 -0.1741 -0.3853 Transformer LLMs 16-bit Float (FP16/BF16) Cost Performance 1 -1 0 ‚Ä¶ ‚Ä¶ 1 -1 -1 -1 1 -1 ‚Ä¶ ‚Ä¶ 0 0 -1 BitNet b1.58 (This Work) {-1, 0, 1} W= Pareto Improvement W= 0.0413 0.3397 0.2812 0.2403 -0.1808 0.1304 -0.4809 0.3244 0.4322 -0.1771 -0.1741 -0.3853 0.2961 -0.0495 -0.0924 -0.4765 ùíôùüé ùíôùüè ùíôùüê ùíôùüë ùüé. ùüêùüóùüîùüèùíôùüé‚àíùüé. ùüéùüíùüóùüìùíôùüè‚àíùüé. ùüéùüóùüêùüíùíôùüê‚àíùüé. ùüíùüïùüîùüìùíôùüë ‚Ä¶ 1 -1 0 1 -1 1 -1 -1 -1 0 -1 1 1 -1 1 0 ùíôùüé‚àíùíôùüè‚àíùíôùüê+ ùíôùüë ‚Ä¶ ùíôùüé ùíôùüè ùíôùüê ùíôùüë 1(.58)-bit FP16 Model W Input X Output Y Y = f(W, X) GPU New Hardware Figure 1: 1-bit LLMs (e.g., BitNet b1.58) provide a Pareto solution to reduce inference cost (latency, throughput, and energy) of LLMs while maintaining model performance. The new computation paradigm of BitNet b1.58 calls for actions to design new hardware optimized for 1-bit LLMs. ‚àóEqual contribution. ‚ãÑCorresponding author. S. Ma, L. Ma, L. Wang, W. Wang, S. Huang, L. Dong, J. Xue, F. Wei are with Microsoft Research. H. Wang and R. Wang
maintaining model performance. The new computation paradigm of BitNet b1.58 calls for actions to design new hardware optimized for 1-bit LLMs. ‚àóEqual contribution. ‚ãÑCorresponding author. S. Ma, L. Ma, L. Wang, W. Wang, S. Huang, L. Dong, J. Xue, F. Wei are with Microsoft Research. H. Wang and R. Wang are with University of Chinese Academy of Sciences. arXiv:2402.17764v1 [cs.CL] 27 Feb 2024 1 The Era of 1-bit LLMs In recent years, the field of AI has seen a rapid growth in the size and capabilities of Large Language Models (LLMs). These models have demonstrated remarkable performance in a wide range of natural language processing tasks, but their increasing size has posed challenges for deployment and raised concerns about their environmental and economic impact due to high energy consumption. One approach to address these challenges is to use post-training quantization to create low-bit models for inference [XLS+23, FAHA23, CCKS23, TCS+24]. This technique reduces the precision of weights and activations, significantly reducing the memory and computational requirements of LLMs. The trend has been to move from 16 bits to lower bits, such as 4-bit variants [FAHA23, LTT+23]. However, post-training quantization is sub-optimal, even though it is widely used in industry LLMs.
This technique reduces the precision of weights and activations, significantly reducing the memory and computational requirements of LLMs. The trend has been to move from 16 bits to lower bits, such as 4-bit variants [FAHA23, LTT+23]. However, post-training quantization is sub-optimal, even though it is widely used in industry LLMs. Recent work on 1-bit model architectures, such as BitNet [WMD+23], presents a promising direction for reducing the cost of LLMs while maintaining their performance. Vanilla LLMs are in 16-bit floating values (i.e., FP16 or BF16), and the bulk of any LLMs is matrix multiplication. Therefore, the major computation cost comes from the floating-point addition and multiplication operations. In contrast, the matrix multiplication of BitNet only involves integer addition, which saves orders of energy cost for LLMs. As the fundamental limit to compute performance in many chips is power, the energy savings can also be translated into faster computation. In addition to computation, the process of transferring model parameters from DRAM to the memory of an on-chip accelerator (e.g., SRAM) can be expensive during inference. There have been attempts to enlarge SRAM to improve throughput, but this introduces significantly higher costs than DRAM. Compared to full-precision models, 1-bit LLMs have a
to computation, the process of transferring model parameters from DRAM to the memory of an on-chip accelerator (e.g., SRAM) can be expensive during inference. There have been attempts to enlarge SRAM to improve throughput, but this introduces significantly higher costs than DRAM. Compared to full-precision models, 1-bit LLMs have a much lower memory footprint from both a capacity and bandwidth standpoint. This can significantly reduce the cost and time of loading weights from DRAM, leading to faster and more efficient inference. In this work, we introduce a significant 1-bit LLM variant called BitNet b1.58, where every parameter is ternary, taking on values of {-1, 0, 1}. We have added an additional value of 0 to the original 1-bit BitNet, resulting in 1.58 bits in the binary system. BitNet b1.58 retains all the benefits of the original 1-bit BitNet, including its new computation paradigm, which requires almost no multiplication operations for matrix multiplication and can be highly optimized. Additionally, it has the same energy consumption as the original 1-bit BitNet and is much more efficient in terms of memory consumption, throughput and latency compared to FP16 LLM baselines. Furthermore, BitNet b1.58 offers two additional advantages. Firstly, its modeling capability is stronger
matrix multiplication and can be highly optimized. Additionally, it has the same energy consumption as the original 1-bit BitNet and is much more efficient in terms of memory consumption, throughput and latency compared to FP16 LLM baselines. Furthermore, BitNet b1.58 offers two additional advantages. Firstly, its modeling capability is stronger due to its explicit support for feature filtering, made possible by the inclusion of 0 in the model weights, which can significantly improve the performance of 1-bit LLMs. Secondly, our experiments show that BitNet b1.58 can match full precision (i.e., FP16) baselines in terms of both perplexity and end-task performance, starting from a 3B size, when using the same configuration (e.g., model size, training tokens, etc.). 2 BitNet b1.58 BitNet b1.58 is based on the BitNet architecture, which is a Transformer that replaces nn.Linear with BitLinear. It is trained from scratch, with 1.58-bit weights and 8-bit activations. Compared to the original BitNet, it introduces some modifications that we summarize below. Quantization Function. To constrain the weights to -1, 0, or +1, we adopt an absmean quantization function. It first scales the weight matrix by its average absolute value, and then round each value to the nearest integer among {-1, 0,
original BitNet, it introduces some modifications that we summarize below. Quantization Function. To constrain the weights to -1, 0, or +1, we adopt an absmean quantization function. It first scales the weight matrix by its average absolute value, and then round each value to the nearest integer among {-1, 0, +1}: f W = RoundClip( W Œ≥ + œµ, ‚àí1, 1), (1) RoundClip(x, a, b) = max(a, min(b, round(x))), (2) Œ≥ = 1 nm X ij |Wij|. (3) The quantization function for activations follows the same implementation in BitNet, except that we do not scale the activations before the non-linear functions to the range [0, Qb]. Instead, the 2 Models Size Memory (GB)‚Üì Latency (ms)‚Üì PPL‚Üì LLaMA LLM 700M 2.08 (1.00x) 1.18 (1.00x) 12.33 BitNet b1.58 700M 0.80 (2.60x) 0.96 (1.23x) 12.87 LLaMA LLM 1.3B 3.34 (1.00x) 1.62 (1.00x) 11.25 BitNet b1.58 1.3B 1.14 (2.93x) 0.97 (1.67x) 11.29 LLaMA LLM 3B 7.89 (1.00x) 5.07 (1.00x) 10.04 BitNet b1.58 3B 2.22 (3.55x) 1.87 (2.71x) 9.91 BitNet b1.58 3.9B 2.38 (3.32x) 2.11 (2.40x) 9.62 Table 1: Perplexity as well as the cost of BitNet b1.58 and LLaMA LLM. Models Size ARCe ARCc HS BQ OQ PQ WGe Avg. LLaMA LLM 700M 54.7
3B 7.89 (1.00x) 5.07 (1.00x) 10.04 BitNet b1.58 3B 2.22 (3.55x) 1.87 (2.71x) 9.91 BitNet b1.58 3.9B 2.38 (3.32x) 2.11 (2.40x) 9.62 Table 1: Perplexity as well as the cost of BitNet b1.58 and LLaMA LLM. Models Size ARCe ARCc HS BQ OQ PQ WGe Avg. LLaMA LLM 700M 54.7 23.0 37.0 60.0 20.2 68.9 54.8 45.5 BitNet b1.58 700M 51.8 21.4 35.1 58.2 20.0 68.1 55.2 44.3 LLaMA LLM 1.3B 56.9 23.5 38.5 59.1 21.6 70.0 53.9 46.2 BitNet b1.58 1.3B 54.9 24.2 37.7 56.7 19.6 68.8 55.8 45.4 LLaMA LLM 3B 62.1 25.6 43.3 61.8 24.6 72.1 58.2 49.7 BitNet b1.58 3B 61.4 28.3 42.9 61.5 26.6 71.5 59.3 50.2 BitNet b1.58 3.9B 64.2 28.7 44.2 63.5 24.2 73.2 60.5 51.2 Table 2: Zero-shot accuracy of BitNet b1.58 and LLaMA LLM on the end tasks. activations are all scaled to [‚àíQb, Qb] per token to get rid of the zero-point quantization. This is more convenient and simple for both implementation and system-level optimization, while introduces negligible effects to the performance in our experiments. LLaMA-alike Components. The architecture of LLaMA [TLI+23, TMS+23] has been the de- facto backbone for open-source LLMs. To embrace the open-source community, our design of
the zero-point quantization. This is more convenient and simple for both implementation and system-level optimization, while introduces negligible effects to the performance in our experiments. LLaMA-alike Components. The architecture of LLaMA [TLI+23, TMS+23] has been the de- facto backbone for open-source LLMs. To embrace the open-source community, our design of BitNet b1.58 adopts the LLaMA-alike components. Specifically, it uses RMSNorm [ZS19], SwiGLU [Sha20], rotary embedding [SAL+24], and removes all biases. In this way, BitNet b1.58 can be integrated into the popular open-source software (e.g., Huggingface, vLLM [KLZ+23], and llama.cpp2) with minimal efforts. 3 Results We compared BitNet b1.58 to our reproduced FP16 LLaMA LLM in various sizes. To ensure a fair comparison, we pre-trained the models on the RedPajama dataset [Com23] for 100 billion tokens. We evaluated the zero-shot performance on a range of language tasks, including ARC-Easy [YBS19], ARC-Challenge [YBS19], Hellaswag [ZHB+19], Winogrande [SBBC20], PIQA [BZB+19], Open- bookQA [MCKS18], and BoolQ [CLC+19]. We also reported the validation perplexity on the WikiText2 [MXBS16] and C4 [RSR+19] datasets. We compared the runtime GPU memory and latency of both LLaMA LLM and BitNet b1.58. The results were measured using the FasterTransformer3 codebase, which is well-optimized for LLM inference latency on GPU devices.
and BoolQ [CLC+19]. We also reported the validation perplexity on the WikiText2 [MXBS16] and C4 [RSR+19] datasets. We compared the runtime GPU memory and latency of both LLaMA LLM and BitNet b1.58. The results were measured using the FasterTransformer3 codebase, which is well-optimized for LLM inference latency on GPU devices. The 2-bit kernel from Ladder [WMC+23] is also integrated for BitNet b1.58. We reported the time per output token, as it is the major cost for inference. Table 1 summarizes the perplexity and the cost for BitNet b1.58 and LLaMA LLM. It shows that BitNet b1.58 starts to match full precision LLaMA LLM at 3B model size in terms of perplexity, while being 2.71 times faster and using 3.55 times less GPU memory. In particular, BitNet b1.58 with a 3.9B model size is 2.4 times faster, consumes 3.32 times less memory, but performs significantly better than LLaMA LLM 3B. 2https://github.com/ggerganov/llama.cpp 3https://github.com/NVIDIA/FasterTransformer 3 1.3B 3B 7B 13B 70B Model Size 100 101 102 Latency (ms) 1.67x 2.71x 2.90x 3.68x 4.10x BitNet b1.58 LLaMA 1.3B 3B 7B 13B 70B Model Size 100 101 102 Memory (GB) 2.93x 3.55x 4.40x 5.12x 7.16x BitNet b1.58 LLaMA Figure 2: Decoding latency (Left) and memory consumption
3https://github.com/NVIDIA/FasterTransformer 3 1.3B 3B 7B 13B 70B Model Size 100 101 102 Latency (ms) 1.67x 2.71x 2.90x 3.68x 4.10x BitNet b1.58 LLaMA 1.3B 3B 7B 13B 70B Model Size 100 101 102 Memory (GB) 2.93x 3.55x 4.40x 5.12x 7.16x BitNet b1.58 LLaMA Figure 2: Decoding latency (Left) and memory consumption (Right) of BitNet b1.58 varying the model size. Models Size Max Batch Size Throughput (tokens/s) LLaMA LLM 70B 16 (1.0x) 333 (1.0x) BitNet b1.58 70B 176 (11.0x) 2977 (8.9x) Table 3: Comparison of the throughput between BitNet b1.58 70B and LLaMA LLM 70B. Table 2 reports the detailed results of the zero-shot accuracy on the end tasks. We followed the pipeline from lm-evaluation-harness4 to perform the evaluation. The results show that the performance gap between BitNet b1.58 and LLaMA LLM narrows as the model size increases. More importantly, BitNet b1.58 can match the performance of the full precision baseline starting from a 3B size. Similar to the observation of the perplexity, the end-task results reveal that BitNet b1.58 3.9B outperforms LLaMA LLM 3B with lower memory and latency cost. This demonstrates that BitNet b1.58 is a Pareto improvement over the state-of-the-art LLM models. Memory and Latency We further scaled up
from a 3B size. Similar to the observation of the perplexity, the end-task results reveal that BitNet b1.58 3.9B outperforms LLaMA LLM 3B with lower memory and latency cost. This demonstrates that BitNet b1.58 is a Pareto improvement over the state-of-the-art LLM models. Memory and Latency We further scaled up the model size to 7B, 13B, and 70B and evaluated the cost. Figure 2 illustrates the trends of latency and memory, showing that the speed-up increases as the model size scales. In particular, BitNet b1.58 70B is 4.1 times faster than the LLaMA LLM baseline. This is because the time cost for nn.Linear grows with the model size. The memory consumption follows a similar trend, as the embedding remains full precision and its memory proportion is smaller for larger models. Both latency and memory were measured with a 2-bit kernel, so there is still room for optimization to further reduce the cost. Energy We also estimate the arithmetic operations energy consumption of both BitNet b1.58 and LLaMA LLM. We focus mainly on the calculation for matrix multiplication, since it contributes the most to the cost of LLMs. Figure 3 illustrates the composition of the energy cost. The majority of BitNet
the cost. Energy We also estimate the arithmetic operations energy consumption of both BitNet b1.58 and LLaMA LLM. We focus mainly on the calculation for matrix multiplication, since it contributes the most to the cost of LLMs. Figure 3 illustrates the composition of the energy cost. The majority of BitNet b1.58 is INT8 addition calculation, while LLaMA LLM consists of both FP16 addition and FP16 multiplication. According to the energy model in [Hor14, ZZL22], BitNet b1.58 saves 71.4 times arithmetic operations energy consumption for matrix multiplication on 7nm chips. We further reported the end-to-end energy cost for models with 512 tokens. Our results show that as the model size scales, BitNet b1.58 becomes increasingly more efficient in terms of energy consumption compared to the FP16 LLaMA LLM baseline. This is due to the fact that the percentage of nn.Linear grows with the model size, while the cost from other components is smaller for larger models. Throughput We compare the throughput of BitNet b1.58 and LLaMA LLM with 70B parameters on two 80GB A100 cards, using pipeline parallelism [HCB+19] so that LLaMA LLM 70B could be run on the devices. We increased the batch size until the GPU memory limit was
is smaller for larger models. Throughput We compare the throughput of BitNet b1.58 and LLaMA LLM with 70B parameters on two 80GB A100 cards, using pipeline parallelism [HCB+19] so that LLaMA LLM 70B could be run on the devices. We increased the batch size until the GPU memory limit was reached, with a sequence length of 512. Table 3 shows that BitNet b1.58 70B can support up to 11 times the batch size of LLaMA LLM, resulting an 8.9 times higher throughput. 4https://github.com/EleutherAI/lm-evaluation-harness 4 BitNet b1.58 LLaMA 0.0 0.1 0.2 0.3 0.4 0.5 7nm Energy Cost (pJ) 71.4x INT8 Add FP16 Add FP16 Mul 1.3B 3B 7B 13B 70B Model Size 10 1 100 101 Energy (J) 18.6x 21.7x 29.1x 32.9x 41.2x BitNet b1.58 LLaMA Figure 3: Energy consumption of BitNet b1.58 compared to LLaMA LLM at 7nm process nodes. On the left is the components of arithmetic operations energy. On the right is the end-to-end energy cost across different model sizes. Models Tokens Winogrande PIQA SciQ LAMBADA ARC-easy Avg. StableLM-3B 2T 64.56 76.93 90.75 66.09 67.78 73.22 BitNet b1.58 3B 2T 66.37 78.40 91.20 67.63 68.12 74.34 Table 4: Comparison of BitNet b1.58 with StableLM-3B with 2T tokens. BitNet
On the right is the end-to-end energy cost across different model sizes. Models Tokens Winogrande PIQA SciQ LAMBADA ARC-easy Avg. StableLM-3B 2T 64.56 76.93 90.75 66.09 67.78 73.22 BitNet b1.58 3B 2T 66.37 78.40 91.20 67.63 68.12 74.34 Table 4: Comparison of BitNet b1.58 with StableLM-3B with 2T tokens. BitNet b1.58 is enabling a new scaling law with respect to model performance and inference cost. As a reference, we can have the following equivalence between different model sizes in 1.58-bit and 16-bit based on the results in Figure 2 and 3. ‚Ä¢ 13B BitNet b1.58 is more efficient, in terms of latency, memory usage and energy consump- tion, than 3B FP16 LLM. ‚Ä¢ 30B BitNet b1.58 is more efficient, in terms of latency, memory usage and energy consump- tion, than 7B FP16 LLM. ‚Ä¢ 70B BitNet b1.58 is more efficient, in terms of latency, memory usage and energy consump- tion, than 13B FP16 LLM. Training with 2T Tokens The number of training tokens is a crucial factor for LLMs. To test the scalability of BitNet b1.58 in terms of tokens, we trained a BitNet b1.58 model with 2T to- kens following the data recipe of StableLM-3B [TBMR], which is the
than 13B FP16 LLM. Training with 2T Tokens The number of training tokens is a crucial factor for LLMs. To test the scalability of BitNet b1.58 in terms of tokens, we trained a BitNet b1.58 model with 2T to- kens following the data recipe of StableLM-3B [TBMR], which is the state-of-the-art open-source 3B model. Both models were evaluated on a benchmark that consists of Winogrande [SBBC20], PIQA [BZB+19], SciQ [WLG17], LAMBADA [PKL+16], and ARC-easy [YBS19]. We reported the zero-shot accuracy in Table 4. For tasks measured with accuracy and normalized accuracy, we take the average of the two. The results of StableLM 3b at 2T tokens are taken directly from its technical report. Our findings shows that BitNet b1.58 achieves a superior performance on all end tasks, indicating that 1.58-bit LLMs also have strong generalization capabilities. 4 Discussion and Future Work 1-bit Mixture-of-Experts (MoE) LLMs Mixture-of-Experts (MoE) have proven to be a cost-effective approach for LLMs. While it signifi- cantly reduces the computation FLOPs, the high memory consumption and inter-chip communication overhead limit its deployment and application. These challenges can be addressed by 1.58-bit LLMs. Firstly, the reduced memory footprint reduces the number of devices required to deploy MoE models.
be a cost-effective approach for LLMs. While it signifi- cantly reduces the computation FLOPs, the high memory consumption and inter-chip communication overhead limit its deployment and application. These challenges can be addressed by 1.58-bit LLMs. Firstly, the reduced memory footprint reduces the number of devices required to deploy MoE models. Moreover, it significantly reduces the overhead of transferring activations across networks. Ultimately, there would be no overhead if the entire models could be placed on a single chip. 5 Native Support of Long Sequence in LLMs In the era of LLMs, the ability to handle long sequence has become a critical demand. One major challenge for long sequence inference is the memory consumption introduced by the KV caches. BitNet b1.58 represents a significant step towards native support for long sequences, as it reduces the activations from 16 bits to 8 bits, allowing the context length to be doubled given the same resources. This can be further losslessly compressed to 4 bits or even lower for 1.58-bit LLMs, which we leave as future work. LLMs on Edge and Mobile The use of 1.58-bit LLMs has the potential to greatly improve the performance of language models on edge and mobile devices. These
same resources. This can be further losslessly compressed to 4 bits or even lower for 1.58-bit LLMs, which we leave as future work. LLMs on Edge and Mobile The use of 1.58-bit LLMs has the potential to greatly improve the performance of language models on edge and mobile devices. These devices are often limited by their memory and computational power, which can restrict the performance and the scale of LLMs. However, the reduced memory and energy consumption of 1.58-bit LLMs allows them to be deployed on these devices, enabling a wide range of applications that were previously not possible. This can greatly enhance the capabilities of edge and mobile devices and enable new and exciting applications of LLMs. Moreover, 1.58-bit LLMs are more friendly to CPU devices, which are the main processors used in edge and mobile devices. This means that BitNet b1.58 can be efficiently executed on these devices, further improving their performance and capabilities. New Hardware for 1-bit LLMs Recent work like Groq5 has demonstrated promising results and great potential for building specific hardware (e.g., LPUs) for LLMs. Going one step further, we envision and call for actions to design new hardware and system specifically optimized for 1-bit
further improving their performance and capabilities. New Hardware for 1-bit LLMs Recent work like Groq5 has demonstrated promising results and great potential for building specific hardware (e.g., LPUs) for LLMs. Going one step further, we envision and call for actions to design new hardware and system specifically optimized for 1-bit LLMs, given the new computation paradigm enabled in BitNet [WMD+23]. References [BZB+19] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about physical commonsense in natural language. CoRR, abs/1911.11641, 2019. [CCKS23] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. QuIP: 2-bit quantization of large language models with guarantees. CoRR, abs/2307.13304, 2023. [CLC+19] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. CoRR, abs/1905.10044, 2019. [Com23] Together Computer. Redpajama: an open dataset for training large language models, 2023. [FAHA23] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: accurate quantization for generative pre-trained transformers. In The Eleventh International Conference on Learning Representations, 2023. [HCB+19] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Xu Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. Gpipe:
Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: accurate quantization for generative pre-trained transformers. In The Eleventh International Conference on Learning Representations, 2023. [HCB+19] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Xu Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. Gpipe: Efficient training of giant neural networks using pipeline parallelism. In Advances in Neural Information Processing Systems, pages 103‚Äì112, 2019. [Hor14] Mark Horowitz. 1.1 computing‚Äôs energy problem (and what we can do about it). In 2014 IEEE International Conference on Solid-State Circuits Conference, ISSCC 2014, Digest of Technical Papers, San Francisco, CA, USA, February 9-13, 2014, pages 10‚Äì14, 2014. [KLZ+23] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. 5https://groq.com/ 6 [LTT+23] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. AWQ: activation-aware weight quantization for LLM compression and acceleration. CoRR, abs/2306.00978, 2023. [MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? A new
Systems Principles, 2023. 5https://groq.com/ 6 [LTT+23] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. AWQ: activation-aware weight quantization for LLM compression and acceleration. CoRR, abs/2306.00978, 2023. [MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? A new dataset for open book question answering. CoRR, abs/1809.02789, 2018. [MXBS16] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. [PKL+16] Denis Paperno, Germ√°n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern√°ndez. The LAMBADA dataset: Word prediction requiring a broad discourse context. In Proceed- ings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics, 2016. [RSR+19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. CoRR, abs/1910.10683, 2019. [SAL+24] Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [SBBC20] Keisuke Sakaguchi,
Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. CoRR, abs/1910.10683, 2019. [SAL+24] Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [SBBC20] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Wino- Grande: an adversarial winograd schema challenge at scale. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, pages 8732‚Äì8740, 2020. [Sha20] Noam Shazeer. GLU variants improve transformer. CoRR, abs/2002.05202, 2020. [TBMR] Jonathan Tow, Marco Bellagente, Dakota Mahan, and Carlos Riquelme. Stablelm 3b 4e1t. [TCS+24] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. Quip#: Even better LLM quantization with hadamard incoherence and lattice codebooks. CoRR, abs/2402.04396, 2024. [TLI+23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: open and efficient foundation language models. CoRR, abs/2302.13971, 2023. [TMS+23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull,
Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: open and efficient foundation language models. CoRR, abs/2302.13971, 2023. [TMS+23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, and et al. Llama 2: open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023. [WLG17] Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. In Leon Derczynski, Wei Xu, Alan Ritter, and Tim Baldwin, editors, Proceedings of the 3rd Workshop on Noisy User-generated Text, NUT@EMNLP 2017, Copenhagen, Denmark, September 7, 2017, pages 94‚Äì106. Association for Computa- tional Linguistics, 2017. [WMC+23] Lei Wang, Lingxiao Ma, Shijie Cao, Ningxin Zheng, Quanlu Zhang, Jilong Xue, Ziming Miao, Ting Cao, , and Yuqing Yang. Ladder: Efficient tensor compilation on customized data format. In OSDI, 2023. [WMD+23] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. Bitnet: Scaling 1-bit transformers for large language models. CoRR, abs/2310.11453, 2023. 7 [XLS+23] Guangxuan Xiao, Ji Lin, Micka√´l Seznec, Hao Wu, Julien Demouth, and Song Han. SmoothQuant:
2023. [WMD+23] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. Bitnet: Scaling 1-bit transformers for large language models. CoRR, abs/2310.11453, 2023. 7 [XLS+23] Guangxuan Xiao, Ji Lin, Micka√´l Seznec, Hao Wu, Julien Demouth, and Song Han. SmoothQuant: accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, 2023. [YBS19] Vikas Yadav, Steven Bethard, and Mihai Surdeanu. Quick and (not so) dirty: Unsuper- vised selection of justification sentences for multi-hop question answering. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, EMNLP-IJCNLP, 2019. [ZHB+19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: can a machine really finish your sentence? In Proceedings of the 57th Conference of the Association for Computational Linguistics, pages 4791‚Äì4800, 2019. [ZS19] Biao Zhang and Rico Sennrich. Root mean square layer normalization. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d‚ÄôAlch√©-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems, pages 12360‚Äì12371, 2019. [ZZL22] Yichi Zhang, Zhiru Zhang, and Lukasz Lew. PokeBNN: A binary pursuit of lightweight accuracy. In IEEE/CVF Conference
Sennrich. Root mean square layer normalization. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d‚ÄôAlch√©-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems, pages 12360‚Äì12371, 2019. [ZZL22] Yichi Zhang, Zhiru Zhang, and Lukasz Lew. PokeBNN: A binary pursuit of lightweight accuracy. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12465‚Äì12475. IEEE, 2022. 8
